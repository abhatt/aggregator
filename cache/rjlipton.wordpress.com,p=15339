<?xml version="1.0" encoding="utf-8"?><entry xml:lang="en" xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/"><id>http://rjlipton.wordpress.com/?p=15339</id><link href="https://rjlipton.wordpress.com/2018/10/18/london-calling/" rel="alternate" type="text/html"/><title>London Calling</title><summary>For chess and science: a cautionary tale about decision models Clarke Chronicler blog source Marmaduke Wyvill was a British chess master and Member of Parliament in the 1800s. He was runner-up in what is considered the first major international chess tournament, London 1851, but never played in a comparable tournament again. He promoted chess and […]<div class="commentbar"><p/></div></summary><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>For chess and science: a cautionary tale about decision models</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2018/10/18/london-calling/marmadukewyvill/" rel="attachment wp-att-15340"><img alt="" class="alignright wp-image-15340" height="176" src="https://rjlipton.files.wordpress.com/2018/10/marmadukewyvill.jpg?w=135&amp;h=176" width="135"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Clarke Chronicler blog <a href="http://clarkechroniclerspoliticians.blogspot.com/2013/05/150-marmaduke-wyvill.html">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Marmaduke Wyvill was a British chess master and Member of Parliament in the 1800s. He was runner-up in what is considered the first major international chess tournament, London 1851, but never played in a comparable tournament again. He promoted chess and helped organize and sponsor the great London 1883 chess tournament. Here is a <a href="https://www.thenorthernecho.co.uk/history/8271561.To_the_manor_pawn_for_chess_playing_MP/">fount</a> of information on the name and the man, including that he once proposed marriage to Florence Nightingale, who became a pioneer of statistics.</p>
<p>
Today we use Wyvill’s London 1883 tournament to critique statistical models. Our critique extends to ask, <em>how extensively are models cross-checked?</em></p>
<p>
London is about to take center stage again in chess. The World Championship <a href="https://en.wikipedia.org/wiki/World_Chess_Championship_2018">match</a> between the current world champion, Magnus Carlsen of Norway, and his American challenger Fabiano Caruana will begin there on November 9. This is the first time since 1972 that an American will play for the title. The organizer is <a href="https://worldchess.com/">WorldChess</a> (previously <a href="https://en.wikipedia.org/wiki/Agon_Limited">Agon Ltd.</a>) in partnership with the World Chess Federation (<a href="http://fide.com/">FIDE</a>). <span id="more-15339"/></p>
<p>
The London 1883 tournament had two innovations. It was the first to use chess clocks. The second was that in the event of a game being drawn, the players had to play another game, twice if needed. Only after three draws would the point be considered halved, and this happened only seven times in 182 meetings. Chess clocks have been used in virtually every competition since, but the second experiment has never been repeated—the closest to it will come <a href="https://en.chessbase.com/post/norway-chess-armageddon-gambit">next year</a>. Two scientific imports were that the time to decide on a move was regulated and games without critical action were set aside.</p>
<p>
</p><p/><h2> Chess and Science </h2><p/>
<p/><p>
Chess has long been considered a (or “the”) “game of science.” It has been the focus of numerous scientific studies. Here we emphasize how it is a copious source of scientific <em>data</em>. Millions of games—every top-level game and nowadays many games at lower levels—have been preserved in databases. Except that the time taken by players to choose a move at each turn is recorded only sporadically, we have easy access to full information about each player’s choices of moves. </p>
<p>
What we also have now is authoritative judgment on the <em>true values</em> of those choices via analysis by strong computer chess programs. Those programs, called “engines,” can beat even Carlsen and Caruana with regularity, so we humans have no standing to doubt their judgments. The programs’ values for moves are a robust quality metric, and <a href="https://rjlipton.wordpress.com/2016/11/30/when-data-serves-turkey/">correlate</a> <a href="https://rjlipton.wordpress.com/2016/12/08/magnus-and-the-turkey-grinder/">supremely</a> with the <a href="https://en.wikipedia.org/wiki/Elo_rating_system">Elo</a> <a href="https://fivethirtyeight.com/features/introducing-nfl-elo-ratings/">Rating</a>, which provides a robust skill metric. </p>
<p>
The move values are the only chess-specific input to my statistical choice model. I have <a href="https://rjlipton.wordpress.com/2018/09/07/sliding-scale-problems/">covered</a> <a href="https://rjlipton.wordpress.com/2017/05/23/stopped-watches-and-data-analytics/">it</a> <a href="https://rjlipton.wordpress.com/2016/11/08/unskewing-the-election/">several</a> <a href="https://rjlipton.wordpress.com/2015/10/06/depth-of-satisficing/">times</a> <a href="https://rjlipton.wordpress.com/2012/03/30/when-is-a-law-natural/">before</a>, but not yet in the sense of going “back to square one” to say how it originated—where it fits among decision models. </p>
<p>
This year I have overhauled the model’s 28,000+ lines of C++ code. More exactly I have “underhauled” it by chopping out stale features, removing assumptions, and simplifying operations. I widened the equations to accommodate multiple alternative models and fitting methods, besides the ones I’ve deployed to judge allegations of cheating on behalf of FIDE and other chess bodies. The main alternative discussed here is one I did already program and reject nine years ago, but having recently tried multiple other possibilities reinforces the points about models that I am making here. So let’s first see one general form of decision model and how the chess application fits the framework.</p>
<p>
</p><p/><h2> The Multinomial Logit Model </h2><p/>
<p/><p>
The general goal is to project the probabilities <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> of certain decision choices or event outcomes <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> in terms of data <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> about the <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> and attributes <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> of the decision makers. An index <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> can refer to multiple actors and/or multiple situations; we will suppress it when intent is clear. The index <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j}"/> refers to multiple alternatives <img alt="{m_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_j}"/> (<img alt="{j = 1,\dots,J}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj+%3D+1%2C%5Cdots%2CJ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j = 1,\dots,J}"/>) in any situation and their probabilities <img alt="{p_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_j}"/>. The goal for any <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> is to infer <img alt="{(p_j) = \{p_{i,j}\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28p_j%29+%3D+%5C%7Bp_%7Bi%2Cj%7D%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(p_j) = \{p_{i,j}\}}"/> as a function <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> of <img alt="{X = \{X_{i,j}\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX+%3D+%5C%7BX_%7Bi%2Cj%7D%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X = \{X_{i,j}\}}"/>, <img alt="{S_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S_i}"/>, and internal model parameters <img alt="{\vec{\beta}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7B%5Cbeta%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\vec{\beta}}"/>. The <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> function is “the” model.</p>
<p>
The models we consider all incorporate into <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> a function that takes <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> and <img alt="{S_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S_i}"/> and outputs quantities <img alt="{u_{i,j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu_%7Bi%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u_{i,j}}"/>—which we will speak of as single numbers but which could be vectors over a separate index <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>. In many settings, <img alt="{u_{i,j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu_%7Bi%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u_{i,j}}"/> this represents the <em>utility</em> of outcome <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j}"/> for the actor or situation <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>, which the actor wants to maximize or at least gain enough of to satisfy needs. Insofar as <img alt="{u_{i,j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu_%7Bi%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u_{i,j}}"/> depends on <img alt="{S_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S_i}"/> it is distinct from a neutral notion of “objective value” <img alt="{v_{i,j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_%7Bi%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_{i,j}}"/>. Such a distinction was already <a href="https://en.wikipedia.org/wiki/Expected_utility_hypothesis#Bernoulli's_formulation">observed</a> in the early 1700s. </p>
<p>
The <a href="https://en.wikipedia.org/wiki/Discrete_choice#Multinomial_choice_without_correlation_among_alternatives">multinomial</a> <a href="https://en.wikipedia.org/wiki/Multinomial_logistic_regression">logit</a> <a href="http://cess.nyu.edu/wp-content/uploads/2018/03/Multinomial-Logit-Processes.pdf">model</a>, and <em>log-linear</em> models in general, represent the logarithms of the probabilities as linear functions of the other elements. Using the utility function this means setting <a name="loglin"/></p><a name="loglin">
<p align="center"><img alt="\displaystyle  \log(p_j) = \alpha + \beta u_j, \ \ \ \ \ (1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clog%28p_j%29+%3D+%5Calpha+%2B+%5Cbeta+u_j%2C+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \log(p_j) = \alpha + \beta u_j, \ \ \ \ \ (1)"/></p>
</a><p><a name="loglin"/> where we have suppressed <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> and <img alt="{\beta u_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta+u_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta u_j}"/> could be multiple linear terms <img alt="{\beta_1 u_{j,1} + \cdots + \beta_k u_{j,k}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta_1+u_%7Bj%2C1%7D+%2B+%5Ccdots+%2B+%5Cbeta_k+u_%7Bj%2Ck%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta_1 u_{j,1} + \cdots + \beta_k u_{j,k}}"/>. This makes <a name="exp"/></p><a name="exp">
<p align="center"><img alt="\displaystyle  p_j = e^{\alpha + \beta u_j} \ \ \ \ \ \ \ \ \ \ \ \ \ (2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++p_j+%3D+e%5E%7B%5Calpha+%2B+%5Cbeta+u_j%7D+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  p_j = e^{\alpha + \beta u_j} \ \ \ \ \ \ \ \ \ \ \ \ \ (2)"/></p>
</a><p><a name="exp"/> for all <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j}"/>. Then <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha}"/> becomes a normalization constant to ensure that the probabilities sum to <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>, dropping out to give the final equations <a name="softmax"/></p><a name="softmax">
<p align="center"><img alt="\displaystyle  p_j = \frac{e^{\beta u_j}}{\sum_{\ell=1}^J e^{\beta u_\ell}}. \ \ \ \ \ (3)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++p_j+%3D+%5Cfrac%7Be%5E%7B%5Cbeta+u_j%7D%7D%7B%5Csum_%7B%5Cell%3D1%7D%5EJ+e%5E%7B%5Cbeta+u_%5Cell%7D%7D.+%5C+%5C+%5C+%5C+%5C+%283%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  p_j = \frac{e^{\beta u_j}}{\sum_{\ell=1}^J e^{\beta u_\ell}}. \ \ \ \ \ (3)"/></p>
</a><p><a name="softmax"/> Fitting <img alt="{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta}"/> thus yields all the probabilities. Note that putting a difference of probabilities <img alt="{\log(p_1) - \log(p_i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog%28p_1%29+-+%5Clog%28p_i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log(p_1) - \log(p_i)}"/> on the left-hand side of (<a href="https://rjlipton.wordpress.com/feed/#loglin">1</a>), which is the log of the ratio of the probabilities, leads to the same model and normalization (up to the sign of <img alt="{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta}"/>). The function of normalizing exponentiated quantities is so common it has its own pet name, <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a>.</p>
<p>
These last three equations were known already in 1883 via the physicists Josiah Gibbs and Ludwig Boltzmann, with <img alt="{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta}"/> coming out in units of inverse temperature, the denominator of (<a href="https://rjlipton.wordpress.com/feed/#softmax">3</a>) representing the <a href="https://en.wikipedia.org/wiki/Partition_function_(statistical_mechanics)">partition function</a> of a physical system, and the numerator the Boltzmann <a href="https://en.wikipedia.org/wiki/Boltzmann_distribution">factor</a>. It seems curious that apart from some contemporary references by Charles Peirce they were not used in wider contexts until the World War II era. Equation (<a href="https://rjlipton.wordpress.com/feed/#softmax">3</a>) essentially appears as equation (1) in the 2000 Economics Nobel <a href="https://pubs.aeaweb.org/doi/pdfplus/10.1257/aer.91.3.351">lecture</a> by Daniel McFadden, who calls it “the” multinomial logit model (see also <a href="https://www.jstor.org/stable/3440992?seq=1#page_scan_tab_contents">this</a>) and traces it to work by Duncan Luce in 1959. Such pan-scientific heft makes its failure in chess all the more surprising.</p>
<p>
</p><p/><h2> The Chess Case </h2><p/>
<p/><p>
In chess tournaments we have multiple players, but only one is involved in deciding each move. So we focus on one player but can treat multiple players as a group. Instead what we represent with the <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> index is the player(s) facing multiple positions. To a large extent we can treat those decisions as independent. Even if the player executes a plan over a few moves the covariance is still <em>sparse</em>, and often players realize they have to revise their plans on the next turn. Thus we replace <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> by an index <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> signifying “turn” or “time” for each position faced by the player. Clearly we want to fit by regression over multiple turns <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>, so <img alt="{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta}"/> and any other fitted parameters will not depend on <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>, which again we sometimes suppress.</p>
<p>
Each possible move <img alt="{m_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_j}"/> at each turn <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> is given a value <img alt="{v_{t,j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_%7Bt%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_{t,j}}"/> by the chess engine(s) used to analyze the games. We order the moves by those values, so the engine’s first-listed move <img alt="{m_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_1}"/> has the optimal value <img alt="{v_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_1}"/>. In just over 90% of positions there is a unique optimal move. There are four salient ways to define the utility <img alt="{u_j = u_{t,j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu_j+%3D+u_%7Bt%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u_j = u_{t,j}}"/> from these values—prefatory to involving model parameters describing the player:</p>
<ul>
<li>
(a) Equate <img alt="{u_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u_j}"/> with the move’s value <img alt="{v_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_j}"/>. <p/>
</li><li>
(b) Equate it with the win expectation <img alt="{e_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e_j}"/> corresponding to <img alt="{v_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_j}"/>, which I described along with its “sliding-scale” issues in this recent <a href="https://rjlipton.wordpress.com/2018/09/07/sliding-scale-problems/">post</a>. <p/>
</li><li>
(c) Use the loss in value <img alt="{v_1 - v_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_1+-+v_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_1 - v_j}"/> compared to an optimal move. <p/>
</li><li>
(d) Use the loss in expectation <img alt="{e_1 - e_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be_1+-+e_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e_1 - e_j}"/> instead.
</li></ul>
<p>
Option (d) automatically scales down differences in positions where one side is significantly ahead. The same small slip that would halve one’s chances in a balanced position might only reduce 90% to 85% in a strong position or be nearly irrelevant in a losing one. I remove the most extremely unbalanced positions from samples anyway. For (c) I use a “non-sliding” scale function <img alt="{\delta(v_1,v_j)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%28v_1%2Cv_j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta(v_1,v_j)}"/> whose efficacy I detailed <a href="https://rjlipton.wordpress.com/2016/11/30/when-data-serves-turkey/">here</a> but I can easily generate results without it. Note that if I were to cut the sample down only to balanced positions—<img alt="{v_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_1}"/> near <img alt="{0.00}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0.00%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0.00}"/>—of which there are a lot, then (a) and (b) become respectively equivalent to (c) and (d) anyway, up to signs which are handled by flipping the sign of <img alt="{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta}"/>.</p>
<p>
My primary model parameter, called <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/> for “sensitivity,” is just a divisor of the values and so gets absorbed by <img alt="{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta}"/>. I have a second main parameter <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> for “consistency” but more on it later. Having <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/> is enough to fill the dictates of the multinomial logit model in the simplest manners. </p>
<p>
Criteria for fitting log-linear models are also a general issue. For linear regressions, least-squares fitting is distinguished by its being equivalent to maximum-likelihood estimation (MLE) under Gaussian error, but <img alt="{L_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_1}"/> or other <img alt="{L_p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_p%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_p}"/> distance can be minimized instead. With log-linear regressions the flex is wider and MLE competes with criteria that minimize various discrepancies between quantities projected from the fitted probabilities <img alt="{p_{t,j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_%7Bt%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_{t,j}}"/> and their actual values in the sample. Here are four of them—we will see more:</p>
<ol>
<li>
The frequency of playing (i.e., “matching”) the move <img alt="{m_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_1}"/>. <p/>
</li><li>
The frequency of playing <img alt="{m_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_1}"/> or another move of equal-optimal value (for the 10% of positions that have them). <p/>
</li><li>
The total “loss of Pawns,” whether scaled as <img alt="{\delta(v_1,v_j)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%28v_1%2Cv_j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta(v_1,v_j)}"/> or left unscaled as <img alt="{v_1 - v_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_1+-+v_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_1 - v_j}"/>. <p/>
</li><li>
The total loss of expectation <img alt="{e_1 - e_\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be_1+-+e_%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e_1 - e_\ell}"/> between the optimal move and the played move <img alt="{m_\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_\ell}"/>.
</li></ol>
<p>
The reason for the first three in particular is that they create my three main tests for possible cheating, so I want to fit them on my training data (which now encompasses every rating level from Elo 1025 to Elo 2800 in steps of 25) to be <em>unbiased estimators</em>. Besides those and MLE—which here means maximizing the projected likelihood of the moves that were observed to be played (or alternately the likelihood of the observed <img alt="{m_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_1}"/>-match/non-match sequence and various others)—my code allows composing a <a href="https://en.wikipedia.org/wiki/Loss_function">loss function</a> from myriad components and weighting them ad-lib. Components unused in the fitting become the cross-checks.</p>
<p>
Ideally, we’d like all the fitting criteria to produce similar fits—that is, close sets of fitted values for <img alt="{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta}"/> and other parameters on the same data. Finally, the code implements other modeling equations besides multinomial logit—and we’d like their results to agree too. But let’s first see how multinomial logit performs.</p>
<p>
</p><p/><h2> One Log Bad </h2><p/>
<p/><p>
I analyzed the 168 official played games of London 1883 (one competitor left just after the halfway point), and separately, the 76 rejected draws, using Stockfish 7 to high depth. The former give 10,289 analyzed game turns after applying the extreme-value cutoff and a few others. Using the simple unscaled version (c) of utility and fitting <img alt="{\beta \equiv 1/s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta+%5Cequiv+1%2Fs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta \equiv 1/s}"/> according to <img alt="{m_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_1}"/> matching gives these results for the first three fitting criteria:</p>
<pre>          Test Name       ProjVal   Actual   Proj%  Actual%  z-score
          MoveMatch       4871.02  4871.00  47.34%  47.34%  z = -0.00
          EqValueMatch    5228.95  5201.00  50.82%  50.55%  z = -0.73
          ExpectationLoss  259.20   297.34  0.0252  0.0289  z = -10.58
</pre>
<p>
This is actual output from my code, except that to avoid crowding I have elided some columns including the standard deviations on which the <a href="https://en.wikipedia.org/wiki/Standard_score"><img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/>-scores</a> are based. The <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/>-scores give a uniform way to judge goodness of fit. The first one is exactly zero because that was the criterion expressly fitted. The fitted model generates a projection for the second one that is higher than what actually happened at London 1883, but only slightly: the <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/>-score is within one standard deviation. The third, however, is under-projected by more than 10 standard deviations. In absolute terms it doesn’t look so bad—259 is only 13% smaller than 297—but the large <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/>-score reflects our having a lot of data. Well, there’s large and there’s huge:</p>
<pre>           Test Name       ProjVal   Actual  Proj%  Actual%  z-score
           AvgDifference   1493.46  2780.07  0.145  0.270  z = -60.9638
</pre>
<p>
The projection is only half what it should be. The <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/>-score is inconceivable.</p>
<p>
For more cross-checks, there are the projected versus actual frequency of playing the <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>-th best move for <img alt="{k &gt; 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+%3E+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k &gt; 1}"/>. Here is the table for ranks 1–10:</p>
<pre>           Rk  ProjVal  Actual   Proj%  Actual%  z-score
            1  4871.02  4871.00  47.34%  47.34%  z = -0.00
            2  1416.72  1729.00  13.80%  16.85%  z = +9.44
            3  761.84    951.00   7.47%   9.32%  z = +7.33
            4  523.25    593.00   5.19%   5.88%  z = +3.20
            5  401.63    410.00   4.03%   4.11%  z = +0.43
            6  325.30    295.00   3.29%   2.99%  z = -1.73
            7  272.12    247.00   2.77%   2.51%  z = -1.57
            8  232.05    197.00   2.37%   2.01%  z = -2.36
            9  200.88    169.00   2.06%   1.73%  z = -2.30
           10  175.95    104.00   1.81%   1.07%  z = -5.54
</pre>
<p>
The first row was the one fitted. Then the projections are off by three percentage points for <img alt="{m_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_2}"/> and almost two for <img alt="{m_3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_3}"/>. For ranks 5–9 they are tantalizingly close but by <img alt="{m_{10}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_%7B10%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_{10}}"/> they have clearly overshot—as they must for the probabilities to add to 1.</p>
<p>
There are yet more cross-checks of even greater importance. They are the frequency with which players make errors of a given range of magnitude: a small slip, a mistake, a serious misstep, a blunder. Those results are too gruesome to show here. Fitting by MLE helps in some places but throws off the <img alt="{m_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_1}"/> fit entirely. </p>
<p>
The huge gaps in these and especially in the “AvgDifference” test (AD for short) rule out any patch to the log-linear model with one <img alt="{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta}"/>. I have tried adding other linear <img alt="{\beta_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta_k}"/> terms representing features such as a move <img alt="{m_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_j}"/> turning an advantage into a disadvantage (<img alt="{v_1 &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_1+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_1 &gt; 0}"/> but <img alt="{v_j &lt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_j+%3C+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_j &lt; 0}"/>). They give haywire results unless the nonlinearity described next is introduced.</p>
<p>
</p><p/><h2> Log-Linear With Revised Utility: Still Bad </h2><p/>
<p/><p>
This is to define the utility function using a new parameter <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> as </p>
<p align="center"><img alt="\displaystyle  u_j = \delta(v_1,v_j)^c. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++u_j+%3D+%5Cdelta%28v_1%2Cv_j%29%5Ec.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  u_j = \delta(v_1,v_j)^c. "/></p>
<p>Without scaling this is just <img alt="{(v_1 - v_j)^c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28v_1+-+v_j%29%5Ec%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(v_1 - v_j)^c}"/>; one can also use <img alt="{(e_1 - e_j)^c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28e_1+-+e_j%29%5Ec%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(e_1 - e_j)^c}"/> or put the power on the values separately. In forming <img alt="{\beta u_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta+u_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta u_j}"/> it does not matter whether the fitted value is represented as <img alt="{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta}"/> or as <img alt="{\beta^c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%5Ec%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta^c}"/>. Using the notation <img alt="{\beta = \frac{1}{s}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta+%3D+%5Cfrac%7B1%7D%7Bs%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta = \frac{1}{s}}"/>, so that <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/> divides out the “pawn units” of <img alt="{v_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_1}"/> and <img alt="{v_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_j}"/>, this means that without loss of generality we can write </p>
<p align="center"><img alt="\displaystyle  u_j = \left(\frac{\delta(v_1,v_j)}{s}\right)^c. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++u_j+%3D+%5Cleft%28%5Cfrac%7B%5Cdelta%28v_1%2Cv_j%29%7D%7Bs%7D%5Cright%29%5Ec.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  u_j = \left(\frac{\delta(v_1,v_j)}{s}\right)^c. "/></p>
<p>This makes clear that the quantity being powered is dimensionless. The motivation for <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> is that in any quantity of the form <img alt="{(\delta/s)^c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28%5Cdelta%2Fs%29%5Ec%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(\delta/s)^c}"/>, the marginal influence of <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> becomes greater for large <img alt="{\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta}"/> than that of <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/>. Thus <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> can be said to govern the propensity for making large mistakes while <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/> governs the perception of small differences <img alt="{\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta}"/> in value. Higher <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> and lower <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/> correspond to higher skill. The former connotes the ability to navigate tactical minefields, the latter strategic skill of amassing small advantages. </p>
<p>
Thus I regard <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> as natural in chess. In my results, <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> usually fits with values going up from below <img alt="{0.45}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0.45%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0.45}"/> to about <img alt="{0.55}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0.55%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0.55}"/> as the Elo level increases. This is in the rough neighborhood of square-root and definitely apart from <img alt="{c=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c=1}"/>. It also changes the calculus on a property called “independence from irrelevant alternatives,” which McFadden cites from Luce but has issues discussed e.g. <a href="http://www.its.caltech.edu/~mshum/ec106/discrete.pdf">here</a>. </p>
<p>
Since <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> is part of the revised utility function, the model is still log-linear in the utility and the probabilities are still obtained via the procedure (<a href="https://rjlipton.wordpress.com/feed/#loglin">1</a>)–(<a href="https://rjlipton.wordpress.com/feed/#softmax">3</a>). The end-product is that having <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> allows fitting two criteria exactly to yield them as unbiased estimators. Here are the results of fitting <img alt="{m_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_1}"/> and AD in what is now a “log-radical(<img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/>)” model:</p>
<pre>           Test Name       ProjVal   Actual   Proj%  Actual%   z-score
           MoveMatch       4870.99  4871.00  47.34%  47.34%  z = +0.00
           AvgDifference   2780.10  2780.07  0.2702  0.2702  z = +0.00
           EqValueMatch    5261.35  5201.00  51.14%  50.55%  z = -1.44
           ExpectationLoss  413.42   297.35  0.0402  0.0289  z = +15.89
</pre>
<p>
The equal-optimal projection remains OK. The expectation loss, however, flips from an under-projection to a vast over-projection. The cross-checks from the move ranks give further bad news:</p>
<pre>           Rk  ProjVal  Actual   Proj% Actual%  z-score
            1  4870.99  4871.00  47.34% 47.34% z =  +0.00
            2  1123.22  1729.00  10.94% 16.85% z = +19.88
            3  633.30   951.00   6.21%  9.32%  z = +13.27
            4  459.83   593.00   4.56%  5.88%  z =  +6.44
            5  370.58   410.00   3.72%  4.11%  z =  +2.11
            6  311.98   295.00   3.16%  2.99%  z =  -0.99
            7  270.56   247.00   2.75%  2.51%  z =  -1.46
            8  239.36   197.00   2.44%  2.01%  z =  -2.79
            9  214.30   169.00   2.19%  1.73%  z =  -3.15
           10  193.93   104.00   1.99%  1.07%  z =  -6.57
</pre>
<p>
The discrepancy in the second-best move has doubled to <em>six</em> percentage points while the third-best move is off by more than three. </p>
<p>
Maximum-likelihood fitting makes the gaps even worse. No re-jiggering of fitting methods nor the formula for <img alt="{u_{t,j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu_%7Bt%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u_{t,j}}"/> comes anywhere close to coherence. Inconsistency in the second-best move kills everything. The fault must be tied all the way to the log-linear model for the probabilities.</p>
<p>
</p><p/><h2> Two Logs Good </h2><p/>
<p/><p>
As we have noted, taking the difference of logs, and inverting so that signs stay positive like so: </p>
<p align="center"><img alt="\displaystyle  \log(1/p_j) - \log(1/p_1) = \alpha + \beta u_j, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clog%281%2Fp_j%29+-+%5Clog%281%2Fp_1%29+%3D+%5Calpha+%2B+%5Cbeta+u_j%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \log(1/p_j) - \log(1/p_1) = \alpha + \beta u_j, "/></p>
<p>does not change the model. The likelihoods <img alt="{e^{\alpha + \beta u_j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%5E%7B%5Calpha+%2B+%5Cbeta+u_j%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e^{\alpha + \beta u_j}}"/> are still normalized arithmetically as in the Gibbs equations. Taking a difference of <em>double</em> logarithms, however, yields something different: </p>
<p align="center"><img alt="\displaystyle  \begin{array}{rcl}  \log\log(1/p_j) - \log\log(1/p_1) &amp;=&amp; \alpha + \beta u_j\\ \implies \frac{\log(1/p_j)}{\log(1/p_1)} &amp;=&amp; \exp(\alpha + \beta u_j)\\ \implies \log(p_j) &amp;=&amp; (\log(p_1))\exp(\alpha + \beta u_j)\\ \implies p_j &amp;=&amp; p_1^{\exp(\alpha + \beta u_j)}. \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++%5Clog%5Clog%281%2Fp_j%29+-+%5Clog%5Clog%281%2Fp_1%29+%26%3D%26+%5Calpha+%2B+%5Cbeta+u_j%5C%5C+%5Cimplies+%5Cfrac%7B%5Clog%281%2Fp_j%29%7D%7B%5Clog%281%2Fp_1%29%7D+%26%3D%26+%5Cexp%28%5Calpha+%2B+%5Cbeta+u_j%29%5C%5C+%5Cimplies+%5Clog%28p_j%29+%26%3D%26+%28%5Clog%28p_1%29%29%5Cexp%28%5Calpha+%2B+%5Cbeta+u_j%29%5C%5C+%5Cimplies+p_j+%26%3D%26+p_1%5E%7B%5Cexp%28%5Calpha+%2B+%5Cbeta+u_j%29%7D.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{array}{rcl}  \log\log(1/p_j) - \log\log(1/p_1) &amp;=&amp; \alpha + \beta u_j\\ \implies \frac{\log(1/p_j)}{\log(1/p_1)} &amp;=&amp; \exp(\alpha + \beta u_j)\\ \implies \log(p_j) &amp;=&amp; (\log(p_1))\exp(\alpha + \beta u_j)\\ \implies p_j &amp;=&amp; p_1^{\exp(\alpha + \beta u_j)}. \end{array} "/></p>
<p>With the utility <img alt="{u_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u_j}"/> still defined as <img alt="{\delta(v_1,v_j)^c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%28v_1%2Cv_j%29%5Ec%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta(v_1,v_j)^c}"/> this creates a triple stack of exponentials on the right-hand side. This all looks really unnatural, but see the results it gives, now also showing the interval and large-error tests that were “too gruesome” before:</p>
<pre>           Test Name       ProjVal   Actual   Proj%  Actual%  z-score
           MoveMatch       4871.02  4871.00  47.34%  47.34% z = -0.00
           AvgScaledDiff   1142.61  1142.59   0.111   0.111 z = +0.00
           EqValueMatch    5251.90  5201.00  51.04%  50.55% z = -1.10
           ExpectationLoss  333.20   334.46  0.0324  0.0325 z = -0.19

           Rk ProjVal  Sigma    Actual  Proj% Actual%  z-score
            1  4871.02  47.02  4871.00 47.34% 47.34%  z = -0.00
            2  1786.89  37.32  1729.00 17.41% 16.85%  z = -1.55
            3   929.87  28.60   951.00  9.11%  9.32%  z = +0.74
            4   589.93  23.29   593.00  5.85%  5.88%  z = +0.13
            5   419.35  19.84   410.00  4.21%  4.11%  z = -0.47
            6   315.24  17.32   295.00  3.19%  2.99%  z = -1.17
            7   246.68  15.39   247.00  2.51%  2.51%  z = +0.02
            8   198.71  13.85   197.00  2.03%  2.01%  z = -0.12
            9   161.54  12.52   169.00  1.65%  1.73%  z = +0.60
           10   134.18  11.43   104.00  1.38%  1.07%  z = -2.64
           11   111.41  10.43    97.00  1.15%  1.00%  z = -1.38
           12    93.90   9.59    99.00  0.97%  1.02%  z = +0.53
           13    77.94   8.75    76.00  0.81%  0.79%  z = -0.22
           14    65.40   8.02    78.00  0.68%  0.82%  z = +1.57
           15    55.13   7.37    62.00  0.58%  0.65%  z = +0.93

           Selec. Test ProjVal  Actual  Proj%  Actual%  z-score
           Delta01-10   656.08  645.00  6.38%  6.27%  z = -0.56
           Delta11-30   800.75  824.00  7.78%  8.01%  z = +1.02
           Delta31-70   596.51  607.00  5.80%  5.90%  z = +0.50
           Delt71-150   295.70  290.00  2.87%  2.82%  z = -0.38
           Error&gt;=050   709.46  675.00  6.90%  6.56%  z = -1.55
           Error&gt;=100   331.88  300.00  3.23%  2.92%  z = -2.02
           Error&gt;=200   141.56  114.00  1.38%  1.11%  z = -2.62
           Error&gt;=400    68.76   35.00  0.67%  0.34%  z = -4.61
</pre>
<p>
Only the first two lines have been fitted. The other lines follow like obedient ducks—and this persists through all tournaments that I have run. </p>
<p>
There are some wobbles that also persist: The second-best move is somewhat over-projected and the third-best move slightly under—but the remaining indices are off by small amounts whose signs seem random. So are the interval tests at the end, except that large errors are over-projected. The match to moves of equal-optimal worth tends to be over-projected regardless of the patch described <a href="https://rjlipton.wordpress.com/2012/03/30/when-is-a-law-natural/">here</a>. Nevertheless, the overall fidelity under so much cross-validation is an amazing change from the log-linear cases.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p>
The most particular issue I see grants that the original log-linear formulation could be <i>fine</i> for a one-shot purpose, say if the match-to-<img alt="{m_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_1}"/> cheating test were the only thing cared about.  The concern is that in the absence of validation beyond what is needed for that, “mission creep” could extend the usage unknowingly into flawed territory.  It is important to me that a model should score well on a larger slate of pertinent phenomena.  Do other models have as rich a field of data and cross-checks as in chess? </p>
<p>
Is there extensive literature on modeling the <em>double</em> logarithms of probabilities—and on representing probabilities as powers rather than multiples of the “pivot” <img alt="{p_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_1}"/>? We have seen scant references. The term “log-log model” instead refers to having a logarithm on both sides, e.g., <img alt="{\log(p_j) = \alpha + \beta\log(x_j)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog%28p_j%29+%3D+%5Calpha+%2B+%5Cbeta%5Clog%28x_j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log(p_j) = \alpha + \beta\log(x_j)}"/>. Alternatives to log-linear models need to be more conscious of error terms in the utility functions, so perhaps uncertainty needs a more express representation in my formulas.</p>
<p>
The form where <img alt="{p_j = p_1^{L_j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_j+%3D+p_1%5E%7BL_j%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_j = p_1^{L_j}}"/> does have the general issue that when <img alt="{p_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_1}"/> should be very close to 1—as for a completely obvious move in chess—there is strain on getting the exponents <img alt="{L_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_j}"/> large enough to make <img alt="{p_j = p_1^{L_j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_j+%3D+p_1%5E%7BL_j%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_j = p_1^{L_j}}"/> tiny. The over-projection of large errors (<img alt="{p_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_j}"/> too high) is a symptom of this. Some of <a href="https://rjlipton.wordpress.com/2015/10/06/depth-of-satisficing/">my</a> <a href="https://rjlipton.wordpress.com/2016/11/08/unskewing-the-election/">past</a> <a href="https://rjlipton.wordpress.com/2017/05/23/stopped-watches-and-data-analytics/">posts</a> give my thinking on this, but the implementations have been hard to control, so I would be grateful to hear reader thoughts.</p>
<p>
[some name and word fixes]</p></font></font></div></content><updated planet:format="October 18, 2018 11:29 PM">2018-10-18T23:29:55Z</updated><published planet:format="October 18, 2018 11:29 PM">2018-10-18T23:29:55Z</published><category term="chess"/><category term="detection"/><category term="History"/><category term="Ideas"/><category term="News"/><category term="cross-validation"/><category term="Daniel McFadden"/><category term="decision theory"/><category term="discrete choice theory"/><category term="Marmaduke Wyvill"/><category term="models"/><category term="multinomial logit model"/><category term="statistics"/><author><name>KWRegan</name></author><source><id>https://rjlipton.wordpress.com</id><logo>https://s0.wp.com/i/buttonw-com.png</logo><link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/><link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/><link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/><link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/><subtitle>a personal view of the theory of computation</subtitle><title>Gödel’s Lost Letter and P=NP</title><updated planet:format="December 17, 2018 05:29 AM">2018-12-17T05:29:26Z</updated><planet:module>toc</planet:module><planet:format>atom10</planet:format><planet:http_last_modified>Sat, 15 Dec 2018 18:49:29 GMT</planet:http_last_modified><planet:bozo>false</planet:bozo><planet:items_per_page>40</planet:items_per_page><planet:css-id>richard-lipton</planet:css-id><planet:face>lipton.jpeg</planet:face><planet:name>Richard Lipton</planet:name><planet:http_location>https://rjlipton.wordpress.com/feed/</planet:http_location><planet:http_status>301</planet:http_status></source></entry>
