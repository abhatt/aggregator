<?xml version="1.0" ?><entry xml:lang="en" xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/"><id>http://mittheory.wordpress.com/?p=525</id><link href="https://mittheory.wordpress.com/2014/07/07/faster-i-say-the-race-for-the-fastest-sdd-linear-system-solver-stoc-2014-recaps-part-9/" rel="alternate" type="text/html"/><title>Faster, I say! The race for the fastest SDD linear system solver – STOC 2014 Recaps (Part 9)</title><summary>In the next post in our series of STOC 2014 recaps, Adrian Vladu tells us about some of the latest and greatest in Laplacian and SDD linear system solvers. There’s been a flurry of exciting results in this line of work, so we hope this gets you up to speed. The Monday morning session was […]</summary><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In the next post in our series of STOC 2014 recaps, <a href="http://math.mit.edu/people/profile?pid=1350">Adrian Vladu</a> tells us about some of the latest and greatest in Laplacian and SDD linear system solvers. There’s been a flurry of exciting results in this line of work, so we hope this gets you up to speed.</p>
<hr/>
<p>The Monday morning session was dominated by a nowadays popular topic, <a href="http://en.wikipedia.org/wiki/Diagonally_dominant_matrix">symmetric diagonally dominant</a> (SDD) linear system solvers. <a href="http://math.mit.edu/~rpeng/">Richard Peng</a> started by presenting his work with <a href="http://www.cs.yale.edu/homes/spielman/">Dan Spielman</a>, the first parallel solver with near linear work and poly-logarithmic depth! This is exciting, since parallel algorithms are used for large scale problems in scientific computing, so this is a result with great practical applications.</p>
<p>The second talk was given by <a href="http://www.cs.cmu.edu/directory/jakub-pachocki">Jakub Pachocki </a>and <a href="https://www.cs.cmu.edu/directory/shen-chen-xu">Shen Chen Xu</a> from CMU, which was a result of merging two papers. The first result is a new type of trees that can be used as preconditioners. The second one is a more efficient solver, which together with the trees shaved one more <img alt="\log n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+n&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\log n"/> factor in the race for the fastest solver.</p>
<p>Before getting into more specific details, it might be a good idea to provide a bit of background on the vast literature of Laplacian solvers.</p>
<p>Typically, linear systems <img alt="Ax = b" class="latex" src="https://s0.wp.com/latex.php?latex=Ax+%3D+b&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="Ax = b"/> are easier to solve whenever <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="A"/> has some structure on it. A particular class we care about are <a href="http://en.wikipedia.org/wiki/Positive-semidefinite_matrix#Negative-definite.2C_semidefinite_and_indefinite_matrices">positive semidefinite (PSD) matrices</a>. They work nicely because the solution is the minimizer of the quadratic form <img alt="f(x) = \frac{1}{2} x^T A x - b x" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29+%3D+%5Cfrac%7B1%7D%7B2%7D+x%5ET+A+x+-+b+x&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f(x) = \frac{1}{2} x^T A x - b x"/> , which happens to be a convex function due to the PSD-ness of <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="A"/>. Hence we can use various versions of <a href="http://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a>, the convergence of which depends usually on the <a href="http://en.wikipedia.org/wiki/Condition_number">condition number</a> of <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="A"/>.</p>
<p>A subset of PSD matrices are <a href="http://en.wikipedia.org/wiki/Laplacian_matrix">Laplacian matrices</a>, which are nothing else but graph Laplacians; using an easy reduction, one can show that any SDD system can be reduced to a Laplacian system. Laplacians are great because they carry a lot of combinatorial structure. Instead of having to suffer through a lot of scary algebra, this is the place where we finally get to solve some fun problems on graphs. The algorithms we aim for have running time close to linear in the sparsity of the matrix.</p>
<p>One reason why graphs are nice is that we know how to approximate them with other simpler graphs. More specifically, when given a graph <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="G"/>, we care about finding a sparser graph <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="H"/> such that <img alt="L_H \preceq L_G \preceq \alpha \cdot L_H" class="latex" src="https://s0.wp.com/latex.php?latex=L_H+%5Cpreceq+L_G+%5Cpreceq+%5Calpha+%5Ccdot+L_H&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L_H \preceq L_G \preceq \alpha \cdot L_H"/><sup><a href="https://mittheory.wordpress.com/feed/#fn1" id="ref1">a</a></sup>, for some small <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\alpha"/> (the smaller, the better). The point is that whenever you do gradient descent in order to minimize <img alt="f(x)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f(x)"/>, you can take large steps by solving a system in the sparser <img alt="L_H" class="latex" src="https://s0.wp.com/latex.php?latex=L_H&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L_H"/>. Of course, this requires another linear system solve, only that this only needs to be done on a sparser graph. Applying this idea recursively eventually yields efficient solvers. A lot of combinatorial work is spent on understanding how to compute these sparser graphs.</p>
<p>In their seminal work, Spielman and Teng used ultrasparsifiers<sup><a href="https://mittheory.wordpress.com/feed/#fn2" id="ref2">b</a></sup> as their underlying combinatorial structure, and after many pages of work they obtained a near linear algorithm with a large polylogarithmic factor in the running time. Eventually, Koutis, Miller and Peng came up with a much cleaner construction, and showed how to construct a chain of sparser and sparser graphs, which yielded a solver that was actually practical. Subsequently, people spent a lot of time trying to shave log factors from the running time, see [9], [6], [7], [12] (the last of which was presented at this STOC), and the list will probably continue.</p>
<p>After this digression, we can get back to the conference program and discuss the results.</p>
<hr/>
<p><strong><a href="http://arxiv.org/abs/1311.3286">An Efficient Parallel Solver for SDD Linear Systems</a></strong> by <a href="http://math.mit.edu/~rpeng/">Richard Peng</a> and <a href="http://www.cs.yale.edu/homes/spielman/">Daniel Spielman</a></p>
<p>How do we solve a system <img alt="L_G x = b" class="latex" src="https://s0.wp.com/latex.php?latex=L_G+x+%3D+b&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L_G x = b"/>? We need to find away to efficiently apply the operator <img alt="L_G^+" class="latex" src="https://s0.wp.com/latex.php?latex=L_G%5E%2B&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L_G^+"/> to <img alt="b" class="latex" src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="b"/>. Even Laplacians are not easy to invert, and what’s worse, their <a href="http://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_pseudoinverse">pseudoinverses</a> might not even be sparse. However, we can still represent <img alt="L_G^+" class="latex" src="https://s0.wp.com/latex.php?latex=L_G%5E%2B&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L_G^+"/> as a product of sparse matrices which are easy to compute.</p>
<p>We can gain some inspiration from trying to numerically approximate the inverse of <img alt="(1-x)" class="latex" src="https://s0.wp.com/latex.php?latex=%281-x%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="(1-x)"/> for some small real <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="x"/>. Taking the Taylor expansion we get that <img alt="(1-x)^{-1} = \sum_{i \geq 0} x^i = \prod_{k \geq 0} (1+x^{2^k})" class="latex" src="https://s0.wp.com/latex.php?latex=%281-x%29%5E%7B-1%7D+%3D+%5Csum_%7Bi+%5Cgeq+0%7D+x%5Ei+%3D+%5Cprod_%7Bk+%5Cgeq+0%7D+%281%2Bx%5E%7B2%5Ek%7D%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="(1-x)^{-1} = \sum_{i \geq 0} x^i = \prod_{k \geq 0} (1+x^{2^k})"/>. Notice that in order to get <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\epsilon"/> precision, we only need to take the product of the first <img alt="\Theta(\log 1/\epsilon)" class="latex" src="https://s0.wp.com/latex.php?latex=%5CTheta%28%5Clog+1%2F%5Cepsilon%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\Theta(\log 1/\epsilon)"/> factors. It would be great if we could approximate matrix inverses the same way. Actually, we can, since for matrices of norm less than <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="1"/> we have the identity <img alt="(I-A)^{-1} = \prod_{k\geq 0}(I + A^{2^k})" class="latex" src="https://s0.wp.com/latex.php?latex=%28I-A%29%5E%7B-1%7D+%3D+%5Cprod_%7Bk%5Cgeq+0%7D%28I+%2B+A%5E%7B2%5Ek%7D%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="(I-A)^{-1} = \prod_{k\geq 0}(I + A^{2^k})"/>. At this point we’d be tempted to think that we’re almost done, since we can just write <img alt="L_G = D - A" class="latex" src="https://s0.wp.com/latex.php?latex=L_G+%3D+D+-+A&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L_G = D - A"/>, and try to invert <img alt="I - D^{-1/2} A D^{-1/2}" class="latex" src="https://s0.wp.com/latex.php?latex=I+-+D%5E%7B-1%2F2%7D+A+D%5E%7B-1%2F2%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="I - D^{-1/2} A D^{-1/2}"/>. However we would still need to compute matrix powers, and those matrices might again not even be sparse, so this approach needs more work.</p>
<p>Richard presented a variation of this idea that is more amenable to SDD matrices. He writes</p>
<p style="text-align: center;"><img alt="(D-A)^{-1} = \frac{1}{2}( D^{-1} + (I+D^{-1}A)(D - AD^{-1}A)^{-1}(I+AD^{-1} ) )" class="latex" src="https://s0.wp.com/latex.php?latex=%28D-A%29%5E%7B-1%7D+%3D+%5Cfrac%7B1%7D%7B2%7D%28+D%5E%7B-1%7D+%2B+%28I%2BD%5E%7B-1%7DA%29%28D+-+AD%5E%7B-1%7DA%29%5E%7B-1%7D%28I%2BAD%5E%7B-1%7D+%29+%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="(D-A)^{-1} = \frac{1}{2}( D^{-1} + (I+D^{-1}A)(D - AD^{-1}A)^{-1}(I+AD^{-1} ) )"/></p>
<p>The only hard part of applying this inverse operator <img alt="(D-A)^{-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%28D-A%29%5E%7B-1%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="(D-A)^{-1}"/> to a vector consists of left multiplying by <img alt="(D-AD^{-1}A)^{-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%28D-AD%5E%7B-1%7DA%29%5E%7B-1%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="(D-AD^{-1}A)^{-1}"/>. How to do this? One crucial ingredient is the fact that <img alt="M = D-AD^{-1}A" class="latex" src="https://s0.wp.com/latex.php?latex=M+%3D+D-AD%5E%7B-1%7DA&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="M = D-AD^{-1}A"/> is also SDD! Therefore we can recurse, and solve a linear system in <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="M"/>. You might say that we won’t be able to do it efficiently, since <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="M"/> is not sparse. But with a little bit of work and the help of existing spectral sparsification algorithms <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="M"/> can be approximated with a sparse matrix.</p>
<p>Notice that at the <img alt="k^{th}" class="latex" src="https://s0.wp.com/latex.php?latex=k%5E%7Bth%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="k^{th}"/> level of recursion, the operator we need to apply is <img alt="\left(D-(D^{-1}A)^{2^k}\right)^{-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%28D-%28D%5E%7B-1%7DA%29%5E%7B2%5Ek%7D%5Cright%29%5E%7B-1%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\left(D-(D^{-1}A)^{2^k}\right)^{-1}"/>. A quick calculation shows that if the condition number of <img alt="D-A" class="latex" src="https://s0.wp.com/latex.php?latex=D-A&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="D-A"/> is <img alt="\kappa" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ckappa&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\kappa"/>, then the condition number of <img alt="D^{-1}A" class="latex" src="https://s0.wp.com/latex.php?latex=D%5E%7B-1%7DA&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="D^{-1}A"/> is <img alt="1-1/\kappa" class="latex" src="https://s0.wp.com/latex.php?latex=1-1%2F%5Ckappa&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="1-1/\kappa"/>. This means that after <img alt="k = O(\log \kappa)" class="latex" src="https://s0.wp.com/latex.php?latex=k+%3D+O%28%5Clog+%5Ckappa%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="k = O(\log \kappa)"/> iterations, the eigenvalues of <img alt="(D^{-1}A)^{2^k}" class="latex" src="https://s0.wp.com/latex.php?latex=%28D%5E%7B-1%7DA%29%5E%7B2%5Ek%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="(D^{-1}A)^{2^k}"/> are close to <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="0"/>, so we can just approximate the operator with <img alt="D^{-1}" class="latex" src="https://s0.wp.com/latex.php?latex=D%5E%7B-1%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="D^{-1}"/> without paying too much for the error.</p>
<p>There are a few details left out. Sparsifying <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="M"/> requires a bit of understanding of its underlying structure. Also, in order to do this in parallel, the authors originally employed the spectral sparsification algorithm of Spielman and Teng, combined with a local clustering algorithm of Orecchia, Sachdeva and Vishnoi. Blackboxing these two sophisticated results might question the practicality of the algorithm. Fortunately, Koutis recently produced a simple self-contained spectral sparsification algorithm, which parallelizes and can replace all the heavy machinery in Richard and Dan’s paper.</p>
<hr/>
<p><strong><a href="http://dl.acm.org/citation.cfm?id=2591833">Solving SDD Linear Systems in Nearly <img alt="m \log^{1/2} n" class="latex" src="https://s0.wp.com/latex.php?latex=m+%5Clog%5E%7B1%2F2%7D+n&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="m \log^{1/2} n"/> Time</a></strong> by Michael B. Cohen, <a href="http://cpsc.yale.edu/people/rasmus-kyng">Rasmus Kyng</a>, <a href="http://www.cs.cmu.edu/~glmiller/">Gary L. Miller</a>, <a href="http://www.cs.cmu.edu/directory/jakub-pachocki">Jakub W. Pachocki</a>, <a href="http://math.mit.edu/~rpeng/">Richard Peng</a>, <a href="http://users.math.yale.edu/public_html/People/abr37.html">Anup B. Rao</a>, and <a href="https://www.cs.cmu.edu/directory/shen-chen-xu">Shen Chen Xu</a></p>
<p>Jakub Pachocki and Shen Chen Xu talked about two results, which together yield the fastest SDD system solver to date. The race is still on!</p>
<p>Let me go through a bit of more background. Earlier on I mentioned that graph preconditioners are used to take long steps while doing gradient descent. A dual of gradient descent on the quadratic function is the <a href="http://en.wikipedia.org/wiki/Richardson_iteration">Richardson iteration</a>. This is yet another iterative method, which refines a coarse approximation to the solution of a linear system. Let <img alt="L_G" class="latex" src="https://s0.wp.com/latex.php?latex=L_G&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L_G"/> be the Laplacian of our given graph, and <img alt="L_H" class="latex" src="https://s0.wp.com/latex.php?latex=L_H&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L_H"/> be the Laplacian of its preconditioner. Let us assume that we have access to the inverse of <img alt="L_H" class="latex" src="https://s0.wp.com/latex.php?latex=L_H&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L_H"/>. The Richardson iteration computes a sequence <img alt="x_0, x_1, x_2, \dots" class="latex" src="https://s0.wp.com/latex.php?latex=x_0%2C+x_1%2C+x_2%2C+%5Cdots&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="x_0, x_1, x_2, \dots"/>, which converges to the solution of the system. It starts with a weak estimate for the solution, and iteratively attempts to decrease the norm of the residue <img alt="b- L_G x_k" class="latex" src="https://s0.wp.com/latex.php?latex=b-+L_G+x_k&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="b- L_G x_k"/> by updating the current solution with a coarse approximation to the solution of the system <img alt="L_G x = b -L_G x_k" class="latex" src="https://s0.wp.com/latex.php?latex=L_G+x+%3D+b+-L_G+x_k&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L_G x = b -L_G x_k"/>. That coarse approximation is computed using <img alt="L_H^{-1}" class="latex" src="https://s0.wp.com/latex.php?latex=L_H%5E%7B-1%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L_H^{-1}"/>. Therefore steps are given by</p>
<p style="text-align: center;"><img alt="x_{k+1} = x_k + \alpha \cdot L_H^{-1} ( b - L_G x_k )" class="latex" src="https://s0.wp.com/latex.php?latex=x_%7Bk%2B1%7D+%3D+x_k+%2B+%5Calpha+%5Ccdot+L_H%5E%7B-1%7D+%28+b+-+L_G+x_k+%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="x_{k+1} = x_k + \alpha \cdot L_H^{-1} ( b - L_G x_k )"/></p>
<p>where <img alt="\alpha \in (0,1]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Cin+%280%2C1%5D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\alpha \in (0,1]"/> is a parameter that adjusts the length of the step. The better <img alt="L_H" class="latex" src="https://s0.wp.com/latex.php?latex=L_H&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L_H"/> approximates <img alt="L_G" class="latex" src="https://s0.wp.com/latex.php?latex=L_G&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L_G"/>, the fewer steps we need to make.</p>
<p>The problem that Jakub and Shen talked about was finding these good preconditioners. The way they do it is by looking more closely at the Richardson iteration, and weakening the requirements. Instead of having the preconditioner approximate <img alt="L_G" class="latex" src="https://s0.wp.com/latex.php?latex=L_G&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L_G"/> spectrally, they only impose some moment bounds. I will not describe them here, but feel free to read the paper. Proving that these moment bounds can be satisfied using a sparser preconditioner than those that have been so far used in the literature constitutes the technical core of the paper.</p>
<p>Just like in the past literature, these preconditioners are obtained by starting with a good tree, and sampling extra edges. Traditionally, people used low stretch spanning trees. The issue with them is that the number of edges in the preconditioner is determined by the average stretch of the tree, and we can easily check that for the square grid this is <img alt="\Omega(\log n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5COmega%28%5Clog+n%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\Omega(\log n)"/>. Unfortunately, in general we don’t know how to achieve this bound yet; the best known result is off by a <img alt="\log \log n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+%5Clog+n&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\log \log n"/> factor. It turns out that we can still get preconditioners by looking at a different quantity, called the <img alt="\ell_p" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell_p&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\ell_p"/> stretch (<img alt="0 &lt; p &lt; 1" class="latex" src="https://s0.wp.com/latex.php?latex=0+%3C+p+%3C+1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="0 &lt; p &lt; 1"/>), which can be brought down to <img alt="O(\log^p n)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28%5Clog%5Ep+n%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="O(\log^p n)"/>. This essentially eliminates the need for computing optimal low stretch spanning trees. Furthermore, these trees can be computed really fast, <img alt="O(m \log \log n)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28m+%5Clog+%5Clog+n%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="O(m \log \log n)"/> time in the RAM model, and the algorithm parallelizes.</p>
<p>This result consists of a careful combination of existing algorithms on low stretch embeddings of graphs into tree metrics and low stretch spanning trees. I will talk more about these embedding results in a future post.</p>
<p><sup id="fn1">a. <img alt="\preceq" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpreceq&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\preceq"/> is also known as the Lowner partial order. <img alt="A \preceq B" class="latex" src="https://s0.wp.com/latex.php?latex=A+%5Cpreceq+B&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="A \preceq B"/> is equivalent to <img alt="0 \preceq B-A" class="latex" src="https://s0.wp.com/latex.php?latex=0+%5Cpreceq+B-A&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="0 \preceq B-A"/>, which says that <img alt="B-A" class="latex" src="https://s0.wp.com/latex.php?latex=B-A&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="B-A"/> is PSD.<a href="https://mittheory.wordpress.com/feed/#ref1" title="Jump back to footnote a in the text."><img alt="↩" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/21a9.png" style="height: 1em;"/></a></sup><br/>
<sup id="fn2">b. A <img alt="(k, h)" class="latex" src="https://s0.wp.com/latex.php?latex=%28k%2C+h%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="(k, h)"/>-ultrasparsifier <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="H"/> of <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="G"/> is a graph with <img alt="n-k+1" class="latex" src="https://s0.wp.com/latex.php?latex=n-k%2B1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="n-k+1"/> edges such that <img alt="L_H \preceq L_G \preceq h \cdot L_H" class="latex" src="https://s0.wp.com/latex.php?latex=L_H+%5Cpreceq+L_G+%5Cpreceq+h+%5Ccdot+L_H&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L_H \preceq L_G \preceq h \cdot L_H"/>. It turns out that one is able to efficiently construct <img alt="(k, n/k\ \mathrm{polylog}(n))" class="latex" src="https://s0.wp.com/latex.php?latex=%28k%2C+n%2Fk%5C+%5Cmathrm%7Bpolylog%7D%28n%29%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="(k, n/k\ \mathrm{polylog}(n))"/> ultrasparsifiers. So by adding a few edges to a spanning tree, you can drastically reduce the relative condition number with the initial graph.<a href="https://mittheory.wordpress.com/feed/#ref2" title="Jump back to footnote b in the text."><img alt="↩" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/21a9.png" style="height: 1em;"/></a></sup></p>
<p>[1] <a href="http://arxiv.org/abs/1003.2958">Approaching optimality for solving SDD systems</a>, Koutis, Miller, Peng (2010).<br/>
[2] <a href="http://arxiv.org/abs/cs.DS/0310051">Nearly-Linear Time Algorithms for Graph Partitioning, Graph Sparsification, and Solving Linear Systems</a>, Spielman, Teng (2004).<br/>
[3] <a href="http://arxiv.org/abs/0809.3232">A Local Clustering Algorithm for Massive Graphs and its Application to Nearly-Linear Time Graph Partitioning</a>, Spielman, Teng (2013).<br/>
[4] <a href="http://arxiv.org/abs/0808.4134">Spectral Sparsification of Graphs</a>, Spielman, Teng (2011).<br/>
[5] <a href="http://arxiv.org/abs/cs.NA/0607105">Nearly-Linear Time Algorithms for Preconditioning and Solving Symmetric, Diagonally Dominant Linear Systems</a>, Spielman, Teng (2014).<br/>
[6] <a href="http://arxiv.org/abs/1301.6628">A Simple, Combinatorial Algorithm for Solving SDD Systems in Nearly-Linear Time</a>, Kelner, Orecchia, Sidford, Zhu (2013).<br/>
[7] <a href="http://arxiv.org/abs/1305.1922">Efficient Accelerated Coordinate Descent Methods and Faster Algorithms for Solving Linear Systems</a>, Lee, Sidford (2013).<br/>
[8] <a href="https://mittheory.wordpress.com/feed/A%20nearly-mlogn time solver for SDD linear systems">A nearly-mlogn time solver for SDD linear systems</a>, Koutis, Miller, Peng (2011).<br/>
[9] <a href="http://arxiv.org/abs/1209.5821">Improved Spectral Sparsification and Numerical Algorithms for SDD Matrices</a>, Koutis, Levin, Peng (2012).<br/>
[10] <a href="http://arxiv.org/abs/1402.3851">Simple parallel and distributed algorithms for spectral graph sparsification</a>, Koutis (2014).<br/>
[11] <a href="http://arxiv.org/abs/1401.6236">Preconditioning in Expectation</a>, Cohen, Kyng, Pachocki, Peng, Rao (2014).<br/>
[12] <a href="http://arxiv.org/abs/1401.2454">Stretching Stretch</a>, Cohen, Miller, Pachocki, Peng, Xu (2014).<br/>
[13] <a href="http://dl.acm.org/citation.cfm?id=2591833">Solving SDD Linear Systems in Nearly mlog1/2n Time</a>, Cohen, Kyng, Miller, Pachocki, Peng, Rao, Xu (2014).<br/>
[14] <a href="http://arxiv.org/abs/1111.1491">Approximating the Exponential, the Lanczos Method and an <img alt="\tilde{O}(m)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BO%7D%28m%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\tilde{O}(m)"/>-Time Spectral Algorithm for Balanced Separator</a>, Orecchia, Sachdeva, Vishoni (2012).</p></div></content><updated planet:format="July 08, 2014 02:46 AM">2014-07-08T02:46:37Z</updated><published planet:format="July 08, 2014 02:46 AM">2014-07-08T02:46:37Z</published><category term="algorithms"/><category term="conferences"/><category term="STOC"/><author><name>mittheory</name></author><source><id>https://mittheory.wordpress.com</id><logo>https://s0.wp.com/i/buttonw-com.png</logo><link href="https://mittheory.wordpress.com/feed/" rel="self" type="application/atom+xml"/><link href="https://mittheory.wordpress.com" rel="alternate" type="text/html"/><link href="https://mittheory.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/><link href="https://mittheory.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/><subtitle>A student blog of MIT CSAIL Theory of Computation Group</subtitle><title>Not so Great Ideas in Theoretical Computer Science</title><updated planet:format="February 28, 2020 03:25 AM">2020-02-28T03:25:10Z</updated><planet:module>toc</planet:module><planet:format>atom10</planet:format><planet:http_last_modified>Fri, 06 Sep 2019 10:00:22 GMT</planet:http_last_modified><planet:bozo>false</planet:bozo><planet:items_per_page>40</planet:items_per_page><planet:css-id>mit-csail-student-blog</planet:css-id><planet:face>csail.png</planet:face><planet:name>MIT CSAIL student blog</planet:name><planet:http_location>https://mittheory.wordpress.com/feed/</planet:http_location><planet:http_status>301</planet:http_status></source></entry>