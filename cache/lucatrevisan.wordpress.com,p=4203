<?xml version="1.0" encoding="utf-8"?><entry xml:lang="en" xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/"><id>http://lucatrevisan.wordpress.com/?p=4203</id><link href="https://lucatrevisan.wordpress.com/2018/11/08/average-case-complexity-news/" rel="alternate" type="text/html"/><title>Average Case Complexity News</title><summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Greetings from Taiwan, a tropical country with democracy, free press, and rule of law, in which the trains run on time, and the food is awesome. Also people are friendly, drivers don’t honk, and the “close doors” buttons in elevators … <a href="https://lucatrevisan.wordpress.com/2018/11/08/average-case-complexity-news/">Continue reading <span class="meta-nav">→</span></a></div><div class="commentbar"><p/></div></summary><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
 Greetings from Taiwan, a tropical country with democracy, free press, and rule of law, in which the trains run on time, and the food is awesome. Also people are friendly, drivers don’t honk, and the “close doors” buttons in elevators actually work. Talk about exceptionalism! On November 24, <em>In Theory</em> endorses voting No on 10, No on 11, No on 12, Yes on 13, Yes on 14 and Yes on 15.</p>
<p>
More on Taiwan in a later post. Today I would like to give a couple of updates on the <a href="https://www.nowpublishers.com/article/Details/TCS-004">survey paper on average-case complexity theory</a> that Andrej Bogdanov and I wrote in 2006: apologies to the readers for a number of missing references, and news on the front of worst-case to average-case reductions.</p>
<p>
<span id="more-4203"/></p>
<p>
</p><p><b>1. Addendum and missing references </b></p>
<p/><p>
In Section 2 and 3, we discuss Levin’s proof of the analog of NP-completeness for the theory of average-case complexity. Levin proves that there is a decision problem in NP, let’s call it <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>, such that if there is an “average polynomial time” algorithm for <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> with respect to the uniform distribution, then for every decision problem <img alt="{L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L}"/> in NP and for every distribution <img alt="{\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mu}"/> of instances that is “polynomial time computable” in a certain technical sense, then <img alt="{L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L}"/> also admits an “average polynomial time” algorithm with respect to the distribution <img alt="{\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mu}"/>.</p>
<p>
To prove such a result, we want to take an instance <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> sampled from <img alt="{\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mu}"/> and transform it into one (or more) instance <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/>, such that if we feed <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/> into an “average polynomial time” algorithm for problem <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> then </p>
<ol>
<li> the algorithm will run also in “average polynomial time” and
</li><li> from the answer of the algorithm we can decide whether <img alt="{x\in L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+L%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x\in L}"/> or not.
</li></ol>
<p> This is different from the Cook-Levin theorem in that we need the instance <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/> produced by the reduction to be approximately uniformly distributed, or else we are not guaranteed that the algorithm for <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> that runs in “average polynomial time” with respect to the uniform distribution will also run in “average polynomial time” in whatever is the distribution of <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/>.</p>
<p>
The key idea in Levin’s proof is to use compression: for the class of “polynomial time computable” distributions <img alt="{\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mu}"/> that he considers, there exist essentially optimal compression algorithms, such that if <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> is sampled from <img alt="{\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mu}"/> and then mapped to a compressed (but efficiently invertible) representation <img alt="{x'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x'}"/>, then <img alt="{x'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x'}"/> is “uniformly distributed.” Then one has to map <img alt="{x'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x'}"/> to <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/> in a way that does not mess up the distribution too much. (Above, “uniformly distributed” is in quotes because <img alt="{x'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x'}"/> will be a string of variable length, so the sense in which it is approximately uniformly distributed has to be defined carefully.)</p>
<p>
In order to turn this intuition into a proof, one has to give a precise definition of “average polynomial time” and of reductions between distributional problems, and prove that reductions preserve the existence of average polynomial time algorithms. These matters are quite subtle, because the first definitions that come to mind don’t quite work well, and the definitions that do work well are a bit complicated.</p>
<p>
The paper with Andrej lacks references to several important works that clarified, simplified, and generalized Levin’s original presentation. In particular, our definition of average polynomial time, and the proof that it is preserved by reductions, is due to Russell Impagliazzo, in his famous “<a href="https://ieeexplore.ieee.org/abstract/document/514853/">five worlds</a>” paper. The presentation of the completeness result follows <a href="http://www.wisdom.weizmann.ac.il/~oded/COL/lnd.pdf">notes by Oded Goldreich</a>. As in Oded’s notes, we present the completeness of a Bounded Halting problem, which we erroneously attribute to Levin, while it is due to <a href="https://www.sciencedirect.com/science/article/pii/002200009190007R">Gurevich</a>. After one proves the existence of one complete problem, one would like to see a series of reductions to natural problems, establishing more completeness results, that is, one would like the average-case complexity analog of Karp’s paper! This has not happened yet, although there have been a number of important average-case completeness results, which we also fail to cite.</p>
<p>
We have submitted to the publisher an <a href="https://people.eecs.berkeley.edu/~luca/pubs/addendum-2018.pdf">addendum to correct all the above omissions</a>. We have already privately apologized to the authors of the above works, but we would also like to publicly apologize to the readers of the survey.</p>
<p>
</p><p><b>2. Worst-case to average-case reductions </b></p>
<p/><p>
In Section 7, Andrej and I discuss the problem of constructing “worst-case to average-case reductions” for NP-complete problems. Ideally, we would like to “collapse” Levin’s theory to the standard theory of NP-completeness, and to prove that if <img alt="{P\neq NP}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%5Cneq+NP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P\neq NP}"/>, or something morally equivalent like <img alt="{NP \not\subseteq BPP}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BNP+%5Cnot%5Csubseteq+BPP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{NP \not\subseteq BPP}"/>, then Levin’s complete problem <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> does not admit an average polynomial time algorithm with respect to the uniform distribution (same for other complete distributional problems). More ambitiously, one may hope to base symmetric-key cryptography, or even public-key cryptography, on the assumption that <img alt="{NP \not\subseteq BPP}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BNP+%5Cnot%5Csubseteq+BPP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{NP \not\subseteq BPP}"/>, which, short of settling the <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> versus <img alt="{NP}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BNP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{NP}"/> question, would be the soundest possible basis on which to base cryptography. (One would also get high confidence that the resulting cryptosystems are quantum-resistant!)</p>
<p>
Indeed, there are problems, like the Permanent and (in a limited sense) the Discrete Log problem in which one can take a worst-case instance <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>, and map into to a sequence of instances <img alt="{y_1,\ldots,y_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By_1%2C%5Cldots%2Cy_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y_1,\ldots,y_k}"/>, each of which is uniformly distributed and such that solving the problem on the <img alt="{y_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y_i}"/> gives a solution for <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>. Thus, an average polynomial time algorithm for such problems, which are called random self-reducible, gives a worst-case (randomized) polynomial time algorithm.</p>
<p>
Feigenbaum and Fortnow, unfortunately, prove that if a problem in NP is random self reducible (or even randomly reducible, in a similar sense, to another problem in NP), then it is, essentially, in coNP, and so such techniques cannot apply to NP-complete problem.</p>
<p>
In 2003, Andrej Bogdanov and I generalized this to show that if one has any non-adaptive reduction from a problem <img alt="{L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L}"/> to a distributional problem <img alt="{\langle A, \mu \rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+A%2C+%5Cmu+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\langle A, \mu \rangle}"/> where <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> is in <img alt="{NP}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BNP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{NP}"/> and <img alt="{\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mu}"/> is samplable, then <img alt="{L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L}"/> is in <img alt="{coNP/poly}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BcoNP%2Fpoly%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{coNP/poly}"/> and thus unlikely to be NP-complete. Our definition of reduction allows any non-adaptive procedure that, given an oracle that solves <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> in average polynomial time with respect to <img alt="{\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mu}"/>, is able to solve <img alt="{L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L}"/> in worst-case polynomial time. </p>
<p>
It remains open whether such a result holds for adaptive reductions, and my guess is that it does. I actually had a much stronger guess, namely that for every NP problem <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> and every samplable distribution <img alt="{\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mu}"/>, solving <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> well on average with respect to <img alt="{\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mu}"/> is doable with a computational power in the ballpark of <img alt="{NP \cap coNP}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BNP+%5Ccap+coNP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{NP \cap coNP}"/>, perhaps in randomized polynomial time with oracle access to a language in <img alt="{NP/poly \cap coNP/poly}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BNP%2Fpoly+%5Ccap+coNP%2Fpoly%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{NP/poly \cap coNP/poly}"/>. If so there would be a fundamental gap between the complexity of solving NP-complete problems in the worst case versus the average case.</p>
<p>
A recent, very exciting <a href="https://eccc.weizmann.ac.il/report/2018/138/">paper by Shuichi Hirahara</a>, shows that my guess was wrong, and that there are distributional problems in NP with respect to samplable distributions, whose complexity is fundamentally higher than <img alt="{NP \cap coNP}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BNP+%5Ccap+coNP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{NP \cap coNP}"/>.</p>
<p>
Hirahara considers an NP problem called MINKT, which is the problem of computing a resource-bounded version of Kolmogorov complexity. He shows that if MINKT has an average polynomial time algorithm, then it has a ZPP worst-case algorithm (the latter is an approximation algorithm with additive error). Furthermore, under reasonable assumptions, MINKT is <em>not</em> in coNP, or coNP/poly, or in any morally equivalent class.</p>
<p>
So did Hirahara come up with an adaptive reduction, in order to get around the result of Andrej and myself? No, remarkably he comes up with an argument that <em>is not a reduction</em>! (More modestly, he says that he comes up with a <em>non-black-box</em> reduction.)</p>
<p>
To clarify terminology, I prefer to call a <em>reduction</em> (what most other people prefer to call a “black box” reduction, in this setting) a way of showing that, given an oracle for a problem <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> we show how to solve problem <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/>; hence an algorithm for <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> implies an algorithm for <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/>. </p>
<p>
This seems to be the only possible way of showing that if we have an algorithm for <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> then we have an algorithm for <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/>, but consider the following example. Say that <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> is a complete problem for <img alt="{\Sigma_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CSigma_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Sigma_2}"/>, the second level of the polynomial time hierarchy; we know that if there is a polynomial time algorithm for 3SAT then there is an algorithm for <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>, however the proof does not involve a (black box, if you must) reduction from <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> to 3SAT. In fact, we do not know how to solve <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> in polynomial time given an oracle for 3SAT and, assuming that the polynomial time hierarchy does not collapse, <em>there is no way to do that</em>!</p>
<p>
So how do we prove that if 3SAT is in polynomial time then <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> is in polynomial time? We take the <img alt="{\Sigma_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CSigma_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Sigma_2}"/> “algorithm” for <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>, which is an NP machine with oracle an NP machine, we replace calls to the oracle with executions of the 3SAT algorithm, and now we have a regular NP machine, which we can reduce to 3SAT. Note that, in the first step, we had to <em>use the code of the algorithm for 3SAT</em>, and use the assumption that it runs in polynomial time.</p>
<p>
Hirahara’s argument, likewise, after assuming that MINKT has an average polynomial time algorithm, uses the code of the algorithm and the assumption about efficiency.</p>
<p>
At this point, I do not have a good sense of the computational power that is necessary or sufficient to solve NP distributional problems well on average, but Hirahara’s paper makes me think it’s conceivable that one could base average-case hardness on <img alt="{NP\not\subseteq BPP}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BNP%5Cnot%5Csubseteq+BPP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{NP\not\subseteq BPP}"/>, a possibility that I previously considered hopeless.</p>
<p/></div></content><updated planet:format="November 09, 2018 01:50 AM">2018-11-09T01:50:02Z</updated><published planet:format="November 09, 2018 01:50 AM">2018-11-09T01:50:02Z</published><category term="theory"/><category term="average-case complexity"/><author><name>luca</name></author><source><id>https://lucatrevisan.wordpress.com</id><logo>https://s0.wp.com/i/buttonw-com.png</logo><link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/><link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/><link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/><link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/><subtitle>&quot;Marge, I agree with you - in theory. In theory, communism works. In theory.&quot; -- Homer Simpson</subtitle><title>in   theory</title><updated planet:format="December 17, 2018 05:28 AM">2018-12-17T05:28:58Z</updated><planet:module>toc</planet:module><planet:format>atom10</planet:format><planet:http_last_modified>Thu, 06 Dec 2018 16:46:05 GMT</planet:http_last_modified><planet:bozo>false</planet:bozo><planet:items_per_page>40</planet:items_per_page><planet:css-id>luca-trevisan</planet:css-id><planet:face>trevisan.jpeg</planet:face><planet:name>Luca Trevisan</planet:name><planet:http_location>https://lucatrevisan.wordpress.com/feed/</planet:http_location><planet:http_status>301</planet:http_status></source></entry>
