<?xml version="1.0" encoding="utf-8"?><entry xml:lang="en" xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/"><id>http://windowsontheory.org/?p=6294</id><link href="https://windowsontheory.org/2018/11/11/approximating-partition-functions/" rel="alternate" type="text/html"/><title>Approximating Partition Functions</title><summary>[Guest post by Alex Kelser, Alex Lin, Amil Merchant, and Suproteem Sarkar, scribing for a lecture by Andrej Risteski.] Andrej Risteski: Approximating Partition Functions via Variational Methods and Taylor Series For a probability distribution defined up to a constant of proportionality, we have already seen the partition function. To refresh your memory, given a probability […]<div class="commentbar"><p/></div></summary><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>[Guest post by Alex Kelser, Alex Lin, Amil Merchant, and Suproteem Sarkar, scribing for a lecture by Andrej Risteski.]</em></p>
<h1 id="andrej-risteski-approximating-partition-functions-via-variational-methods-and-taylor-series">Andrej Risteski: Approximating Partition Functions via Variational Methods and Taylor Series</h1>
<p>For a probability distribution defined up to a constant of proportionality, we have already seen the <strong>partition function</strong>. To refresh your memory, given a probability mass function <img alt="p(\vec{x}) \propto f(\vec{x})" src="https://latex.codecogs.com/png.latex?p%28%5Cvec%7Bx%7D%29%20%5Cpropto%20f%28%5Cvec%7Bx%7D%29" style="vertical-align: middle;" title="p(\vec{x}) \propto f(\vec{x})"/> over all <img alt="\vec{x} \in \mathcal{X}" src="https://latex.codecogs.com/png.latex?%5Cvec%7Bx%7D%20%5Cin%20%5Cmathcal%7BX%7D" style="vertical-align: middle;" title="\vec{x} \in \mathcal{X}"/>, the partition function is simply the <em>normalization constant</em> of the distribution, i.e.<br/>
<img alt="Z = \sum_{\vec{x} \in \mathcal{X}} f(\vec{x})" src="https://latex.codecogs.com/png.latex?Z%20%3D%20%5Csum_%7B%5Cvec%7Bx%7D%20%5Cin%20%5Cmathcal%7BX%7D%7D%20f%28%5Cvec%7Bx%7D%29" style="vertical-align: middle;" title="Z = \sum_{\vec{x} \in \mathcal{X}} f(\vec{x})"/></p>
<p>At first glance, the partition function may appear to be uninteresting. However, upon taking a deeper look, this single quantity holds a wide array of intriguing and complex properties. For one thing, its significance in the world of thermodynamics cannot be overstated. The partition function is at the heart of relating the microscopic quantities of a system – such as the individual energies of each probabilistic state <img alt="\vec{x} \in \mathcal{X}" src="https://latex.codecogs.com/png.latex?%5Cvec%7Bx%7D%20%5Cin%20%5Cmathcal%7BX%7D" style="vertical-align: middle;" title="\vec{x} \in \mathcal{X}"/> – to macroscopic entities describing the entire system: the total energy, the energy fluctuation, the heat capacity, and the free energy. For explicit formulas, consult <a href="https://pdfs.semanticscholar.org/456b/b94e824c2637f763233dd3110ba71c900011.pdf">this source</a> for reference. In machine learning the partition function also holds much significance, since it’s intimately linked to computing marginals in the model.</p>
<p>Although there exists an explicit formula for the partition function, the challenge lies in computing the quantity in polynomial time. Suppose <img alt="\vec{x}" src="https://latex.codecogs.com/png.latex?%5Cvec%7Bx%7D" style="vertical-align: middle;" title="\vec{x}"/> is a vector of dimension <img alt="n" src="https://latex.codecogs.com/png.latex?n" style="vertical-align: middle;" title="n"/> where each <img alt="x_i \in \{-1, 1\}" src="https://latex.codecogs.com/png.latex?x_i%20%5Cin%20%5C%7B-1%2C%201%5C%7D" style="vertical-align: middle;" title="x_i \in \{-1, 1\}"/> for <img alt="i \in \{1, \ldots, n\}" src="https://latex.codecogs.com/png.latex?i%20%5Cin%20%5C%7B1%2C%20%5Cldots%2C%20n%5C%7D" style="vertical-align: middle;" title="i \in \{1, \ldots, n\}"/>. Specifically, we would like a smarter way of finding <img alt="Z" src="https://latex.codecogs.com/png.latex?Z" style="vertical-align: middle;" title="Z"/> than simply adding up <img alt="f(x)" src="https://latex.codecogs.com/png.latex?f%28x%29" style="vertical-align: middle;" title="f(x)"/> over all <img alt="2^n" src="https://latex.codecogs.com/png.latex?2%5En" style="vertical-align: middle;" title="2^n"/> combinations of <img alt="x" src="https://latex.codecogs.com/png.latex?x" style="vertical-align: middle;" title="x"/>.</p>
<p>Andrej Risteski’s lecture focused on using two general techniques – variational methods and Taylor series approximations – to find provably approximate estimates of <img alt="Z" src="https://latex.codecogs.com/png.latex?Z" style="vertical-align: middle;" title="Z"/>.</p>
<h1 id="motivation">Motivation</h1>
<p>The general setup addresses undirected graphical models, also known as a Markov random fields (MRF), where the probability mass function <img alt="p : \mathcal{X} \to [0, 1]" src="https://latex.codecogs.com/png.latex?p%20%3A%20%5Cmathcal%7BX%7D%20%5Cto%20%5B0%2C%201%5D" style="vertical-align: middle;" title="p : \mathcal{X} \to [0, 1]"/> has the form<br/>
<img alt=" p(\vec{x}) \propto \exp \left( \sum_{i \sim j} \phi_{ij}(x_i,x_j) \right) " src="https://latex.codecogs.com/png.latex?%20p%28%5Cvec%7Bx%7D%29%20%5Cpropto%20%5Cexp%20%5Cleft%28%20%5Csum_%7Bi%20%5Csim%20j%7D%20%5Cphi_%7Bij%7D%28x_i%2Cx_j%29%20%5Cright%29%20" style="vertical-align: middle;" title=" p(\vec{x}) \propto \exp \left( \sum_{i \sim j} \phi_{ij}(x_i,x_j) \right) "/><br/>
for some random, <img alt="n" src="https://latex.codecogs.com/png.latex?n" style="vertical-align: middle;" title="n"/>-dimensional vector <img alt="\vec{x} \in \mathcal{X}" src="https://latex.codecogs.com/png.latex?%5Cvec%7Bx%7D%20%5Cin%20%5Cmathcal%7BX%7D" style="vertical-align: middle;" title="\vec{x} \in \mathcal{X}"/> and some set of parameterized functions <img alt="\{\phi_{ij}: \mathcal{X} \to \mathcal{R}\}" src="https://latex.codecogs.com/png.latex?%5C%7B%5Cphi_%7Bij%7D%3A%20%5Cmathcal%7BX%7D%20%5Cto%20%5Cmathcal%7BR%7D%5C%7D" style="vertical-align: middle;" title="\{\phi_{ij}: \mathcal{X} \to \mathcal{R}\}"/>. Note that the notation <img alt="i \sim j" src="https://latex.codecogs.com/png.latex?i%20%5Csim%20j" style="vertical-align: middle;" title="i \sim j"/> denotes all pairs <img alt="(i, j)" src="https://latex.codecogs.com/png.latex?%28i%2C%20j%29" style="vertical-align: middle;" title="(i, j)"/> where <img alt="i, j \in \{1, 2, \ldots, n\}" src="https://latex.codecogs.com/png.latex?i%2C%20j%20%5Cin%20%5C%7B1%2C%202%2C%20%5Cldots%2C%20n%5C%7D" style="vertical-align: middle;" title="i, j \in \{1, 2, \ldots, n\}"/> and the edge <img alt="e = (i, j)" src="https://latex.codecogs.com/png.latex?e%20%3D%20%28i%2C%20j%29" style="vertical-align: middle;" title="e = (i, j)"/> exists in the graph.</p>
<p>The talk considered the specific setup where each <img alt="x_i \in \{-1, 1\}" src="https://latex.codecogs.com/png.latex?x_i%20%5Cin%20%5C%7B-1%2C%201%5C%7D" style="vertical-align: middle;" title="x_i \in \{-1, 1\}"/>, so <img alt="\mathcal{X} = \{-1, 1\}^n" src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BX%7D%20%3D%20%5C%7B-1%2C%201%5C%7D%5En" style="vertical-align: middle;" title="\mathcal{X} = \{-1, 1\}^n"/>. Also, we fix</p>
<p><img alt="\phi_{ij}(x_i, x_j) = J_{ij} x_i x_j" src="https://latex.codecogs.com/png.latex?%5Cphi_%7Bij%7D%28x_i%2C%20x_j%29%20%3D%20J_%7Bij%7D%20x_i%20x_j" style="vertical-align: middle;" title="\phi_{ij}(x_i, x_j) = J_{ij} x_i x_j"/><br/>
for some set of coefficients <img alt="\{J_{ij}\}" src="https://latex.codecogs.com/png.latex?%5C%7BJ_%7Bij%7D%5C%7D" style="vertical-align: middle;" title="\{J_{ij}\}"/>, thereby giving us the well-known <a href="https://en.wikipedia.org/wiki/Ising_model">Ising model</a>. Note, the interaction terms could be more complicated and made “less local” if desired, but that was not discussed in the lecture.</p>
<p>These graphical models are common in machine learning, where there are two common tasks of interest:</p>
<ol type="1">
<li><strong>Learning</strong>: given samples, models try to learn <img alt="\phi_{ij}(x_i, x_j)" src="https://latex.codecogs.com/png.latex?%5Cphi_%7Bij%7D%28x_i%2C%20x_j%29" style="vertical-align: middle;" title="\phi_{ij}(x_i, x_j)"/> or <img alt="\{J_{ij}\}" src="https://latex.codecogs.com/png.latex?%5C%7BJ_%7Bij%7D%5C%7D" style="vertical-align: middle;" title="\{J_{ij}\}"/></li>
<li><strong>Inference</strong>: given <img alt="\{\phi_{ij}\}" src="https://latex.codecogs.com/png.latex?%5C%7B%5Cphi_%7Bij%7D%5C%7D" style="vertical-align: middle;" title="\{\phi_{ij}\}"/> or the potentials <img alt="\{J_{ij}\}" src="https://latex.codecogs.com/png.latex?%5C%7BJ_%7Bij%7D%5C%7D" style="vertical-align: middle;" title="\{J_{ij}\}"/> as input, we want to calculate the marginal probabilities, such as<br/>
<img alt="p(x_i = -1) \text{ or }  p(x_i = 1, x_j = -1) " src="https://latex.codecogs.com/png.latex?p%28x_i%20%3D%20-1%29%20%5Ctext%7B%20or%20%7D%20%20p%28x_i%20%3D%201%2C%20x_j%20%3D%20-1%29%20" style="vertical-align: middle;" title="p(x_i = -1) \text{ or }  p(x_i = 1, x_j = -1) "/></li>
</ol>
<p>We focus on the latter task. The problem of inference is closely related to calculating the <strong>partition function</strong>. This value is often used as the normalization constant in many methods, and it is classically defined as<br/>
<img alt=" Z = \sum_{x \in \mathcal{X}} \exp \left( \sum_{i \sim j} \phi_{ij}(x_i, x_j) \right)" src="https://latex.codecogs.com/png.latex?%20Z%20%3D%20%5Csum_%7Bx%20%5Cin%20%5Cmathcal%7BX%7D%7D%20%5Cexp%20%5Cleft%28%20%5Csum_%7Bi%20%5Csim%20j%7D%20%5Cphi_%7Bij%7D%28x_i%2C%20x_j%29%20%5Cright%29" style="vertical-align: middle;" title=" Z = \sum_{x \in \mathcal{X}} \exp \left( \sum_{i \sim j} \phi_{ij}(x_i, x_j) \right)"/></p>
<p>Although we can write down the aforementioned closed-form expression for the partition function, it is difficult to calculate this quantity in polynomial time. There are two broad approaches to solving inference problems:</p>
<ol type="1">
<li><strong>Randomized</strong>: Methods based on Markov chain Monte Carlo (MCMC), such as Gibbs sampling, that construct a Markov chain whose stationary distribution is the distribution of interest. These have been more extensively studied in theoretical computer science.</li>
<li><strong>Deterministic</strong>: Variational methods, self-avoiding walk trees, Taylor expansion methods</li>
</ol>
<p>Although more often used in practice, randomized algorithms such as MCMC are notoriously hard to debug, and it is often unclear at what point the chain reaches stationarity.</p>
<p>In contrast, variational methods are often more difficult to rigorously analyze but have the added benefit of turning inference problems into optimization problems. Risteski’s talk considered some provable instantiations of variational methods for calculating the partition function.</p>
<h1 id="variational-methods">Variational Methods</h1>
<p>Let us start with a basic observation, known as the Gibbs variational principle. This can be stated as the following.</p>
<p><strong>Lemma 1</strong>: Let <img alt="p(\vec{x}) \propto \exp{\sum_{i \sim j} J_{ij} x_i x_j}" src="https://latex.codecogs.com/png.latex?p%28%5Cvec%7Bx%7D%29%20%5Cpropto%20%5Cexp%7B%5Csum_%7Bi%20%5Csim%20j%7D%20J_%7Bij%7D%20x_i%20x_j%7D" style="vertical-align: middle;" title="p(\vec{x}) \propto \exp{\sum_{i \sim j} J_{ij} x_i x_j}"/>. Then, we can show that the corresponding partition function satisfies</p>
<p><img alt=" \log Z = \max_{q \in \mathcal{Q}} \left \{ \sum_{i \sim j} \underbrace{J_{ij}\mathbb{E}_q[x_i x_j]}_{\text{energy term}} + \underbrace{\mathbb{H}(q)}_{\text{entropy term}} \right \}  " src="https://latex.codecogs.com/png.latex?%0A%5Clog%20Z%20%3D%20%5Cmax_%7Bq%20%5Cin%20%5Cmathcal%7BQ%7D%7D%20%5Cleft%20%5C%7B%20%5Csum_%7Bi%20%5Csim%20j%7D%20%5Cunderbrace%7BJ_%7Bij%7D%5Cmathbb%7BE%7D_q%5Bx_i%20x_j%5D%7D_%7B%5Ctext%7Benergy%20term%7D%7D%20%2B%20%5Cunderbrace%7B%5Cmathbb%7BH%7D%28q%29%7D_%7B%5Ctext%7Bentropy%20term%7D%7D%20%5Cright%20%5C%7D%20%0A" style="vertical-align: middle;" title=" \log Z = \max_{q \in \mathcal{Q}} \left \{ \sum_{i \sim j} \underbrace{J_{ij}\mathbb{E}_q[x_i x_j]}_{\text{energy term}} + \underbrace{\mathbb{H}(q)}_{\text{entropy term}} \right \}  "/></p>
<p>where <img alt="q : \mathcal{X} \to [0, 1]" src="https://latex.codecogs.com/png.latex?q%20%3A%20%5Cmathcal%7BX%7D%20%5Cto%20%5B0%2C%201%5D" style="vertical-align: middle;" title="q : \mathcal{X} \to [0, 1]"/> is defined to be a valid distribution over <img alt="\mathcal{X}" src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BX%7D" style="vertical-align: middle;" title="\mathcal{X}"/> and <img alt="\mathcal{Q}" src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BQ%7D" style="vertical-align: middle;" title="\mathcal{Q}"/> is the set of all such distributions.</p>
<p><em>Note that we use <img alt="\mathbb{E}_q(\cdot)" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_q%28%5Ccdot%29" style="vertical-align: middle;" title="\mathbb{E}_q(\cdot)"/> to denote the expectation of the inner argument with respect to the distribution <img alt="q" src="https://latex.codecogs.com/png.latex?q" style="vertical-align: middle;" title="q"/> and <img alt="\mathbb{H}(q)" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D%28q%29" style="vertical-align: middle;" title="\mathbb{H}(q)"/> to denote the Shannon entropy of <img alt="q" src="https://latex.codecogs.com/png.latex?q" style="vertical-align: middle;" title="q"/>.</em></p>
<p><strong>Proof</strong>: For any <img alt="q \in \mathcal{Q}" src="https://latex.codecogs.com/png.latex?q%20%5Cin%20%5Cmathcal%7BQ%7D" style="vertical-align: middle;" title="q \in \mathcal{Q}"/>, the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL divergence</a> between <img alt="q" src="https://latex.codecogs.com/png.latex?q" style="vertical-align: middle;" title="q"/> and <img alt="p" src="https://latex.codecogs.com/png.latex?p" style="vertical-align: middle;" title="p"/> must be greater than or equal to 0.</p>
<p><img alt="    \mathbb{KL} (q || p) \geq 0 " src="https://latex.codecogs.com/png.latex?%20%20%20%20%5Cmathbb%7BKL%7D%20%28q%20%7C%7C%20p%29%20%5Cgeq%200%20" style="vertical-align: middle;" title="    \mathbb{KL} (q || p) \geq 0 "/></p>
<p><img alt=" \underbrace{\mathbb{E}_q [\log q(\vec{x})}_{-\mathbb{H}(q)}] - \mathbb{E}_q \left[ \sum_{i \sim j} J_{ij} x_i x_j \right] + \log Z \geq 0 " src="https://latex.codecogs.com/png.latex?%20%5Cunderbrace%7B%5Cmathbb%7BE%7D_q%20%5B%5Clog%20q%28%5Cvec%7Bx%7D%29%7D_%7B-%5Cmathbb%7BH%7D%28q%29%7D%5D%20-%20%5Cmathbb%7BE%7D_q%20%5Cleft%5B%20%5Csum_%7Bi%20%5Csim%20j%7D%20J_%7Bij%7D%20x_i%20x_j%20%5Cright%5D%20%2B%20%5Clog%20Z%20%5Cgeq%200%20" style="vertical-align: middle;" title=" \underbrace{\mathbb{E}_q [\log q(\vec{x})}_{-\mathbb{H}(q)}] - \mathbb{E}_q \left[ \sum_{i \sim j} J_{ij} x_i x_j \right] + \log Z \geq 0 "/></p>
<p><img alt="\log Z \geq \sum_{i \sim j} J_{ij} \mathbb{E}_q[x_i x_j] + \mathbb{H}(q)" src="https://latex.codecogs.com/png.latex?%5Clog%20Z%20%5Cgeq%20%5Csum_%7Bi%20%5Csim%20j%7D%20J_%7Bij%7D%20%5Cmathbb%7BE%7D_q%5Bx_i%20x_j%5D%20%2B%20%5Cmathbb%7BH%7D%28q%29" style="vertical-align: middle;" title="\log Z \geq \sum_{i \sim j} J_{ij} \mathbb{E}_q[x_i x_j] + \mathbb{H}(q)"/><br/>
Equality holds if <img alt="q = p" src="https://latex.codecogs.com/png.latex?q%20%3D%20p" style="vertical-align: middle;" title="q = p"/>, leading directly to the lemma above.</p>
<p><em>Note that the proof would have also held in the more generic exponential family with <img alt="\phi_{ij}" src="https://latex.codecogs.com/png.latex?%5Cphi_%7Bij%7D" style="vertical-align: middle;" title="\phi_{ij}"/> instead of <img alt="J_{ij} x_i x_j" src="https://latex.codecogs.com/png.latex?J_%7Bij%7D%20x_i%20x_j" style="vertical-align: middle;" title="J_{ij} x_i x_j"/>. Also, at most temperatures, one of the 2 terms, either the energy or the entropy will often dominate.</em></p>
<p><strong>As a result of this lemma, we have framed the original inference problem of calculating <img alt="Z" src="https://latex.codecogs.com/png.latex?Z" style="vertical-align: middle;" title="Z"/> as an optimization problem over <img alt="\mathcal{Q}" src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BQ%7D" style="vertical-align: middle;" title="\mathcal{Q}"/>. This is the crux of variational inference.</strong></p>
<h2 id="optimization-difficulties">Optimization Difficulties</h2>
<p>The issue with this approach is that it is difficult to optimize over the polytope of distributions due to the values of each <img alt="x_i" src="https://latex.codecogs.com/png.latex?x_i" style="vertical-align: middle;" title="x_i"/> coming from <img alt="\pm 1" src="https://latex.codecogs.com/png.latex?%5Cpm%201" style="vertical-align: middle;" title="\pm 1"/>. In general, this is <em>NOT</em> tractable. We can imagine two possible solutions:</p>
<ol type="1">
<li>Inner Approximation
<p>Instead of optimizing over all <img alt="q" src="https://latex.codecogs.com/png.latex?q" style="vertical-align: middle;" title="q"/>, pick a “nice” subfamily of distributions to constrain <img alt="q" src="https://latex.codecogs.com/png.latex?q" style="vertical-align: middle;" title="q"/>. The prototypical example is the mean-field approximation in which we let <img alt="q(\vec{x}) = \prod_{i=1}^n q_i(x_i)" src="https://latex.codecogs.com/png.latex?q%28%5Cvec%7Bx%7D%29%20%3D%20%5Cprod_%7Bi%3D1%7D%5En%20q_i%28x_i%29" style="vertical-align: middle;" title="q(\vec{x}) = \prod_{i=1}^n q_i(x_i)"/>, where each <img alt="q_i : \{-1, 1\} \to [0, 1]" src="https://latex.codecogs.com/png.latex?q_i%20%3A%20%5C%7B-1%2C%201%5C%7D%20%5Cto%20%5B0%2C%201%5D" style="vertical-align: middle;" title="q_i : \{-1, 1\} \to [0, 1]"/> is a univariate distribution. Thus, we approximate <img alt="\log Z" src="https://latex.codecogs.com/png.latex?%5Clog%20Z" style="vertical-align: middle;" title="\log Z"/> with<br/>
<img alt="\max_{q = \prod_i q_i} \left \{ \sum_{i \sim j} J_{ij} \cdot \mathbb{E}_{q_i}[x_i] \cdot \mathbb{E}_{q_j}[x_j] + \sum_i \mathbb{H}(q_i) \right \}" src="https://latex.codecogs.com/png.latex?%5Cmax_%7Bq%20%3D%20%5Cprod_i%20q_i%7D%20%5Cleft%20%5C%7B%20%5Csum_%7Bi%20%5Csim%20j%7D%20J_%7Bij%7D%20%5Ccdot%20%5Cmathbb%7BE%7D_%7Bq_i%7D%5Bx_i%5D%20%5Ccdot%20%5Cmathbb%7BE%7D_%7Bq_j%7D%5Bx_j%5D%20%2B%20%5Csum_i%20%5Cmathbb%7BH%7D%28q_i%29%20%5Cright%20%5C%7D" style="vertical-align: middle;" title="\max_{q = \prod_i q_i} \left \{ \sum_{i \sim j} J_{ij} \cdot \mathbb{E}_{q_i}[x_i] \cdot \mathbb{E}_{q_j}[x_j] + \sum_i \mathbb{H}(q_i) \right \}"/></p>
<p>This would provide a <em>lower</em> bound on <img alt="Z" src="https://latex.codecogs.com/png.latex?Z" style="vertical-align: middle;" title="Z"/>. There are a number of potential issues with this method. For one thing, the functions are typically non-convex. Even ignoring this problem, it is difficult to quantify how good the approximation is.</p></li>
<li>Outer Approximation
<p><em>Note: Given a distribution <img alt="q" src="https://latex.codecogs.com/png.latex?q" style="vertical-align: middle;" title="q"/>, we use <img alt="q_S" src="https://latex.codecogs.com/png.latex?q_S" style="vertical-align: middle;" title="q_S"/> to denote the marginal <img alt="q(\vec{x}_S) = q(x_{s_1}, \ldots, x_{s_k})" src="https://latex.codecogs.com/png.latex?q%28%5Cvec%7Bx%7D_S%29%20%3D%20q%28x_%7Bs_1%7D%2C%20%5Cldots%2C%20x_%7Bs_k%7D%29" style="vertical-align: middle;" title="q(\vec{x}_S) = q(x_{s_1}, \ldots, x_{s_k})"/> for some set <img alt="S = \{s_1, \ldots, s_k\} \subseteq \{1, \ldots, n\}" src="https://latex.codecogs.com/png.latex?S%20%3D%20%5C%7Bs_1%2C%20%5Cldots%2C%20s_k%5C%7D%20%5Csubseteq%20%5C%7B1%2C%20%5Cldots%2C%20n%5C%7D" style="vertical-align: middle;" title="S = \{s_1, \ldots, s_k\} \subseteq \{1, \ldots, n\}"/>.</em></p>
<p>In the outer approximation, we relax the polytope we are optimizing over using convex hierarchies. For instance, we can define <img alt="\mathcal{M}^k" src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D%5Ek" style="vertical-align: middle;" title="\mathcal{M}^k"/> as the polytope containing valid marginals <img alt="q_S" src="https://latex.codecogs.com/png.latex?q_S" style="vertical-align: middle;" title="q_S"/> over subsets <img alt="S" src="https://latex.codecogs.com/png.latex?S" style="vertical-align: middle;" title="S"/> of size at most <img alt="k" src="https://latex.codecogs.com/png.latex?k" style="vertical-align: middle;" title="k"/>. We can then reformulate our objective as a two-part problem of (1) finding a set of pairwise marginals <img alt="\widetilde{q} = \{\widetilde{q}_S \text{ s.t. } \lvert S \rvert \leq k\}\in \mathcal{M}^k" src="https://latex.codecogs.com/png.latex?%5Cwidetilde%7Bq%7D%20%3D%20%5C%7B%5Cwidetilde%7Bq%7D_S%20%5Ctext%7B%20s.t.%20%7D%20%5Clvert%20S%20%5Crvert%20%5Cleq%20k%5C%7D%5Cin%20%5Cmathcal%7BM%7D%5Ek" style="vertical-align: middle;" title="\widetilde{q} = \{\widetilde{q}_S \text{ s.t. } \lvert S \rvert \leq k\}\in \mathcal{M}^k"/> that optimizes the energy term and (2) finding a distribution <img alt="q" src="https://latex.codecogs.com/png.latex?q" style="vertical-align: middle;" title="q"/> with pairwise marginals matching <img alt="\widetilde{q}" src="https://latex.codecogs.com/png.latex?%5Cwidetilde%7Bq%7D" style="vertical-align: middle;" title="\widetilde{q}"/> that optimizes the entropy term. For simplicity of notation, we use <img alt="\widetilde{q}_{ij}" src="https://latex.codecogs.com/png.latex?%5Cwidetilde%7Bq%7D_%7Bij%7D" style="vertical-align: middle;" title="\widetilde{q}_{ij}"/> to denote <img alt="\widetilde{q}_{S}" src="https://latex.codecogs.com/png.latex?%5Cwidetilde%7Bq%7D_%7BS%7D" style="vertical-align: middle;" title="\widetilde{q}_{S}"/>, where <img alt="S = \{i, j\}" src="https://latex.codecogs.com/png.latex?S%20%3D%20%5C%7Bi%2C%20j%5C%7D" style="vertical-align: middle;" title="S = \{i, j\}"/>. It follows that we can rewrite the Gibbs equation as</p>
<p><img alt="\log Z = \max_{\widetilde{q} \in \mathcal{M}^k} \left \{ \sum_{i \sim j} J_{ij} \cdot \mathbb{E}_{\widetilde{q}_{ij}}[x_i x_j] + \max_{q \given q_S = \widetilde{q}_S \forall S, \lvert S \rvert \leq k} \mathbb{H}(q) \right \}" src="https://latex.codecogs.com/png.latex?%5Clog%20Z%20%3D%20%5Cmax_%7B%5Cwidetilde%7Bq%7D%20%5Cin%20%5Cmathcal%7BM%7D%5Ek%7D%20%5Cleft%20%5C%7B%20%5Csum_%7Bi%20%5Csim%20j%7D%20J_%7Bij%7D%20%5Ccdot%20%5Cmathbb%7BE%7D_%7B%5Cwidetilde%7Bq%7D_%7Bij%7D%7D%5Bx_i%20x_j%5D%20%2B%20%5Cmax_%7Bq%20%5Cgiven%20q_S%20%3D%20%5Cwidetilde%7Bq%7D_S%20%5Cforall%20S%2C%20%5Clvert%20S%20%5Crvert%20%5Cleq%20k%7D%20%5Cmathbb%7BH%7D%28q%29%20%5Cright%20%5C%7D" style="vertical-align: middle;" title="\log Z = \max_{\widetilde{q} \in \mathcal{M}^k} \left \{ \sum_{i \sim j} J_{ij} \cdot \mathbb{E}_{\widetilde{q}_{ij}}[x_i x_j] + \max_{q \given q_S = \widetilde{q}_S \forall S, \lvert S \rvert \leq k} \mathbb{H}(q) \right \}"/></p>
<p>The first term here represents the energy on pairwise marginals, whereas the second term is the maximum entropy subject to a constraint about matching the energy distribution’s pairwise marginals. The goal of this procedure is to enlarge the polytope such that <img alt="\mathcal{M}^k" src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D%5Ek" style="vertical-align: middle;" title="\mathcal{M}^k"/> is a tractable set, where we can impose a polynomial number of constraints satisfied by real marginals.</p>
<p>One specific convex hierarchy that is commonly used for relaxation is the <a href="http://www.cs.princeton.edu/courses/archive/spr05/cos598B/liftproj.pdf">Sherali-Adams (SA) hierarchy</a>. Sherali-Adams will allow us to formulate the optimization of the energy term (and an approximation of the entropy term) as a convex program. We introduce the polytope <img alt="\text{SA}(k)" src="https://latex.codecogs.com/png.latex?%5Ctext%7BSA%7D%28k%29" style="vertical-align: middle;" title="\text{SA}(k)"/> for <img alt="k \geq 2" src="https://latex.codecogs.com/png.latex?k%20%5Cgeq%202" style="vertical-align: middle;" title="k \geq 2"/>, which will relax the constraints on <img alt="\vec{x}" src="https://latex.codecogs.com/png.latex?%5Cvec%7Bx%7D" style="vertical-align: middle;" title="\vec{x}"/> to allow for values outside of <img alt="\{-1, 1\}^n" src="https://latex.codecogs.com/png.latex?%5C%7B-1%2C%201%5C%7D%5En" style="vertical-align: middle;" title="\{-1, 1\}^n"/> in order to generate a polynomial-time solution for <img alt="\log Z" src="https://latex.codecogs.com/png.latex?%5Clog%20Z" style="vertical-align: middle;" title="\log Z"/>.</p>
<p>The Sherali-Adams hierarchy will take care of the energy term, but it remains unclear how to rewrite the entropy term in the context of the convex program. In fact, we will need approximations for <img alt="\mathbb{H}" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D" style="vertical-align: middle;" title="\mathbb{H}"/> in order to accomplish this task.</p>
<p>In this talk, we’ll consider two approximations: one classical one – the <strong>Bethe entropy</strong> <img alt="\mathbb{H}_\text{BETHE}" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D_%5Ctext%7BBETHE%7D" style="vertical-align: middle;" title="\mathbb{H}_\text{BETHE}"/> and one more recent one – the <strong>augmented mean-field pseudo-entropy</strong> <img alt="\mathbb{H}_\text{aMF}" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D_%5Ctext%7BaMF%7D" style="vertical-align: middle;" title="\mathbb{H}_\text{aMF}"/>.</p></li>
</ol>
<h2 id="bethe-entropy-approximation">Bethe Entropy Approximation</h2>
<p>The Bethe entropy works by pretending that the Markov random field is a tree. In fact, we can show that if the graph <em>is</em> a tree, then using the Bethe entropy approximation in the convex program defined by <img alt="\text{SA}(2)" src="https://latex.codecogs.com/png.latex?%5Ctext%7BSA%7D%282%29" style="vertical-align: middle;" title="\text{SA}(2)"/> will yield an <em>exact calculation</em> of <img alt="\log Z" src="https://latex.codecogs.com/png.latex?%5Clog%20Z" style="vertical-align: middle;" title="\log Z"/>.</p>
<p>Specifically, the Bethe entropy is defined as<br/>
<img alt="\mathbb{H}_\text{BETHE} (\widetilde{q}) = \sum_{i \sim j} \mathbb{H}(\widetilde{q}_{ij}) - \sum_i \mathbb{H}(\widetilde{q}_i) (d_i - 1)" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D_%5Ctext%7BBETHE%7D%20%28%5Cwidetilde%7Bq%7D%29%20%3D%20%5Csum_%7Bi%20%5Csim%20j%7D%20%5Cmathbb%7BH%7D%28%5Cwidetilde%7Bq%7D_%7Bij%7D%29%20-%20%5Csum_i%20%5Cmathbb%7BH%7D%28%5Cwidetilde%7Bq%7D_i%29%20%28d_i%20-%201%29" style="vertical-align: middle;" title="\mathbb{H}_\text{BETHE} (\widetilde{q}) = \sum_{i \sim j} \mathbb{H}(\widetilde{q}_{ij}) - \sum_i \mathbb{H}(\widetilde{q}_i) (d_i - 1)"/><br/>
where <img alt="d_i" src="https://latex.codecogs.com/png.latex?d_i" style="vertical-align: middle;" title="d_i"/> is defined to be the degree of the particular vertex <img alt="i" src="https://latex.codecogs.com/png.latex?i" style="vertical-align: middle;" title="i"/>. Note that there are no marginals over sets of dimension greater than two in this expression; thus, we have a valid convex program over the polytope <img alt="\text{SA}(2)" src="https://latex.codecogs.com/png.latex?%5Ctext%7BSA%7D%282%29" style="vertical-align: middle;" title="\text{SA}(2)"/>.</p>
<p>This lemma is well-lnown, but it will be a useful warmup:</p>
<p><strong>Lemma 2</strong> Define the output of the convex program<br/>
<img alt="\log Z_{\text{BETHE}} =      \max_{\widetilde{q} \in \text{SA}(2)} \left \{ \sum_{i \sim j} J_{ij} \cdot \mathbb{E}_{\widetilde{q}_{ij}} [x_i x_j] + \mathbb{H}_{\text{BETHE}} (\widetilde{q}) \right \}" src="https://latex.codecogs.com/png.latex?%5Clog%20Z_%7B%5Ctext%7BBETHE%7D%7D%20%3D%20%0A%20%20%20%20%5Cmax_%7B%5Cwidetilde%7Bq%7D%20%5Cin%20%5Ctext%7BSA%7D%282%29%7D%20%5Cleft%20%5C%7B%20%5Csum_%7Bi%20%5Csim%20j%7D%20J_%7Bij%7D%20%5Ccdot%20%5Cmathbb%7BE%7D_%7B%5Cwidetilde%7Bq%7D_%7Bij%7D%7D%20%5Bx_i%20x_j%5D%20%2B%20%5Cmathbb%7BH%7D_%7B%5Ctext%7BBETHE%7D%7D%20%28%5Cwidetilde%7Bq%7D%29%20%5Cright%20%5C%7D" style="vertical-align: middle;" title="\log Z_{\text{BETHE}} =      \max_{\widetilde{q} \in \text{SA}(2)} \left \{ \sum_{i \sim j} J_{ij} \cdot \mathbb{E}_{\widetilde{q}_{ij}} [x_i x_j] + \mathbb{H}_{\text{BETHE}} (\widetilde{q}) \right \}"/><br/>
On a tree, <img alt="\log Z_{\text{BETHE}} = \log Z" src="https://latex.codecogs.com/png.latex?%5Clog%20Z_%7B%5Ctext%7BBETHE%7D%7D%20%3D%20%5Clog%20Z" style="vertical-align: middle;" title="\log Z_{\text{BETHE}} = \log Z"/> and the optimization objective is concave with respect to the <img alt="\text{SA}(2)" src="https://latex.codecogs.com/png.latex?%5Ctext%7BSA%7D%282%29" style="vertical-align: middle;" title="\text{SA}(2)"/> variables, so it can be solved in polynomial time.</p>
<p><strong>Proof sketch</strong> We will prove 3 claims:</p>
<ol type="1">
<li><img alt="\log Z_{\text{BETHE}} \geq \log Z" src="https://latex.codecogs.com/png.latex?%5Clog%20Z_%7B%5Ctext%7BBETHE%7D%7D%20%5Cgeq%20%5Clog%20Z" style="vertical-align: middle;" title="\log Z_{\text{BETHE}} \geq \log Z"/>
<p>Since the energy term is exact, it suffices to show that for valid marginals <img alt="\widetilde{q}_S" src="https://latex.codecogs.com/png.latex?%5Cwidetilde%7Bq%7D_S" style="vertical-align: middle;" title="\widetilde{q}_S"/>, <img alt="\mathbb{H}_\text{BETHE}(\widetilde{q}) \geq \max_{q \given q_S = \widetilde{q}_S} \mathbb{H}(q)" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D_%5Ctext%7BBETHE%7D%28%5Cwidetilde%7Bq%7D%29%20%5Cgeq%20%5Cmax_%7Bq%20%5Cgiven%20q_S%20%3D%20%5Cwidetilde%7Bq%7D_S%7D%20%5Cmathbb%7BH%7D%28q%29" style="vertical-align: middle;" title="\mathbb{H}_\text{BETHE}(\widetilde{q}) \geq \max_{q \given q_S = \widetilde{q}_S} \mathbb{H}(q)"/>. This can be done by re-writing <img alt="\mathbb{H}_\text{BETHE}" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D_%5Ctext%7BBETHE%7D" style="vertical-align: middle;" title="\mathbb{H}_\text{BETHE}"/> and using a property of conditional entropy, namely that <img alt="\mathbb{H}(X,Y) = \mathbb{H}(X) + \mathbb{H}(Y|X)" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D%28X%2CY%29%20%3D%20%5Cmathbb%7BH%7D%28X%29%20%2B%20%5Cmathbb%7BH%7D%28Y%7CX%29" style="vertical-align: middle;" title="\mathbb{H}(X,Y) = \mathbb{H}(X) + \mathbb{H}(Y|X)"/>.</p></li>
<li>For trees, <img alt="\log Z_\text{BETHE}" src="https://latex.codecogs.com/png.latex?%5Clog%20Z_%5Ctext%7BBETHE%7D" style="vertical-align: middle;" title="\log Z_\text{BETHE}"/> is concave in the <img alt="\text{SA}(2)" src="https://latex.codecogs.com/png.latex?%5Ctext%7BSA%7D%282%29" style="vertical-align: middle;" title="\text{SA}(2)"/> variables.
<p>This proof was only sketched in class but is similar to the usual proof of concavity of entropy.</p></li>
<li>For trees, we can <em>round</em> a solution to a proper distribution over <img alt="\{-1, 1\}^n" src="https://latex.codecogs.com/png.latex?%5C%7B-1%2C%201%5C%7D%5En" style="vertical-align: middle;" title="\{-1, 1\}^n"/> which attains the same value for the original <img alt="\log Z" src="https://latex.codecogs.com/png.latex?%5Clog%20Z" style="vertical-align: middle;" title="\log Z"/> optimization.
<p>The distribution <img alt="q" src="https://latex.codecogs.com/png.latex?q" style="vertical-align: middle;" title="q"/> that we will produce is<br/>
<img alt="q(\vec{x}) = \prod_i q(x_i \given x_{\text{pa}(i)})" src="https://latex.codecogs.com/png.latex?q%28%5Cvec%7Bx%7D%29%20%3D%20%5Cprod_i%20q%28x_i%20%5Cgiven%20x_%7B%5Ctext%7Bpa%7D%28i%29%7D%29" style="vertical-align: middle;" title="q(\vec{x}) = \prod_i q(x_i \given x_{\text{pa}(i)})"/><br/>
where we start at the root and keep sampling down the tree. Based on the tree structure, <img alt="\mathbb{H}(q) = \mathbb{H}_\text{BETHE}(q)" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D%28q%29%20%3D%20%5Cmathbb%7BH%7D_%5Ctext%7BBETHE%7D%28q%29" style="vertical-align: middle;" title="\mathbb{H}(q) = \mathbb{H}_\text{BETHE}(q)"/>. The energy is also the same since <img alt="q_{ij}(x_i, x_j) = \widetilde{q}_{ij}(x_i, x_j)" src="https://latex.codecogs.com/png.latex?q_%7Bij%7D%28x_i%2C%20x_j%29%20%3D%20%5Cwidetilde%7Bq%7D_%7Bij%7D%28x_i%2C%20x_j%29" style="vertical-align: middle;" title="q_{ij}(x_i, x_j) = \widetilde{q}_{ij}(x_i, x_j)"/>. Therefore, since both terms in the equation are equal to the respective terms in the partition function, we get that the equation must equal the partition function.</p></li>
</ol>
<p><strong>Remarks:</strong></p>
<ol type="1">
<li><img alt="\mathbb{H}_\text{BETHE}" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D_%5Ctext%7BBETHE%7D" style="vertical-align: middle;" title="\mathbb{H}_\text{BETHE}"/> is commonly used on graphs that are not trees.</li>
<li>However, for non-trees, the optimization is often no longer concave.</li>
<li>Therefore, the value obtained by the output equation is neither an upper or lower bound.</li>
<li>The fixed points of the <a href="https://en.wikipedia.org/wiki/Belief_propagation">belief propagation algorithm</a> give a 1-1 correspondence with the stationary points of the Bethe objective. (See a classical paper by <a href="http://qwone.com/~jason/trg/papers/yedidia-belief-01.pdf">Yedidia et al.</a>.)</li>
</ol>
<h2 id="augmented-mean-field-pseudo-entropy">Augmented Mean Field Pseudo-Entropy</h2>
<p>For this approximation, we will focus on dense graphs: namely graphs in which each vertex has degree <img alt="\geq \Delta \cdot n" src="https://latex.codecogs.com/png.latex?%5Cgeq%20%5CDelta%20%5Ccdot%20n" style="vertical-align: middle;" title="\geq \Delta \cdot n"/> for some <img alt="\Delta &gt; 0" src="https://latex.codecogs.com/png.latex?%5CDelta%20%3E%200" style="vertical-align: middle;" title="\Delta &gt; 0"/>. To simplify things, we focus on distributions of the form<br/>
<img alt=" p(\vec{x}) \propto \exp \left(\sum_{i \sim j} \pm J x_i x_j\right) " src="https://latex.codecogs.com/png.latex?%0Ap%28%5Cvec%7Bx%7D%29%20%5Cpropto%20%5Cexp%20%5Cleft%28%5Csum_%7Bi%20%5Csim%20j%7D%20%5Cpm%20J%20x_i%20x_j%5Cright%29%0A" style="vertical-align: middle;" title=" p(\vec{x}) \propto \exp \left(\sum_{i \sim j} \pm J x_i x_j\right) "/><br/>
where <img alt="J" src="https://latex.codecogs.com/png.latex?J" style="vertical-align: middle;" title="J"/> is some positive constant parameter. The Bethe entropy approximation is undesirable in the dense case, because we cannot bound its error on non-trees. Instead, we proved the following theorem.</p>
<p><strong>Theorem</strong> <a href="http://arxiv.org/abs/1607.03183">(Risteski ’16)</a></p>
<p>For a dense graph with parameter <img alt="\Delta" src="https://latex.codecogs.com/png.latex?%5CDelta" style="vertical-align: middle;" title="\Delta"/>, there exists an outer approximation based on <img alt="\text{SA}(\frac{1}{\epsilon^2\Delta})" src="https://latex.codecogs.com/png.latex?%5Ctext%7BSA%7D%28%5Cfrac%7B1%7D%7B%5Cepsilon%5E2%5CDelta%7D%29" style="vertical-align: middle;" title="\text{SA}(\frac{1}{\epsilon^2\Delta})"/> and an entropy proxy which achieves a <img alt="O(J \epsilon n^2 \Delta)" src="https://latex.codecogs.com/png.latex?O%28J%20%5Cepsilon%20n%5E2%20%5CDelta%29" style="vertical-align: middle;" title="O(J \epsilon n^2 \Delta)"/> additive approximation to <img alt="\log Z" src="https://latex.codecogs.com/png.latex?%5Clog%20Z" style="vertical-align: middle;" title="\log Z"/> for some <img alt="\epsilon &gt; 0" src="https://latex.codecogs.com/png.latex?%5Cepsilon%20%3E%200" style="vertical-align: middle;" title="\epsilon &gt; 0"/>. There is an algorithm with runtime <img alt="O(\frac{1}{\epsilon^2\Delta})" src="https://latex.codecogs.com/png.latex?O%28%5Cfrac%7B1%7D%7B%5Cepsilon%5E2%5CDelta%7D%29" style="vertical-align: middle;" title="O(\frac{1}{\epsilon^2\Delta})"/>.</p>
<p>To parse the theorem statement, consider the potential regimes for <img alt="J" src="https://latex.codecogs.com/png.latex?J" style="vertical-align: middle;" title="J"/>:</p>
<ol type="1">
<li><img alt="J &gt;&gt; \frac{1}{n} \rightarrow" src="https://latex.codecogs.com/png.latex?J%20%3E%3E%20%5Cfrac%7B1%7D%7Bn%7D%20%5Crightarrow" style="vertical-align: middle;" title="J &gt;&gt; \frac{1}{n} \rightarrow"/> One can ignore the entropy portion and solve for the energy portion that dominates. (So the guarantee is not that interesting.)</li>
<li><img alt="J &lt;&lt; \frac{1}{n} \rightarrow" src="https://latex.codecogs.com/png.latex?J%20%3C%3C%20%5Cfrac%7B1%7D%7Bn%7D%20%5Crightarrow" style="vertical-align: middle;" title="J &lt;&lt; \frac{1}{n} \rightarrow"/> MCMC methods give a <img alt="(1+\epsilon)" src="https://latex.codecogs.com/png.latex?%281%2B%5Cepsilon%29" style="vertical-align: middle;" title="(1+\epsilon)"/>-factor approximation in time poly<img alt="(n, \frac{1}{\epsilon})" src="https://latex.codecogs.com/png.latex?%28n%2C%20%5Cfrac%7B1%7D%7B%5Cepsilon%7D%29" style="vertical-align: middle;" title="(n, \frac{1}{\epsilon})"/>. (It’s not clear if the methods suggested here can give such a guarantee – this is an interesting problem to explore.)</li>
<li><img alt="J = \Theta(\frac{1}{n}) \rightarrow" src="https://latex.codecogs.com/png.latex?J%20%3D%20%5CTheta%28%5Cfrac%7B1%7D%7Bn%7D%29%20%5Crightarrow" style="vertical-align: middle;" title="J = \Theta(\frac{1}{n}) \rightarrow"/> MCMC mixes slowly, but there is no other way to get a comparable guarantee. This is the interesting regime!</li>
</ol>
<p><strong>Proof Sketch</strong> The proof strategy is as follows: We will formulate a convex program under the SA(<img alt="k" src="https://latex.codecogs.com/png.latex?k" style="vertical-align: middle;" title="k"/>) relaxation that can return a solution <img alt="\widetilde{q}" src="https://latex.codecogs.com/png.latex?%5Cwidetilde%7Bq%7D" style="vertical-align: middle;" title="\widetilde{q}"/> in polynomial time with value <img alt="\log \tilde{Z}" src="https://latex.codecogs.com/png.latex?%5Clog%20%5Ctilde%7BZ%7D" style="vertical-align: middle;" title="\log \tilde{Z}"/>. Note that this value of the relaxation will be an upperbound on the true partition function value <img alt="\log Z^*" src="https://latex.codecogs.com/png.latex?%5Clog%20Z%5E%2A" style="vertical-align: middle;" title="\log Z^*"/>. The convex program solution <img alt="\widetilde{q}" src="https://latex.codecogs.com/png.latex?%5Cwidetilde%7Bq%7D" style="vertical-align: middle;" title="\widetilde{q}"/> may not be a valid probability distribution, so from this, we will construct a “rounded solution’’ – an actual distribution <img alt="q" src="https://latex.codecogs.com/png.latex?q" style="vertical-align: middle;" title="q"/> with value <img alt="\log Z" src="https://latex.codecogs.com/png.latex?%5Clog%20Z" style="vertical-align: middle;" title="\log Z"/>. It follows that<br/>
<img alt="\log \tilde{Z} \geq \log Z^* \geq \log Z" src="https://latex.codecogs.com/png.latex?%5Clog%20%5Ctilde%7BZ%7D%20%5Cgeq%20%5Clog%20Z%5E%2A%20%5Cgeq%20%5Clog%20Z" style="vertical-align: middle;" title="\log \tilde{Z} \geq \log Z^* \geq \log Z"/></p>
<p>We will then aim to put a bound on the gap between <img alt="\log \tilde{Z}" src="https://latex.codecogs.com/png.latex?%5Clog%20%5Ctilde%7BZ%7D" style="vertical-align: middle;" title="\log \tilde{Z}"/> and <img alt="\log Z" src="https://latex.codecogs.com/png.latex?%5Clog%20Z" style="vertical-align: middle;" title="\log Z"/>; namely, that<br/>
<img alt="    \log Z \geq \log \tilde{Z} - \epsilon \cdot n^2 \cdot J \cdot \Delta " src="https://latex.codecogs.com/png.latex?%20%20%20%20%5Clog%20Z%20%5Cgeq%20%5Clog%20%5Ctilde%7BZ%7D%20-%20%5Cepsilon%20%5Ccdot%20n%5E2%20%5Ccdot%20J%20%5Ccdot%20%5CDelta%0A" style="vertical-align: middle;" title="    \log Z \geq \log \tilde{Z} - \epsilon \cdot n^2 \cdot J \cdot \Delta "/><br/>
or equivalently,<br/>
<img alt="    \sum_{i \sim j} \mathbb{E}_q[x_i x_j] \cdot J + \mathbb{H}(q) \geq \sum_{i \sim j} \mathbb{E}_{\widetilde{q}}[x_i x_j] \cdot J + \mathbb{H}_\text{aMF}(\widetilde{q}) - \epsilon n^2 J \Delta " src="https://latex.codecogs.com/png.latex?%20%20%20%20%5Csum_%7Bi%20%5Csim%20j%7D%20%5Cmathbb%7BE%7D_q%5Bx_i%20x_j%5D%20%5Ccdot%20J%20%2B%20%5Cmathbb%7BH%7D%28q%29%20%5Cgeq%20%5Csum_%7Bi%20%5Csim%20j%7D%20%5Cmathbb%7BE%7D_%7B%5Cwidetilde%7Bq%7D%7D%5Bx_i%20x_j%5D%20%5Ccdot%20J%20%2B%20%5Cmathbb%7BH%7D_%5Ctext%7BaMF%7D%28%5Cwidetilde%7Bq%7D%29%20-%20%5Cepsilon%20n%5E2%20J%20%5CDelta%20" style="vertical-align: middle;" title="    \sum_{i \sim j} \mathbb{E}_q[x_i x_j] \cdot J + \mathbb{H}(q) \geq \sum_{i \sim j} \mathbb{E}_{\widetilde{q}}[x_i x_j] \cdot J + \mathbb{H}_\text{aMF}(\widetilde{q}) - \epsilon n^2 J \Delta "/><br/>
This equivalently places a bound on the optimality gap between <img alt="\log \tilde{Z}" src="https://latex.codecogs.com/png.latex?%5Clog%20%5Ctilde%7BZ%7D" style="vertical-align: middle;" title="\log \tilde{Z}"/> and <img alt="\log Z^*" src="https://latex.codecogs.com/png.latex?%5Clog%20Z%5E%2A" style="vertical-align: middle;" title="\log Z^*"/>, thereby proving the theorem. Here are the main components of the proof:</p>
<p><strong>1. Entropy Proxy</strong></p>
<p>To approximate <img alt="\mathbb{H}" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D" style="vertical-align: middle;" title="\mathbb{H}"/>, we will use the pseudo-augmented mean field entropy approximation <img alt="\mathbb{H}_\text{aMF}" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D_%5Ctext%7BaMF%7D" style="vertical-align: middle;" title="\mathbb{H}_\text{aMF}"/>. This is defined as</p>
<p><img alt=" \mathbb{H}_\text{aMF}(q) = \mathbb{I}n_{|S| \le k} \{ \overbrace{\mathbb{H}(q_S)}^\text{entropy of $S$} + \overbrace{\sum_{i \notin S} \mathbb{H}(q_i|q_S)}^\text{entropy of everything else} \} " src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BH%7D_%5Ctext%7BaMF%7D%28q%29%20%3D%20%5Cmathbb%7BI%7Dn_%7B%7CS%7C%20%5Cle%20k%7D%20%5C%7B%20%5Coverbrace%7B%5Cmathbb%7BH%7D%28q_S%29%7D%5E%5Ctext%7Bentropy%20of%20%24S%24%7D%20%2B%20%5Coverbrace%7B%5Csum_%7Bi%20%5Cnotin%20S%7D%20%5Cmathbb%7BH%7D%28q_i%7Cq_S%29%7D%5E%5Ctext%7Bentropy%20of%20everything%20else%7D%20%5C%7D%0A" style="vertical-align: middle;" title=" \mathbb{H}_\text{aMF}(q) = \mathbb{I}n_{|S| \le k} \{ \overbrace{\mathbb{H}(q_S)}^\text{entropy of $S$} + \overbrace{\sum_{i \notin S} \mathbb{H}(q_i|q_S)}^\text{entropy of everything else} \} "/></p>
<p>Using <img alt="\mathbb{H}_\text{aMF}" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D_%5Ctext%7BaMF%7D" style="vertical-align: middle;" title="\mathbb{H}_\text{aMF}"/>, we can write a convex program under SA(<img alt="k" src="https://latex.codecogs.com/png.latex?k" style="vertical-align: middle;" title="k"/>) that provides a relaxation for optimizing <img alt="\log Z" src="https://latex.codecogs.com/png.latex?%5Clog%20Z" style="vertical-align: middle;" title="\log Z"/>. Specifically, we will let <img alt="k = O(1 / (\Delta \cdot \epsilon^2))" src="https://latex.codecogs.com/png.latex?k%20%3D%20O%281%20%2F%20%28%5CDelta%20%5Ccdot%20%5Cepsilon%5E2%29%29" style="vertical-align: middle;" title="k = O(1 / (\Delta \cdot \epsilon^2))"/>. Let <img alt="\widetilde{q}" src="https://latex.codecogs.com/png.latex?%5Cwidetilde%7Bq%7D" style="vertical-align: middle;" title="\widetilde{q}"/> be the outputed solution to this relaxation. We define the <em>rounded solution</em> as<br/>
<img alt="q(\vec{x}) = \widetilde{q}_S(\vec{x}_S) \cdot \prod_{i \notin S} \widetilde{q}_S(x_i \given \vec{x}_S)" src="https://latex.codecogs.com/png.latex?q%28%5Cvec%7Bx%7D%29%20%3D%20%5Cwidetilde%7Bq%7D_S%28%5Cvec%7Bx%7D_S%29%20%5Ccdot%20%5Cprod_%7Bi%20%5Cnotin%20S%7D%20%5Cwidetilde%7Bq%7D_S%28x_i%20%5Cgiven%20%5Cvec%7Bx%7D_S%29" style="vertical-align: middle;" title="q(\vec{x}) = \widetilde{q}_S(\vec{x}_S) \cdot \prod_{i \notin S} \widetilde{q}_S(x_i \given \vec{x}_S)"/><br/>
Using the chain rule for entropy, we can show that<br/>
<img alt="\mathbb{H}(q) \geq \mathbb{H}_\text{aMF}(\widetilde{q})" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BH%7D%28q%29%20%5Cgeq%20%5Cmathbb%7BH%7D_%5Ctext%7BaMF%7D%28%5Cwidetilde%7Bq%7D%29" style="vertical-align: middle;" title="\mathbb{H}(q) \geq \mathbb{H}_\text{aMF}(\widetilde{q})"/></p>
<p><strong>2. Rounding Scheme</strong> The above distribution <img alt="q" src="https://latex.codecogs.com/png.latex?q" style="vertical-align: middle;" title="q"/> is actually the same distribution that correlation rounding, as introduced by a seminal paper of <a href="https://arxiv.org/abs/1104.4680">Barak, Raghavendra, and Steurer</a>, produces. By using definition of mutual information <img alt="\mathbb{I}(X, Y)" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BI%7D%28X%2C%20Y%29" style="vertical-align: middle;" title="\mathbb{I}(X, Y)"/> and Pinsker’s Theorem showing that <img alt="\mathrm{\mathbb{C}ov}(X, Y) = O(\sqrt{\mathbb{I}(X, Y)})" src="https://latex.codecogs.com/png.latex?%5Cmathrm%7B%5Cmathbb%7BC%7Dov%7D%28X%2C%20Y%29%20%3D%20O%28%5Csqrt%7B%5Cmathbb%7BI%7D%28X%2C%20Y%29%7D%29" style="vertical-align: middle;" title="\mathrm{\mathbb{C}ov}(X, Y) = O(\sqrt{\mathbb{I}(X, Y)})"/>, we can work through some algebra to show that</p>
<p><img alt="\sum_{i \sim j} \mathbb{E}_q[x_i x_j] \cdot J \geq \sum_{i \sim j} \mathbb{E}_{\widetilde{q}}[x_i x_j] \cdot J - \epsilon n^2 J \Delta" src="https://latex.codecogs.com/png.latex?%5Csum_%7Bi%20%5Csim%20j%7D%20%5Cmathbb%7BE%7D_q%5Bx_i%20x_j%5D%20%5Ccdot%20J%20%5Cgeq%20%5Csum_%7Bi%20%5Csim%20j%7D%20%5Cmathbb%7BE%7D_%7B%5Cwidetilde%7Bq%7D%7D%5Bx_i%20x_j%5D%20%5Ccdot%20J%20-%20%5Cepsilon%20n%5E2%20J%20%5CDelta" style="vertical-align: middle;" title="\sum_{i \sim j} \mathbb{E}_q[x_i x_j] \cdot J \geq \sum_{i \sim j} \mathbb{E}_{\widetilde{q}}[x_i x_j] \cdot J - \epsilon n^2 J \Delta"/></p>
<p>Then, putting together the entropy and energy effects, we can prove the main theorem.</p>
<p><strong>Remarks:</strong></p>
<ol type="1">
<li>The above proof easily extends to a more general notion of density, as well as to sparse graphs with “low-rank’’ spectral structure. See <a href="http://arxiv.org/abs/1607.03183">the paper</a> for more information.</li>
<li>In fact, with more work, one can extract guarantees on the the best <strong>mean field</strong> approximation to the Gibbs distribution in the dense regime. (This intuitively can be done as the distribution <img alt="q" src="https://latex.codecogs.com/png.latex?q" style="vertical-align: middle;" title="q"/> produced above is almost mean-field, except for the set <img alt="S" src="https://latex.codecogs.com/png.latex?S" style="vertical-align: middle;" title="S"/>). This is interesting, since as we mentioned, the quality of the mean-field approximation is usually difficult to characterize. (Moreover, the procedure is algorithmic.) See the recent work of <a href="https://arxiv.org/abs/1808.07226">Jain, Koehler and Risteski ’18</a> for more details and results.</li>
</ol>
<h1 id="taylor-series-approximation">Taylor Series Approximation</h1>
<p>Another method of calculating the partition function involves Taylor expanding its logarithm around a cleverly selected point. This approach was first developed by <a href="https://arxiv.org/abs/1405.1974">Barvinok ’14</a>, for the purpose of computing the partition function of cliques in a graph. The mathematical techniques developed in this paper can be naturally extended to evaluate a variety of partition functions, including that of the ferromagnetic Ising model. We will use this example to illustrate the general idea of the Taylor expansion technique.</p>
<p>The goal here is to devise a deterministic, fully polynomial-time approximation scheme (an FPTAS) for evaluating the partition function of the ferromagnetic Ising model. This means that the the algorithm must run in poly(<img alt="n, \frac{1}{\epsilon}" src="https://latex.codecogs.com/png.latex?n%2C%20%5Cfrac%7B1%7D%7B%5Cepsilon%7D" style="vertical-align: middle;" title="n, \frac{1}{\epsilon}"/>) and be correct within a multiplicative factor of <img alt="1 + \epsilon" src="https://latex.codecogs.com/png.latex?1%20%2B%20%5Cepsilon" style="vertical-align: middle;" title="1 + \epsilon"/>. We will work in the general case where the Hamiltonian includes an external magnetic field term, as well as the neighboring spin interaction terms. Using logarithmic identities, we can re-write the partition function slightly differently from last time:</p>
<p><img alt=" Z(\lambda) = \sum_{x \in \{ \pm 1 \}^n} \lambda^{|{1: x_i = 1}|} \beta^{|E(x)|}" src="https://latex.codecogs.com/png.latex?%20Z%28%5Clambda%29%20%3D%20%5Csum_%7Bx%20%5Cin%20%5C%7B%20%5Cpm%201%20%5C%7D%5En%7D%20%5Clambda%5E%7B%7C%7B1%3A%20x_i%20%3D%201%7D%7C%7D%20%5Cbeta%5E%7B%7CE%28x%29%7C%7D" style="vertical-align: middle;" title=" Z(\lambda) = \sum_{x \in \{ \pm 1 \}^n} \lambda^{|{1: x_i = 1}|} \beta^{|E(x)|}"/><br/>
Here, <img alt="\lambda" src="https://latex.codecogs.com/png.latex?%5Clambda" style="vertical-align: middle;" title="\lambda"/> is called the vertex activity (or external field), and characterizes the likelihood of a vertex to be in the + configuration. Additionally, <img alt="\beta \geq 0" src="https://latex.codecogs.com/png.latex?%5Cbeta%20%5Cgeq%200" style="vertical-align: middle;" title="\beta \geq 0"/> is referred to as the edge activity, and characterizes the propensity of a vertex to agree with its neighbors. The ferromagnetic regime (where agreement of spins is favored) corresponds to <img alt="\beta &lt; 1" src="https://latex.codecogs.com/png.latex?%5Cbeta%20%3C%201" style="vertical-align: middle;" title="\beta &lt; 1"/>. <img alt="|E(x)|" src="https://latex.codecogs.com/png.latex?%7CE%28x%29%7C" style="vertical-align: middle;" title="|E(x)|"/> denotes the number of edge cut in the graph, which is equivalent to the number of neighboring pairs with opposite spins.</p>
<p>As one final disclaimer, the approximation scheme presented below is not valid for <img alt="| \lambda| = 1" src="https://latex.codecogs.com/png.latex?%7C%20%5Clambda%7C%20%3D%201" style="vertical-align: middle;" title="| \lambda| = 1"/>, which corresponds to the zero-magnetic field case. Randomized algorithms exist which can handle this case.</p>
<p>The general idea here is to hold one parameter fixed (<img alt="\beta" src="https://latex.codecogs.com/png.latex?%5Cbeta" style="vertical-align: middle;" title="\beta"/> in our case), and express the logarithm of the partition function as a Taylor series in the other parameter (<img alt="\lambda" src="https://latex.codecogs.com/png.latex?%5Clambda" style="vertical-align: middle;" title="\lambda"/>). From a physics standpoint, the Taylor expansion around <img alt="\lambda = 0" src="https://latex.codecogs.com/png.latex?%5Clambda%20%3D%200" style="vertical-align: middle;" title="\lambda = 0"/> tells us how the partition function is changing as a function of the magnetic field. Without loss of generality, we focus on the case where <img alt="| \lambda | &lt; 1" src="https://latex.codecogs.com/png.latex?%7C%20%5Clambda%20%7C%20%3C%201" style="vertical-align: middle;" title="| \lambda | &lt; 1"/>. This simplification can be justified by a simple symmetry argument. Consider <img alt="\bar{x}" src="https://latex.codecogs.com/png.latex?%5Cbar%7Bx%7D" style="vertical-align: middle;" title="\bar{x}"/> as the inverse of <img alt="x" src="https://latex.codecogs.com/png.latex?x" style="vertical-align: middle;" title="x"/> where all the 1’s (spin-up) are flipped to -1s (spin-down) and vice-versa. The partition function of this flipped system can be related to the original system by a constant factor:</p>
<p><img alt="\begin{align*} \sum_x \beta^{|E(x)|} \lambda^{|\{1: x_i = 1\}|} &amp;= \sum_x \beta^{|E(\bar{x})|} \lambda^{n - |\{1: \bar{x}_i = 1\}|} \\ &amp;= \lambda^{n} \sum_x \beta^{|E(\bar{x})|} \left(\frac{1}{\lambda}\right)^{|\{1: \bar{x}_i = 1\}|} \end{align*}" src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%2A%7D%0A%5Csum_x%20%5Cbeta%5E%7B%7CE%28x%29%7C%7D%20%5Clambda%5E%7B%7C%5C%7B1%3A%20x_i%20%3D%201%5C%7D%7C%7D%20%26%3D%20%5Csum_x%20%5Cbeta%5E%7B%7CE%28%5Cbar%7Bx%7D%29%7C%7D%20%5Clambda%5E%7Bn%20-%20%7C%5C%7B1%3A%20%5Cbar%7Bx%7D_i%20%3D%201%5C%7D%7C%7D%20%5C%5C%0A%26%3D%20%5Clambda%5E%7Bn%7D%20%5Csum_x%20%5Cbeta%5E%7B%7CE%28%5Cbar%7Bx%7D%29%7C%7D%20%5Cleft%28%5Cfrac%7B1%7D%7B%5Clambda%7D%5Cright%29%5E%7B%7C%5C%7B1%3A%20%5Cbar%7Bx%7D_i%20%3D%201%5C%7D%7C%7D%0A%5Cend%7Balign%2A%7D" style="vertical-align: middle;" title="\begin{align*} \sum_x \beta^{|E(x)|} \lambda^{|\{1: x_i = 1\}|} &amp;= \sum_x \beta^{|E(\bar{x})|} \lambda^{n - |\{1: \bar{x}_i = 1\}|} \\ &amp;= \lambda^{n} \sum_x \beta^{|E(\bar{x})|} \left(\frac{1}{\lambda}\right)^{|\{1: \bar{x}_i = 1\}|} \end{align*}"/></p>
<p>This holds because the number of cut edges remains constant when the values are flipped. Since these two partition functions are related by this factor constant in <img alt="\lambda" src="https://latex.codecogs.com/png.latex?%5Clambda" style="vertical-align: middle;" title="\lambda"/>, for any model with <img alt="\lambda &gt; 1" src="https://latex.codecogs.com/png.latex?%5Clambda%20%3E%201" style="vertical-align: middle;" title="\lambda &gt; 1"/>, we can simply consider the flipped graph <img alt="\bar{x}" src="https://latex.codecogs.com/png.latex?%5Cbar%7Bx%7D" style="vertical-align: middle;" title="\bar{x}"/> and use the above relation.</p>
<p>For our purposes, it will be more convenient to approximate the logarithm of the partition function, because a multiplicative <img alt="(1 + \epsilon)" src="https://latex.codecogs.com/png.latex?%281%20%2B%20%5Cepsilon%29" style="vertical-align: middle;" title="(1 + \epsilon)"/> approximation of <img alt="Z(\lambda)" src="https://latex.codecogs.com/png.latex?Z%28%5Clambda%29" style="vertical-align: middle;" title="Z(\lambda)"/> corresponds to an <img alt="\mathcal{O}(\epsilon)" src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D%28%5Cepsilon%29" style="vertical-align: middle;" title="\mathcal{O}(\epsilon)"/> additive approximation of <img alt="\log Z(\lambda)" src="https://latex.codecogs.com/png.latex?%5Clog%20Z%28%5Clambda%29" style="vertical-align: middle;" title="\log Z(\lambda)"/>. In fact, if we allow the partition function to be complex, then we can easily show that an additive bound of <img alt="\frac{\epsilon}{4}" src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cepsilon%7D%7B4%7D" style="vertical-align: middle;" title="\frac{\epsilon}{4}"/> for <img alt="\log Z(\lambda)" src="https://latex.codecogs.com/png.latex?%5Clog%20Z%28%5Clambda%29" style="vertical-align: middle;" title="\log Z(\lambda)"/> guarantees a multiplicative bound of <img alt="(1+\epsilon)" src="https://latex.codecogs.com/png.latex?%281%2B%5Cepsilon%29" style="vertical-align: middle;" title="(1+\epsilon)"/> for <img alt="Z(\lambda)" src="https://latex.codecogs.com/png.latex?Z%28%5Clambda%29" style="vertical-align: middle;" title="Z(\lambda)"/>.</p>
<p>For notational convenience we define:<br/>
<img alt=" \log Z(\lambda) = f(\lambda)" src="https://latex.codecogs.com/png.latex?%20%5Clog%20Z%28%5Clambda%29%20%3D%20f%28%5Clambda%29" style="vertical-align: middle;" title=" \log Z(\lambda) = f(\lambda)"/><br/>
Thus, the Taylor expansion of <img alt="f(\lambda)" src="https://latex.codecogs.com/png.latex?f%28%5Clambda%29" style="vertical-align: middle;" title="f(\lambda)"/> around <img alt="\lambda = 0" src="https://latex.codecogs.com/png.latex?%5Clambda%20%3D%200" style="vertical-align: middle;" title="\lambda = 0"/> if given by:<br/>
<img alt="f(\lambda) = \sum_j f^{(j)} (0) \frac{\lambda^j}{j!}" src="https://latex.codecogs.com/png.latex?f%28%5Clambda%29%20%3D%20%5Csum_j%20f%5E%7B%28j%29%7D%20%280%29%20%5Cfrac%7B%5Clambda%5Ej%7D%7Bj%21%7D" style="vertical-align: middle;" title="f(\lambda) = \sum_j f^{(j)} (0) \frac{\lambda^j}{j!}"/></p>
<p>The big question now is: Can we get a good approximation from just the lower order terms of the Taylor expansion for <img alt="f(\lambda)" src="https://latex.codecogs.com/png.latex?f%28%5Clambda%29" style="vertical-align: middle;" title="f(\lambda)"/>? Equivalently, can we can bound the sum of the higher order terms at <img alt="\frac{\epsilon}{4}" src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cepsilon%7D%7B4%7D" style="vertical-align: middle;" title="\frac{\epsilon}{4}"/>? Additionally, we still must address the question of how to actually calculate the lower order terms.</p>
<p>To answer these questions, we make simple observations about the derivatives of <img alt="f" src="https://latex.codecogs.com/png.latex?f" style="vertical-align: middle;" title="f"/> in relation to the derivatives of <img alt="Z" src="https://latex.codecogs.com/png.latex?Z" style="vertical-align: middle;" title="Z"/>.<br/>
<img alt="\begin{align*}     f'(\lambda) &amp;= \frac{1}{Z(\lambda)} Z'(\lambda) \\     Z'(\lambda) &amp;= f'(\lambda) Z(\lambda) \\     Z^{(m)} (\lambda) &amp;= \sum_{j=0}^{m-1} {m-1 \choose j} Z^{(j)} (\lambda) f^{(m-j)} (\lambda) \end{align*}" src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%2A%7D%0A%20%20%20%20f%27%28%5Clambda%29%20%26%3D%20%5Cfrac%7B1%7D%7BZ%28%5Clambda%29%7D%20Z%27%28%5Clambda%29%20%5C%5C%0A%20%20%20%20Z%27%28%5Clambda%29%20%26%3D%20f%27%28%5Clambda%29%20Z%28%5Clambda%29%20%5C%5C%0A%20%20%20%20Z%5E%7B%28m%29%7D%20%28%5Clambda%29%20%26%3D%20%5Csum_%7Bj%3D0%7D%5E%7Bm-1%7D%20%7Bm-1%20%5Cchoose%20j%7D%20Z%5E%7B%28j%29%7D%20%28%5Clambda%29%20f%5E%7B%28m-j%29%7D%20%28%5Clambda%29%0A%5Cend%7Balign%2A%7D" style="vertical-align: middle;" title="\begin{align*}     f'(\lambda) &amp;= \frac{1}{Z(\lambda)} Z'(\lambda) \\     Z'(\lambda) &amp;= f'(\lambda) Z(\lambda) \\     Z^{(m)} (\lambda) &amp;= \sum_{j=0}^{m-1} {m-1 \choose j} Z^{(j)} (\lambda) f^{(m-j)} (\lambda) \end{align*}"/><br/>
The last equation just comes from repeated application of the product rule. Using these relations, we can solve for the first <img alt="m" src="https://latex.codecogs.com/png.latex?m" style="vertical-align: middle;" title="m"/> derivatives of <img alt="f" src="https://latex.codecogs.com/png.latex?f" style="vertical-align: middle;" title="f"/> if we have the first <img alt="m" src="https://latex.codecogs.com/png.latex?m" style="vertical-align: middle;" title="m"/> derivatives of <img alt="Z" src="https://latex.codecogs.com/png.latex?Z" style="vertical-align: middle;" title="Z"/> using a triangular linear system of equations. As <img alt="Z(0) = 1" src="https://latex.codecogs.com/png.latex?Z%280%29%20%3D%201" style="vertical-align: middle;" title="Z(0) = 1"/>, we see that the system is non-degenerate.</p>
<p>Note, <img alt="Z(\lambda)" src="https://latex.codecogs.com/png.latex?Z%28%5Clambda%29" style="vertical-align: middle;" title="Z(\lambda)"/> is a n-degree polynomial with leading coefficient of 1 and constant coefficient of 1 (corresponding to the configurations where all vertices are positive / negative respectively). Using this form, we can re-write the partition function in terms of its <img alt="n" src="https://latex.codecogs.com/png.latex?n" style="vertical-align: middle;" title="n"/> (possibly complex) roots <img alt="r_i" src="https://latex.codecogs.com/png.latex?r_i" style="vertical-align: middle;" title="r_i"/>:</p>
<p><img alt="Z(\lambda) = \prod_{i=1}^n \left(1-\frac{\lambda}{r_i} \right) \text{ for some } r_i" src="https://latex.codecogs.com/png.latex?Z%28%5Clambda%29%20%3D%20%5Cprod_%7Bi%3D1%7D%5En%20%5Cleft%281-%5Cfrac%7B%5Clambda%7D%7Br_i%7D%20%5Cright%29%20%5Ctext%7B%20for%20some%20%7D%20r_i" style="vertical-align: middle;" title="Z(\lambda) = \prod_{i=1}^n \left(1-\frac{\lambda}{r_i} \right) \text{ for some } r_i"/></p>
<p><img alt="f(\lambda) = \log Z(\lambda) = \sum_{i=1}^n \log \left(1-\frac{\lambda}{r_i}\right)" src="https://latex.codecogs.com/png.latex?f%28%5Clambda%29%20%3D%20%5Clog%20Z%28%5Clambda%29%20%3D%20%5Csum_%7Bi%3D1%7D%5En%20%5Clog%20%5Cleft%281-%5Cfrac%7B%5Clambda%7D%7Br_i%7D%5Cright%29" style="vertical-align: middle;" title="f(\lambda) = \log Z(\lambda) = \sum_{i=1}^n \log \left(1-\frac{\lambda}{r_i}\right)"/></p>
<p><img alt="= - \sum_{i=1}^n \sum_{j=1}^{\infty} \frac{1}{j} \left(\frac{\lambda}{r_i} \right)^j" src="https://latex.codecogs.com/png.latex?%3D%20-%20%5Csum_%7Bi%3D1%7D%5En%20%5Csum_%7Bj%3D1%7D%5E%7B%5Cinfty%7D%20%5Cfrac%7B1%7D%7Bj%7D%20%5Cleft%28%5Cfrac%7B%5Clambda%7D%7Br_i%7D%20%5Cright%29%5Ej" style="vertical-align: middle;" title="= - \sum_{i=1}^n \sum_{j=1}^{\infty} \frac{1}{j} \left(\frac{\lambda}{r_i} \right)^j"/></p>
<p>Supposing we keep the first <img alt="m" src="https://latex.codecogs.com/png.latex?m" style="vertical-align: middle;" title="m"/> terms, the error is given bounded by:</p>
<p><img alt="|f(\lambda) + \sum_{i=1}^n \sum_{j=1}^{m} \frac{1}{j} (\frac{\lambda}{r_i})^j| \leq  \sum_{i=1}^n \sum_{j=m+1}^{\infty} \frac{1}{|r_i|}\frac{|\lambda|^j}{j} \leq \sum_{i=1}^n \frac{|\lambda|^{m+1}}{(m+1)(1-|\lambda|)}\frac{1}{|r_i|}" src="https://latex.codecogs.com/png.latex?%7Cf%28%5Clambda%29%20%2B%20%5Csum_%7Bi%3D1%7D%5En%20%5Csum_%7Bj%3D1%7D%5E%7Bm%7D%20%5Cfrac%7B1%7D%7Bj%7D%20%28%5Cfrac%7B%5Clambda%7D%7Br_i%7D%29%5Ej%7C%20%5Cleq%20%20%5Csum_%7Bi%3D1%7D%5En%20%5Csum_%7Bj%3Dm%2B1%7D%5E%7B%5Cinfty%7D%20%5Cfrac%7B1%7D%7B%7Cr_i%7C%7D%5Cfrac%7B%7C%5Clambda%7C%5Ej%7D%7Bj%7D%20%5Cleq%20%5Csum_%7Bi%3D1%7D%5En%20%5Cfrac%7B%7C%5Clambda%7C%5E%7Bm%2B1%7D%7D%7B%28m%2B1%29%281-%7C%5Clambda%7C%29%7D%5Cfrac%7B1%7D%7B%7Cr_i%7C%7D" style="vertical-align: middle;" title="|f(\lambda) + \sum_{i=1}^n \sum_{j=1}^{m} \frac{1}{j} (\frac{\lambda}{r_i})^j| \leq  \sum_{i=1}^n \sum_{j=m+1}^{\infty} \frac{1}{|r_i|}\frac{|\lambda|^j}{j} \leq \sum_{i=1}^n \frac{|\lambda|^{m+1}}{(m+1)(1-|\lambda|)}\frac{1}{|r_i|}"/></p>
<p>Here we can invoke the Lee-Yang theorem, which tells us that the zeroes of <img alt="Z(\lambda)" src="https://latex.codecogs.com/png.latex?Z%28%5Clambda%29" style="vertical-align: middle;" title="Z(\lambda)"/> for a system with ferromagnetic interactions lie on the unit circle of the complex plane. So the Lee-Yang theorem guarantees that <img alt="|r_i| = 1" src="https://latex.codecogs.com/png.latex?%7Cr_i%7C%20%3D%201" style="vertical-align: middle;" title="|r_i| = 1"/>, and we see that the error due to truncation is ultimately bounded by:</p>
<p><img alt="\frac{n|\lambda|^{m+1}}{(m+1)(1-|\lambda|)}" src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bn%7C%5Clambda%7C%5E%7Bm%2B1%7D%7D%7B%28m%2B1%29%281-%7C%5Clambda%7C%29%7D" style="vertical-align: middle;" title="\frac{n|\lambda|^{m+1}}{(m+1)(1-|\lambda|)}"/></p>
<p>Now by the previous symmetry argument, we can assume that <img alt="\lambda &lt; 1" src="https://latex.codecogs.com/png.latex?%5Clambda%20%3C%201" style="vertical-align: middle;" title="\lambda &lt; 1"/>. Thus, to achieve an error bound of <img alt="\frac{\epsilon}{4}" src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cepsilon%7D%7B4%7D" style="vertical-align: middle;" title="\frac{\epsilon}{4}"/>, we must have:<br/>
<img alt="\frac{\epsilon}{4} &gt; \frac{n|\lambda|^{m+1}}{(m+1)(1-|\lambda|)}" src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cepsilon%7D%7B4%7D%20%3E%20%5Cfrac%7Bn%7C%5Clambda%7C%5E%7Bm%2B1%7D%7D%7B%28m%2B1%29%281-%7C%5Clambda%7C%29%7D" style="vertical-align: middle;" title="\frac{\epsilon}{4} &gt; \frac{n|\lambda|^{m+1}}{(m+1)(1-|\lambda|)}"/><br/>
Rearranging terms and taking the natural log of both sides (which is justified given that <img alt="|\lambda| &lt; 1" src="https://latex.codecogs.com/png.latex?%7C%5Clambda%7C%20%3C%201" style="vertical-align: middle;" title="|\lambda| &lt; 1"/>), we see that the inequality is satisfied if:</p>
<p><img alt="m \geq \frac{1}{\log(\frac{1}{|\lambda|})}(\log(\frac{4n}{\epsilon}) + \log(\frac{1}{1-|\lambda|}))" src="https://latex.codecogs.com/png.latex?m%20%5Cgeq%20%5Cfrac%7B1%7D%7B%5Clog%28%5Cfrac%7B1%7D%7B%7C%5Clambda%7C%7D%29%7D%28%5Clog%28%5Cfrac%7B4n%7D%7B%5Cepsilon%7D%29%20%2B%20%5Clog%28%5Cfrac%7B1%7D%7B1-%7C%5Clambda%7C%7D%29%29" style="vertical-align: middle;" title="m \geq \frac{1}{\log(\frac{1}{|\lambda|})}(\log(\frac{4n}{\epsilon}) + \log(\frac{1}{1-|\lambda|}))"/></p>
<p>Thus, we need only retain the first <img alt="\frac{1}{\log(\frac{1}{|\lambda|})}(\log(\frac{4n}{\epsilon}) + \log(\frac{1}{1-|\lambda|}))" src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B%5Clog%28%5Cfrac%7B1%7D%7B%7C%5Clambda%7C%7D%29%7D%28%5Clog%28%5Cfrac%7B4n%7D%7B%5Cepsilon%7D%29%20%2B%20%5Clog%28%5Cfrac%7B1%7D%7B1-%7C%5Clambda%7C%7D%29%29" style="vertical-align: middle;" title="\frac{1}{\log(\frac{1}{|\lambda|})}(\log(\frac{4n}{\epsilon}) + \log(\frac{1}{1-|\lambda|}))"/> terms of the Taylor series expansion of <img alt="f(\lambda)" src="https://latex.codecogs.com/png.latex?f%28%5Clambda%29" style="vertical-align: middle;" title="f(\lambda)"/>. This will involve calculating the first <img alt="m = \lceil \frac{1}{\log(\frac{1}{|\lambda|})}(\log(\frac{4n}{\epsilon}) + \log(\frac{1}{1-|\lambda|}))\rceil" src="https://latex.codecogs.com/png.latex?m%20%3D%20%5Clceil%20%5Cfrac%7B1%7D%7B%5Clog%28%5Cfrac%7B1%7D%7B%7C%5Clambda%7C%7D%29%7D%28%5Clog%28%5Cfrac%7B4n%7D%7B%5Cepsilon%7D%29%20%2B%20%5Clog%28%5Cfrac%7B1%7D%7B1-%7C%5Clambda%7C%7D%29%29%5Crceil" style="vertical-align: middle;" title="m = \lceil \frac{1}{\log(\frac{1}{|\lambda|})}(\log(\frac{4n}{\epsilon}) + \log(\frac{1}{1-|\lambda|}))\rceil"/> derivatives of <img alt="Z(\lambda)" src="https://latex.codecogs.com/png.latex?Z%28%5Clambda%29" style="vertical-align: middle;" title="Z(\lambda)"/>, which naively can be done in quasi-polynomial time. This is done by running over all <img alt="S" src="https://latex.codecogs.com/png.latex?S" style="vertical-align: middle;" title="S"/>, where <img alt="|S| = m" src="https://latex.codecogs.com/png.latex?%7CS%7C%20%3D%20m" style="vertical-align: middle;" title="|S| = m"/>. This takes <img alt="O(n \log n + \log (\frac{1}{\epsilon}))" src="https://latex.codecogs.com/png.latex?O%28n%20%5Clog%20n%20%2B%20%5Clog%20%28%5Cfrac%7B1%7D%7B%5Cepsilon%7D%29%29" style="vertical-align: middle;" title="O(n \log n + \log (\frac{1}{\epsilon}))"/> time, which is quasi-polynomial.</p>
<p>Recent work by <a href="https://arxiv.org/abs/1707.05186">Patel and Regts</a>, as well as <a href="https://arxiv.org/abs/1704.06493">Liu, Sinclair and Srivastiva</a> has focused on evaluating these coefficients more efficiently (in polynomial time), but is outside the scope of this lecture. Clever counting arguments aside, to trivially calculate the <img alt="k" src="https://latex.codecogs.com/png.latex?k" style="vertical-align: middle;" title="k"/>-th derivative, we need only sum over the vectors with <img alt="k" src="https://latex.codecogs.com/png.latex?k" style="vertical-align: middle;" title="k"/> spin-up components.</p>
<h2 id="lee-yang-theorem">Lee-Yang Theorem</h2>
<p>Most of the heavy lifting in the above approach is done by the Lee-Yang theorem. In this section, we sketch how it is proved.</p>
<p>First, let’s define the Lee-Yang property, which we will refer to as the LYP. Let <img alt="P(\lambda_1, \dots, \lambda_n)" src="https://latex.codecogs.com/png.latex?P%28%5Clambda_1%2C%20%5Cdots%2C%20%5Clambda_n%29" style="vertical-align: middle;" title="P(\lambda_1, \dots, \lambda_n)"/> be some multilinear polynomial. <img alt="P" src="https://latex.codecogs.com/png.latex?P" style="vertical-align: middle;" title="P"/> has the Lee-Yang property if for any complex numbers <img alt="\lambda_1, \dots, \lambda_n" src="https://latex.codecogs.com/png.latex?%5Clambda_1%2C%20%5Cdots%2C%20%5Clambda_n" style="vertical-align: middle;" title="\lambda_1, \dots, \lambda_n"/> such that <img alt="|\lambda_i| &gt; 1" src="https://latex.codecogs.com/png.latex?%7C%5Clambda_i%7C%20%3E%201" style="vertical-align: middle;" title="|\lambda_i| &gt; 1"/> for all i, then <img alt="P(\lambda_1, \dots, \lambda_n) \neq 0" src="https://latex.codecogs.com/png.latex?P%28%5Clambda_1%2C%20%5Cdots%2C%20%5Clambda_n%29%20%5Cneq%200" style="vertical-align: middle;" title="P(\lambda_1, \dots, \lambda_n) \neq 0"/>.</p>
<p>We can then show that the partition function for the ferromagnetic Ising model, which we wrote as<br/>
<img alt="Z(\lambda_1, \dots, \lambda_n) = \sum_x \beta^{|E(x)|} \prod_{i: x_i = 1} \lambda_i" src="https://latex.codecogs.com/png.latex?Z%28%5Clambda_1%2C%20%5Cdots%2C%20%5Clambda_n%29%20%3D%20%5Csum_x%20%5Cbeta%5E%7B%7CE%28x%29%7C%7D%20%5Cprod_%7Bi%3A%20x_i%20%3D%201%7D%20%5Clambda_i" style="vertical-align: middle;" title="Z(\lambda_1, \dots, \lambda_n) = \sum_x \beta^{|E(x)|} \prod_{i: x_i = 1} \lambda_i"/><br/>
must have this LYP. In the antiferromagnetic case it turns out that all zeroes lie on the negative real axis, but we will focus on the ferromagnetic case.</p>
<p>Proof (sketch):</p>
<p>For the proof that the partition function has the LYP, we will use Asano’s contraction argument. This relies on the fact that certain operations preserve LYP and we can “build” up to the partition function of the full graph by these operations.</p>
<ul>
<li><strong>Disjoint union</strong>: This is fairly simple to prove since the partition function for the disjoint union of <img alt="G" src="https://latex.codecogs.com/png.latex?G" style="vertical-align: middle;" title="G"/> and <img alt="H" src="https://latex.codecogs.com/png.latex?H" style="vertical-align: middle;" title="H"/> would just factor into the multiplication of the two partition function of the original graphs <img alt="Z_{G \sqcup H} = Z_{G} Z_{H}" src="https://latex.codecogs.com/png.latex?Z_%7BG%20%5Csqcup%20H%7D%20%3D%20Z_%7BG%7D%20Z_%7BH%7D" style="vertical-align: middle;" title="Z_{G \sqcup H} = Z_{G} Z_{H}"/>. Where <img alt="Z_{G}" src="https://latex.codecogs.com/png.latex?Z_%7BG%7D" style="vertical-align: middle;" title="Z_{G}"/> and <img alt="Z_{H}" src="https://latex.codecogs.com/png.latex?Z_%7BH%7D" style="vertical-align: middle;" title="Z_{H}"/> have the LYP, then it is clear that <img alt="Z_{G \sqcup H}" src="https://latex.codecogs.com/png.latex?Z_%7BG%20%5Csqcup%20H%7D" style="vertical-align: middle;" title="Z_{G \sqcup H}"/> has the same LYP.</li>
<li><strong>Contraction</strong>: Suppose we produce a graph <img alt="G'" src="https://latex.codecogs.com/png.latex?G%27" style="vertical-align: middle;" title="G'"/> from <img alt="G" src="https://latex.codecogs.com/png.latex?G" style="vertical-align: middle;" title="G"/> by contracting 2 vertices <img alt="z_1, z_2" src="https://latex.codecogs.com/png.latex?z_1%2C%20z_2" style="vertical-align: middle;" title="z_1, z_2"/> in <img alt="G" src="https://latex.codecogs.com/png.latex?G" style="vertical-align: middle;" title="G"/>, which means merging them into one vertex. It can be shown that if <img alt="G" src="https://latex.codecogs.com/png.latex?G" style="vertical-align: middle;" title="G"/> has the LYP, then <img alt="G'" src="https://latex.codecogs.com/png.latex?G%27" style="vertical-align: middle;" title="G'"/> also has the LYP. We write the partition function for G as<br/>
<img alt="A\lambda_1 \lambda_2 + B \lambda_1 + C \lambda_2 + D" src="https://latex.codecogs.com/png.latex?A%5Clambda_1%20%5Clambda_2%20%2B%20B%20%5Clambda_1%20%2B%20C%20%5Clambda_2%20%2B%20D" style="vertical-align: middle;" title="A\lambda_1 \lambda_2 + B \lambda_1 + C \lambda_2 + D"/><br/>
The “contracted” graph amounts to deleting the middle 2 terms so the partition function for <img alt="G'" src="https://latex.codecogs.com/png.latex?G%27" style="vertical-align: middle;" title="G'"/> can be written as<br/>
<img alt="\underbrace{A \lambda'}_{\text{both plus}} + \underbrace{D}_{\text{both minus}}" src="https://latex.codecogs.com/png.latex?%5Cunderbrace%7BA%20%5Clambda%27%7D_%7B%5Ctext%7Bboth%20plus%7D%7D%20%2B%20%5Cunderbrace%7BD%7D_%7B%5Ctext%7Bboth%20minus%7D%7D" style="vertical-align: middle;" title="\underbrace{A \lambda'}_{\text{both plus}} + \underbrace{D}_{\text{both minus}}"/><br/>
We want to show that the partition function for <img alt="G'" src="https://latex.codecogs.com/png.latex?G%27" style="vertical-align: middle;" title="G'"/> has the LYP. Because the partition function of <img alt="G" src="https://latex.codecogs.com/png.latex?G" style="vertical-align: middle;" title="G"/> has the LYP, we can consider the case where <img alt="\lambda_1 = \lambda_2 = \lambda" src="https://latex.codecogs.com/png.latex?%5Clambda_1%20%3D%20%5Clambda_2%20%3D%20%5Clambda" style="vertical-align: middle;" title="\lambda_1 = \lambda_2 = \lambda"/>. By the LYP,<br/>
<img alt="A\lambda^2 + (B+C)\lambda + D \neq  0" src="https://latex.codecogs.com/png.latex?A%5Clambda%5E2%20%2B%20%28B%2BC%29%5Clambda%20%2B%20D%20%5Cneq%20%200" style="vertical-align: middle;" title="A\lambda^2 + (B+C)\lambda + D \neq  0"/><br/>
if <img alt="|\lambda| &gt; 1" src="https://latex.codecogs.com/png.latex?%7C%5Clambda%7C%20%3E%201" style="vertical-align: middle;" title="|\lambda| &gt; 1"/>. By Vieta’s formulas we can find a relation between A and D,<br/>
<img alt=" \frac{|D|}{|A|} = \prod |zeroes| \leq 1" src="https://latex.codecogs.com/png.latex?%20%5Cfrac%7B%7CD%7C%7D%7B%7CA%7C%7D%20%3D%20%5Cprod%20%7Czeroes%7C%20%5Cleq%201" style="vertical-align: middle;" title=" \frac{|D|}{|A|} = \prod |zeroes| \leq 1"/><br/>
Now, to show that the partition function of G’ has the LYP, we assume there is a <img alt="\lambda'" src="https://latex.codecogs.com/png.latex?%5Clambda%27" style="vertical-align: middle;" title="\lambda'"/> such that <img alt="A\lambda' + D = 0" src="https://latex.codecogs.com/png.latex?A%5Clambda%27%20%2B%20D%20%3D%200" style="vertical-align: middle;" title="A\lambda' + D = 0"/>. For this to be true,<br/>
<img alt="|\lambda'| = \frac{|D|}{|A|} " src="https://latex.codecogs.com/png.latex?%7C%5Clambda%27%7C%20%3D%20%5Cfrac%7B%7CD%7C%7D%7B%7CA%7C%7D%20" style="vertical-align: middle;" title="|\lambda'| = \frac{|D|}{|A|} "/><br/>
However, this means that <img alt="|\lambda'| \leq 1" src="https://latex.codecogs.com/png.latex?%7C%5Clambda%27%7C%20%5Cleq%201" style="vertical-align: middle;" title="|\lambda'| \leq 1"/> so there is no solution where <img alt="|\lambda'| &gt; 1" src="https://latex.codecogs.com/png.latex?%7C%5Clambda%27%7C%20%3E%201" style="vertical-align: middle;" title="|\lambda'| &gt; 1"/> such that the partition function of G’ is 0 and so it has the LYP.</li>
</ul>
<p>The final part of this proof is to construct the original graph. We observe is that for a single edge, the partition function has the LYP. In this case, we easily write out the partition function for 2 vertices:<br/>
<img alt="z_1z_2 + B z_1 + B z_2 + 1" src="https://latex.codecogs.com/png.latex?z_1z_2%20%2B%20B%20z_1%20%2B%20B%20z_2%20%2B%201" style="vertical-align: middle;" title="z_1z_2 + B z_1 + B z_2 + 1"/><br/>
Suppose <img alt="|z_1| &gt; 1" src="https://latex.codecogs.com/png.latex?%7Cz_1%7C%20%3E%201" style="vertical-align: middle;" title="|z_1| &gt; 1"/>: then <img alt="z_1z_2 + Bz_1 + Bz_2 + 1=0" src="https://latex.codecogs.com/png.latex?z_1z_2%20%2B%20Bz_1%20%2B%20Bz_2%20%2B%201%3D0" style="vertical-align: middle;" title="z_1z_2 + Bz_1 + Bz_2 + 1=0"/> would imply <img alt="|z_2| = \frac{|1+Bz_1|}{|z_1 + B|}" src="https://latex.codecogs.com/png.latex?%7Cz_2%7C%20%3D%20%5Cfrac%7B%7C1%2BBz_1%7C%7D%7B%7Cz_1%20%2B%20B%7C%7D" style="vertical-align: middle;" title="|z_2| = \frac{|1+Bz_1|}{|z_1 + B|}"/>. However, this is just the Möbius transform mapping the exterior of the unit disk to the interior, so <img alt="|z_2| \leq 1" src="https://latex.codecogs.com/png.latex?%7Cz_2%7C%20%5Cleq%201" style="vertical-align: middle;" title="|z_2| \leq 1"/>. Thus, it cannot be the case that both <img alt="z_1" src="https://latex.codecogs.com/png.latex?z_1" style="vertical-align: middle;" title="z_1"/> and <img alt="z_2" src="https://latex.codecogs.com/png.latex?z_2" style="vertical-align: middle;" title="z_2"/> have absolute value greater than one.</p>
<p>Since single edges have the LYP. We break the graph into single edges, with copies of vertices. These copies are then contracted and we build up the graph to show that the partition function has the LYP.</p>
<p>Knowing that the partition function has the LYP, direct application of the Lee-Yang theorem guarantees that the roots are on the unit circle in the complex plane.</p>
<h1 id="conclusion">Conclusion</h1>
<p>Although there exists an explicit formula for the partition function, the challenge lies in computing the quantity in polynomial time. Andrej Risteski’s lecture focused on two less-explored methods in theoretical computer science – namely variational inference and Taylor series approximations – to find provably approximate estimates. Both of these approaches are relatively new and replete with open problems.</p></div></content><updated planet:format="November 11, 2018 02:24 AM">2018-11-11T02:24:14Z</updated><published planet:format="November 11, 2018 02:24 AM">2018-11-11T02:24:14Z</published><category term="physics"/><author><name>Suproteem Sarkar</name></author><source><id>https://windowsontheory.org</id><logo>https://s0.wp.com/i/buttonw-com.png</logo><link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/><link href="https://windowsontheory.org" rel="alternate" type="text/html"/><link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/><link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/><subtitle>A Research Blog</subtitle><title>Windows On Theory</title><updated planet:format="December 17, 2018 05:29 AM">2018-12-17T05:29:53Z</updated><planet:module>toc</planet:module><planet:format>atom10</planet:format><planet:http_last_modified>Mon, 17 Dec 2018 01:41:40 GMT</planet:http_last_modified><planet:bozo>false</planet:bozo><planet:items_per_page>40</planet:items_per_page><planet:css-id>windows-on-theory</planet:css-id><planet:face>wot.png</planet:face><planet:name>Windows on Theory</planet:name><planet:http_location>https://windowsontheory.org/feed/</planet:http_location><planet:http_status>301</planet:http_status></source></entry>
