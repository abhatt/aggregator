<?xml version="1.0" encoding="utf-8"?><entry xml:lang="en" xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/"><id>http://speedupblogger.wordpress.com/?p=136</id><link href="https://speedupblogger.wordpress.com/2012/10/11/a-fundamental-theorem-on-optimality-and-speedup-guest-post-by-amir-ben-amram/" rel="alternate" type="text/html"/><title>A Fundamental Theorem on Optimality and Speedup (guest post by Amir Ben-Amram)</title><summary>Leonid Levin is known for several fundamental contributions to Complexity Theory. The most widely known is surely the notion of “universal search problem,” a concept similar to (and developed concurrently with) NP-completeness. Next, we might cite the proof of the existence of optimal “honest” algorithms for search problems (an honest algorithm uses a given verifier […]<div class="commentbar"><p/></div></summary><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Leonid Levin is known for several fundamental contributions to Complexity Theory. The most widely known is surely the notion of “universal search problem,” a concept similar to (and developed concurrently with) NP-completeness. Next, we might cite the proof of the existence of optimal “honest” algorithms for search problems (an honest algorithm uses a given verifier to check its result; expositions of this theorem can be found, among else, <a href="http://research.microsoft.com/en-us/um/people/gurevich/annotated.htm#78">here</a> and <a href="http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&amp;tid=8208">here</a>).  This is an important result regarding the tension between speedup and the existence of optimal algorithms, and will surely be discussed here in the future.</p>
<p>Then, there is a third result, which seems to be less widely known, though it definitely ought to: I will call it <strong>The Fundamental Theorem of Complexity Theory</strong>, a name given by Meyer and Winklmann to a theorem they published in 1979, following related work including (Meyer and Fischer, 1972) and (Schnorr and Stumpf, 1975). As with the NP story, similar ideas were being invented by Levin in the USSR at about the same time.  Several years later, with Levin relocated to Boston, these lines of research united and the ultimate, tightest, polished form of the theorem was formed, and presented in a <a href="http://www.sciencedirect.com/science/article/pii/0304397595001638">short paper</a> by Levin and, thankfully, a longer “complete exposition” by Seiferas and Meyer (<a href="https://www.sciencedirect.com/science/article/pii/016800729400026Y">here</a> – my thanks to Janos Simon for pointing this paper out to me). Seiferas and Meyer did not name it <em>The Fundamental Theorem</em>, perhaps to avoid ambiguity, but I think that it does deserve a name more punchy than “a characterization of realizable space complexities” (the title of the article).</p>
<p>My purpose in this blog post is to give a brief impression of this result and its significance to the study of speedup phenomena.  The reader who becomes sufficiently interested can turn to the complete exposition mentioned (another reason to do so is the details I omit, for instance concerning partial functions).</p>
<h3>Why this theorem is really fundamental</h3>
<blockquote><p><strong>Some Definitions: </strong>An<strong> algorithm</strong> will mean, henceforth, a Turing machine with a read-only input tape and a worktape whose alphabet is <img alt="\{0,1\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B0%2C1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{0,1\}"/> (the machine can sense the end of the tape – so no blank necessary).  <strong>Program</strong> also means such a machine, but emphasizes that its “code” is written out as a string. <strong>Complexity</strong> will mean space complexity as measured on the worktape. For a machine <img alt="e" class="latex" src="https://s0.wp.com/latex.php?latex=e&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e"/>, its space complexity function is denoted by <img alt="S_e" class="latex" src="https://s0.wp.com/latex.php?latex=S_e&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S_e"/>. A function that is the <img alt="S_e" class="latex" src="https://s0.wp.com/latex.php?latex=S_e&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S_e"/> of some <img alt="e" class="latex" src="https://s0.wp.com/latex.php?latex=e&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e"/> is known as <em>space-constructible</em>.</p>
<p>Note that up to a certain translation overhead, results will apply to any Turing-equivalent model and a suitable complexity measure (technically, a Blum measure).</p>
<p>The Seiferas-Meyer paper makes use of a variety of clever programming techniques for space-bounded Turing machines; one example is universal simulation with a constant <em>additive</em> overhead.</p></blockquote>
<p>I will argue that this theorem formulates an (almost) ultimate answer to the following (vague) question: “what can be said about optimal algorithms and speedup in general, that is, not regarding specific problems of interest?”</p>
<p>Examples of things that can be said are <strong>Blum’s speedup theorem</strong>: there exist problems with arbitrarily large speedup; the <strong>Hierarchy theorem</strong>: there do exist problems that have an optimal algorithm, at various levels of complexity (the type of result they prove is called a <em>compression theorem</em>, which unfortunately creates confusion with the well-known tape compression theorem. The appelation <em>hierarchy theorem</em> may bring the right kind of theorem to mind).</p>
<p>As shown by Seiferas and Meyer, these results, among others, can all be derived from the Fundamental Theorem, and for our chosen measure of space, the results are particularly tight: their Compression Theorem states that for any space-constructible function <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/>, there are algorithms of that complexity that cannot be sped up by more than an <em>additive</em> constant. So, even a tiny <img alt="(1-\varepsilon)" class="latex" src="https://s0.wp.com/latex.php?latex=%281-%5Cvarepsilon%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(1-\varepsilon)"/> multiplicative speedup is ruled out for these algorithms – and the algorithm may be assumed to compute a predicate (so, the size of the output is one bit and is not responsible for the complexity).</p>
<p>An important step to this result is the choice of a clever way to express the notion of “a problem’s complexity” (more specifically, the complexity of computing a given function). To the readership of this blog it may be clear that such a description cannot be, as one may naïvely assume, a single function that describes the complexity of a single, best algorithm for the given problem. The good answer is a so-called <em>complexity set</em>. This is the set of all functions <img alt="S_e" class="latex" src="https://s0.wp.com/latex.php?latex=S_e&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S_e"/> for machines <img alt="e" class="latex" src="https://s0.wp.com/latex.php?latex=e&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e"/> that solve the given function (Meyer and Fischer introduced a similar concept, <em>complexity sequence</em>, which is specifically intended to describe problems with speedup).</p>
<p>How can a complexity set be specified? Since we are talking about a set of <em>computable</em> functions here (in fact, <em>space-constructible</em>), it can be given as a set <img alt="\cal E" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ccal+E&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\cal E"/> of <em>programs </em>that compute the functions. This is called a <em>complexity specification</em>. The gist of the Theorem is that it tells us which sets of programs <em>do</em> represent the complexity of something – moreover, it offers a choice of equivalent characterizations (an always-useful type of result).</p>
<blockquote><p>Clearly, if a <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f"/> can be computed in space <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/>, it can also be computed in a larger space bound; it can also be computed in a space bound smaller by a constant (a nice exercise in TM hacking – note that we have fixed the alphabet). If <img alt="S_1" class="latex" src="https://s0.wp.com/latex.php?latex=S_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S_1"/> and <img alt="S_2" class="latex" src="https://s0.wp.com/latex.php?latex=S_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S_2"/> are space bounds that suffice, then <img alt="\min(S_1,S_2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmin%28S_1%2CS_2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\min(S_1,S_2)"/> does too (another Turing-machine programming trick). So, we can assume that a set of programs <img alt="\cal E" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ccal+E&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\cal E"/> represent the <em>closure</em> of the corresponding set of functions under the above rules. We can also allow it to include programs that compute functions which are not space-constructible: they will not be space bounds themselves, but will imply that constructible functions above them are. So, even a single program can represent an infinite family of time bounds: specifically, the bounds <img alt="S_e" class="latex" src="https://s0.wp.com/latex.php?latex=S_e&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S_e"/> lower-bounded (up to a constant) by the given function.</p></blockquote>
<h3>A statement of the theorem (roughly) and consequences</h3>
<p><strong>Theorem.</strong> Both of the following are ways to specify all existing complexity sets:</p>
<ul>
<li>Sets <img alt="\cal E" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ccal+E&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\cal E"/> of <em>programs</em> described by <img alt="\Sigma_2" class="latex" src="https://s0.wp.com/latex.php?latex=%5CSigma_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Sigma_2"/> predicates (i.e., <img alt="{\cal E} = \{ e \mid \exists a \forall b \, P(a,b,e)\}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+E%7D+%3D+%5C%7B+e+%5Cmid+%5Cexists+a+%5Cforall+b+%5C%2C+P%28a%2Cb%2Ce%29%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{\cal E} = \{ e \mid \exists a \forall b \, P(a,b,e)\}"/>, where <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P"/> is a decidable predicate).</li>
<li>Singletons.</li>
</ul>
<p>The last item I find the most surprising. It is also very powerful. For any machine <img alt="e" class="latex" src="https://s0.wp.com/latex.php?latex=e&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e"/>, the fact that <img alt="\{e\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Be%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{e\}"/> is a complexity specification tells us there there is a function (in fact, a predicate) computable in space <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/> <strong>if and only if</strong> <img alt="S \ge S_e - O(1)" class="latex" src="https://s0.wp.com/latex.php?latex=S+%5Cge+S_e+-+O%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S \ge S_e - O(1)"/>. Here is our Compression theorem!</p>
<p>The first characterization is important when descriptions by an infinite number of functions are considered. For example, let us prove the following:</p>
<p><strong>Theorem.</strong> There is a decision problem, solvable in polynomial (in fact, quadratic) space, that has no best (up to constant factors) algorithm.</p>
<p>Proof. Let <img alt="e_k" class="latex" src="https://s0.wp.com/latex.php?latex=e_k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e_k"/> be a program that computes <img alt="\lceil n^{1+\frac{1}{k}} \rceil" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clceil+n%5E%7B1%2B%5Cfrac%7B1%7D%7Bk%7D%7D+%5Crceil&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lceil n^{1+\frac{1}{k}} \rceil"/>, written so that <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> is just a hard-coded constant. The idea is for the set of <img alt="e_k" class="latex" src="https://s0.wp.com/latex.php?latex=e_k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e_k"/> to be recursively enumerable. Note that all these functions are constructible, that is, prospective space bounds.</p>
<p>Then, by the fundamental theorem, there is a decision problem solvable in space <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/> if and only if <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/> is at least one of these functions (up to an additive constant). If some algorithm for this problem takes at least <img alt="\lceil n^{1+\frac{1}{k}} \rceil" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clceil+n%5E%7B1%2B%5Cfrac%7B1%7D%7Bk%7D%7D+%5Crceil&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lceil n^{1+\frac{1}{k}} \rceil"/> space, then there is also a solution in <img alt="\lceil n^{1+\frac{1}{k+1}} \rceil" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clceil+n%5E%7B1%2B%5Cfrac%7B1%7D%7Bk%2B1%7D%7D+%5Crceil&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lceil n^{1+\frac{1}{k+1}} \rceil"/>, and so forth.</p>
<h3>Limitations and some loose ends</h3>
<p>As exciting as I find this theorem, it has its limitations. Not all the speedup-related results seem to follow from it; for instance, the other Levin’s theorem doesn’t (or I couldn’t see how). Also, results like those given <a href="http://onlinelibrary.wiley.com/doi/10.1002/malq.19920380112/abstract">here</a> and <a href="http://www.sciencedirect.com/science/article/pii/0304397595000666">here</a> about the measure, or topological category, of sets like the functions that have or do not have speedup, do not seem to follow. In fact, Seiferas and Meyer only prove a handful of “classic” results like Blum speedup, the Compression theorem and a Gap theorem. What new questions about complexity sets <em>can</em> be asked and answered using these techniques?</p>
<p>Another limitation is that for complexity measures other than space we do not have such tight results. So, for example, for Turing-machine <em>time</em> we are stuck with the not-so-tight hierarchy theorems proved by diagonalization, padding etc. (see references in the <a href="http://complexityspeedup.wikia.com/wiki/Speedup_in_Computational_Complexity_and_Proof_Theory_Wiki"> bibliography</a>). Is this a problem with our proof methods? Or could some surprising speedup phenomenon be lurking there?</p>
<p>[Oct 16, 2012. Fixed error in last theorem]</p></div></content><updated planet:format="October 11, 2012 12:19 AM">2012-10-11T00:19:11Z</updated><published planet:format="October 11, 2012 12:19 AM">2012-10-11T00:19:11Z</published><category term="General"/><author><name>amirbenamram</name></author><source><id>https://speedupblogger.wordpress.com</id><logo>https://s0.wp.com/i/buttonw-com.png</logo><link href="https://speedupblogger.wordpress.com/feed/" rel="self" type="application/atom+xml"/><link href="https://speedupblogger.wordpress.com" rel="alternate" type="text/html"/><link href="https://speedupblogger.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/><link href="https://speedupblogger.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/><subtitle>Are there familiar computational problems with no best algorithm?</subtitle><title>Speedup in Computational Complexity</title><updated planet:format="December 17, 2018 05:29 AM">2018-12-17T05:29:56Z</updated><planet:module>toc</planet:module><planet:format>atom10</planet:format><planet:http_last_modified>Mon, 13 Aug 2018 19:00:30 GMT</planet:http_last_modified><planet:bozo>false</planet:bozo><planet:items_per_page>40</planet:items_per_page><planet:css-id>speedup-in-computational-complexity</planet:css-id><planet:face>speedup.png</planet:face><planet:name>Speedup in Computational Complexity</planet:name><planet:http_location>https://speedupblogger.wordpress.com/feed/</planet:http_location><planet:http_status>301</planet:http_status></source></entry>
