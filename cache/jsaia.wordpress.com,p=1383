<?xml version="1.0" encoding="utf-8"?><entry xml:lang="en" xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/"><id>http://jsaia.wordpress.com/?p=1383</id><link href="https://jsaia.wordpress.com/2016/02/10/simons-institute-boot-camp-on-counting-complexity-and-phase-transitions/" rel="alternate" type="text/html"/><title>Simons institute boot camp on counting complexity and phase transitions</title><summary>[The following report on the Counting Complexity and the Phase Transitions Boot Camp at the Simons Institute was written by my student Abhinav Aggarwal] 2016 Boot Camp on Counting Complexity and Phase Transitions Jan 25-28, Simons Institute for the Theory of Computing, University of California, Berkeley Recently I visited the Simons Institute at UC Berkeley to […]<div class="commentbar"><p/></div></summary><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><i>[The following report on the Counting Complexity and the Phase Transitions Boot Camp at the Simons Institute was written by my student <a href="https://www.linkedin.com/in/abhinav-aggarwal-2b8a8822">Abhinav Aggarwal</a>]</i></p>
<h3 style="text-align: justify;"><a class="URL" href="https://simons.berkeley.edu/workshops/counting2016-boot-camp">2016 Boot Camp on Counting Complexity and Phase Transitions</a></h3>
<h3 style="text-align: justify;">Jan 25-28, Simons Institute for the Theory of Computing, University of California, Berkeley</h3>
<hr class="line"/>
<p class="Subsubsection-" style="text-align: justify;">Recently I visited the Simons Institute at UC Berkeley to attend a boot camp on counting complexity and phase transitions. The program for the same is available <a href="https://simons.berkeley.edu/programs/counting2016">here</a>. There were a total of 16 talks spread over 4 days, with various speakers covering topics from basics of counting complexity, dichotomy theorems, Markov chain mixing times and random instances. All the videos from the lectures are available <a href="https://www.youtube.com/channel/UCW1C2xOfXsIzPgjXyuhkw9g">here</a>. The following is a brief summary of talks in two areas that seemed interesting to me and are related to my research at UNM.</p>
<p class="Subsubsection-" style="text-align: justify;"><a href="https://sites.google.com/site/nayantarabhatnagar/">Nayantara Bhatnagar</a> from University of Delaware and <a href="https://www.cs.rit.edu/~ib/">Ivona Bezakova</a> from Rochester Institute of Technology presented 3 talks about properties of Markov chains and their mixing times, along with a couple of applications of the same. The talks started with basics of Markov chain properties and conditions under which a given chain is ergodic (irreducible, aperiodic and finite). Once an ergodic Markov chain is obtained, a metric called total variation distance is defined between the initial distribution on the states and the stationary distribution of this chain (which is unique because of ergodicity). This distance measures how close the former is to latter, purely on the basis of the difference in probability mass allocated by the former to the events in the sample space.</p>
<p class="Subsubsection-" style="text-align: justify;">A well-known result is that for stochastic matrices (like the transition probability matrix for Markov chains), the decrease in the distance from the current distribution to the stationary distribution over time depends solely on the spectral gap. The smaller this gap, the larger the distance, and consequently, the more time taken by the Markov chain to approach its stationary distribution. This property is exploited by techniques called Markov chain Monte Carlo (MCMC), which are used heavily for sampling from non-trivial distributions. Some examples of this technique that were presented in the talks include sampling a uniformly random coloring of a given undirected graph and approximate counting of the number of matchings in a given graph. The details of the construction and proof can be found <a href="https://simons.berkeley.edu/sites/default/files/docs/4177/bezakovamcmcbootcampday1.pdf">here</a>.</p>
<p class="Subsubsection-" style="text-align: justify;">The talks continued with a discussion of various techniques that are popular while sampling and counting using Markov chains. Four of these techniques are:</p>
<ol>
<li class="Subsubsection-" style="text-align: justify;"><strong>Almost uniform sampler</strong> – Given a tolerance parameter <em>d &gt; </em>0, produce a sample from the distribution that is within <em>d</em> total variation distance from the uniform distribution.</li>
<li class="Subsubsection-" style="text-align: justify;"><strong>Fully polynomial almost uniform sampler (FPAUS) – </strong>An almost uniform sampler that runs in time polynomial in the input size and log 1/<em>d</em><em> </em>.</li>
<li class="Subsubsection-" style="text-align: justify;"><strong>Randomized approximation scheme – </strong>Given a counting problem and a tolerance parameter ε&gt;0, produce a count which is within ±ε of the actual count with probability at least 3/4.</li>
<li class="Subsubsection-" style="text-align: justify;"><strong>Fully polynomial randomized approximation scheme (FPRAS) – </strong>A randomized approximation scheme that runs in time polynomial in the input size and (1/ε).</li>
</ol>
<p>Upon defining these techniques, our goal then becomes to design a Markov chain with small mixing time so that the runtime of FPRAS can be minimized. The talks further discussed details about coupling theory, both between Markov chains and probability distributions. Nayantara presented these concepts using coupling between two sequences of coin tosses and the famous card shuffling example, which is used to prove that the top-to-random shuffle takes O(n log n) steps to produce a perfect shuffle of a deck of n cards. Within these many steps, the total variation distance of the Markov chain underlying this shuffling reaches sufficiently close to the uniform distribution.</p>
<p>Another session that I thoroughly enjoyed was the one on approximate counting by <a href="https://www.cs.ox.ac.uk/people/leslieann.goldberg/">Leslie Ann Goldberg</a>. The main topic covered in her talk was relative complexity and its relation to the #BIS problem (Bipartite Independent Sets). She started with the discussion of the Potts model and the partition function in the context. This was in relation to the number of proper q-colorings in a given graph. The aim of the model was to approximate the count of these colorings. She discussed an FRPAS algorithm for the same, which outputs a rational number z such that for a given tolerance ε &gt; 0 and the actual count C, we have Pr (Ce^-ε &lt;= z &lt;= Ce^ε) &gt;= 3/4. The counting problem using FRPAS can be NP Hard in general., however Leslie showed that the problem cannot be much harder than that.</p>
<p>An outline of the proof uses the bisection technique by Vazirani and Valiant, which shows that #SAT can be approximated by a probabilistic polynomial time Turing machine using an oracle for SAT. Leslie then defined the relative complexity for approximate counting using AP-reductions from a function f to a function g. Concretely, a function f is AP-reducible to a function g if 1) there exists a randomized algorithm A to compute f using an oracle for g, and 2) A is a randomized approximation scheme for f whenever the oracle is a randomized approximation scheme for g. This makes the class of functions with an FPRAS closed under AP-reductions.</p>
<p>An immediate dichotomy result that comes out of this formulation is that if NP != RP, then within #P, all FPRASable problems form a class and the rest remain unFPRASable. All problems in a given class are AP-inter-reducible, but an FPRAS doesn’t necessarily exist unless NP = RP. However, within the class of unFPRASable problems, a dichotomy further exists. This class is partitioned into two subclasses, one of which consists of problems that are AP-reducible to #SAT and the other consists of problems that are AP-reducible to #BIS. Leslie concluded her talk by giving an example of this trichotomy in the context of graph-homomorphisms.</p></div></content><updated planet:format="February 10, 2016 04:09 PM">2016-02-10T16:09:47Z</updated><published planet:format="February 10, 2016 04:09 PM">2016-02-10T16:09:47Z</published><category term="Uncategorized"/><category term="randomized algorithms"/><category term="theory"/><author><name>Jared</name></author><source><id>https://jsaia.wordpress.com</id><logo>https://s0.wp.com/i/buttonw-com.png</logo><link href="https://jsaia.wordpress.com/feed/" rel="self" type="application/atom+xml"/><link href="https://jsaia.wordpress.com" rel="alternate" type="text/html"/><link href="https://jsaia.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/><link href="https://jsaia.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/><subtitle>Distributed Algorithms and Security</subtitle><title>Machinations</title><updated planet:format="December 17, 2018 05:29 AM">2018-12-17T05:29:32Z</updated><planet:module>toc</planet:module><planet:format>atom10</planet:format><planet:http_last_modified>Mon, 03 Sep 2018 10:34:51 GMT</planet:http_last_modified><planet:bozo>false</planet:bozo><planet:items_per_page>40</planet:items_per_page><planet:css-id>jared-saia</planet:css-id><planet:face>saia.png</planet:face><planet:name>Jared Saia</planet:name><planet:filters>category.py?cats=theory</planet:filters><planet:http_location>https://jsaia.wordpress.com/feed/</planet:http_location><planet:http_status>301</planet:http_status></source></entry>

