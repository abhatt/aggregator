<?xml version="1.0" encoding="utf-8"?><entry xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/"><id>tag:blogger.com,1999:blog-19908808.post-3461807339876779003</id><link href="http://andysresearch.blogspot.com/feeds/3461807339876779003/comments/default" rel="replies" type="application/atom+xml"/><link href="http://www.blogger.com/comment.g?blogID=19908808&amp;postID=3461807339876779003" rel="replies" type="text/html"/><link href="http://www.blogger.com/feeds/19908808/posts/default/3461807339876779003" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/19908808/posts/default/3461807339876779003" rel="self" type="application/atom+xml"/><link href="http://andysresearch.blogspot.com/2008/05/complexity-calisthenics-part-i.html" rel="alternate" type="text/html"/><title>Complexity Calisthenics (Part I)</title><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Today I want to describe and recommend a paper I quite enjoyed: <a href="http://citeseer.ist.psu.edu/mansour02computational.html">The Computational Complexity of Universal Hashing</a> by Mansour, Nisan, and Tiwari (henceforth MNT).  I think that this paper, while not overly demanding technically, is likely to stretch readers' brains in several interesting directions at once.<br/><br/>As a motivation, consider the following question, which has vexed some of us since the third grade: <span style="font-style: italic;">why is multiplication so hard?</span><br/><br/>The algorithm we learn in school takes time quadratic in the bit-length of the input numbers.  This is far from optimal, and inspired work over the years has brought the running time ever-closer to the conjecturally-optimal n log n; Martin Furer published a <a href="http://www.cse.psu.edu/~furer/Papers/mult.pdf">breakthrough</a> in STOC 2007, and there may have been improvements since then.  But compare this with addition, which can easily be performed with linear time and logarithmic space (simultaneously).  Could we hope for anything similar?  (I don't believe that any of the fast multiplication algorithms run in sublinear space, although I could be mistaken.  <a href="http://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations">Here</a> is Wiki's summary of existing algorithms for arithmetic.)<br/><br/>As MNT observe, the question is made especially interesting when we observed that multiplication could be just as easily achieved in linear time/logspace... <span style="font-style: italic;">if</span> we were willing to accept a different representation for our integer inputs!  Namely, if we're given the prime factorizations of the inputs, we simply add corresponding exponents to determine the product.  There are two hitches, though: first, we'd have to accept the promise that the prime factorizations given really do involve primes (it's not at all clear that we'd have the time/space to check this, even with the recent advances in primality testing); second, and more germane to our discussion, <span style="font-style: italic;">addition</span> just got much harder!<br/><br/>The situation is similar over finite fields of prime order (<span style="font-style: italic;">Z_p</span>): in standard representation, addition is easy and multiplication is less so, while if we represent numbers as powers of a fixed <a href="http://en.wikipedia.org/wiki/Primitive_root_modulo_n">primitive root</a>, the reverse becomes true.  This suggests a woeful but intriguing possibility: perhaps no matter <span style="font-style: italic;">how</span> we represent numbers, one of the two operations must be computationally complex, even though we have latitude to 'trade off' between <span style="font-style: italic;">+</span> and <span style="font-style: italic;">*</span>.  So we are invited to consider<br/><br/><span style="font-weight: bold;">Mental Stretch #1</span>: Can we prove 'representation-independent' complexity lower bounds?<br/><br/><span style="font-weight: bold;">Stretch #2</span>: Can we prove such lower bounds in the form of tradeoffs between two component problems, as seems necessary here?<br/><br/>In the setting of finite-field arithmetic, MNT answer 'yes' to both problems.  The lower bounds they give, however, are not expressed simply in terms of time usage or space usage, but instead as the <span style="font-style: italic;">product</span> of these two measures.  Thus we have<br/><br/><span style="font-weight: bold;">Stretch #3</span>: Prove 'time-space tradeoffs' for computational problems.<br/><br/>To be clear, all three of these 'stretches' had been made in various forms in work prior to MNT; I'm just using their paper as a good example.<br/><br/>The combination of these three elements certainly makes the task seem daunting.  But MNT have convinced me, and I hope to suggest to you, that with the right ideas it's not so hard.  As the paper title indicates, their point of departure is Universal Hashing (UH)--an algorithmic technique in which finite fields have already proved useful.  <span style="font-style: italic;">Use upper bounds to prove lower bounds.</span>  We can call this another Stretch, or call it wisdom of the ages, but it deserves to be stated.<br/><br/>So what is UH?  Fix a domain <span style="font-style: italic;">D</span> and a range <span style="font-style: italic;">R</span>.  a (finite) family <span style="font-style: italic;">H</span> of functions from <span style="font-style: italic;">D</span> to <span style="font-style: italic;">R</span> is called a Universal Hash Family (UHF) if the following holds: <br/><br/>For every pair of distinct elements <span style="font-style: italic;">d, d'</span> in <span style="font-style: italic;">D</span>, <br/><br/>and for every pair of (not necessarily distinct) elements <span style="font-style: italic;">r, r'</span> in <span style="font-style: italic;">R</span>,<br/><br/>if we pick a function <span style="font-style: italic;">h</span> at random from <span style="font-style: italic;">H</span>,<br/><br/>Prob<span style="font-style: italic;">[h(d) = r,  h(d') = r' ]  =  1/|R|^2</span>.<br/><br/>In other words, the randomly chosen <span style="font-style: italic;">h</span> behaves just like a truly random function from <span style="font-style: italic;">D</span> to <span style="font-style: italic;">R</span> when we restrict attention to any two domain elements.  (In typical applications we hope to save time and randomness, since <span style="font-style: italic;">H</span> may be much smaller than the space of all functions.)<br/><br/><br/>Here is what MNT do: they prove that implementing <span style="font-style: italic;">any</span> UHF necessitates a complexity lower bound in the form of a time-space product.  (To 'implement' a UHF <span style="font-style: italic;">H</span> is to compute the function <span style="font-style: italic;">f_H(h, x) = h(x)</span>.)  <br/><br/>This is in fact the main message of the paper, but they obtain our desired application to <span style="font-style: italic;">+</span> and <span style="font-style: italic;">*</span> as a corollary, by citing the well known fact that, fixing a prime field <span style="font-style: italic;">Z_p = D = R</span>,<br/><br/><span style="font-style: italic;">H = {h_{a, b}(<span style="font-weight: bold;">x</span>) = a*<span style="font-weight: bold;">x</span> + b  mod p}</span><br/><br/>is a UHF, where <span style="font-style: italic;">a, b</span> range over all field elements.  (Left as an easy exercise.)<br/><br/>Note the gain from this perspective: implementing a UHF is a 'representation-invariant' property of a function, so Stretch 1 becomes possible.  Moreover, Stretch 2 now makes more sense: it is only jointly that <span style="font-style: italic;">+</span> and <span style="font-style: italic;">*</span> define a UHF, so whatever complexity measure <span style="font-style: italic;">M</span> we lower-bound for UHFs implies only a tradeoff between <span style="font-style: italic;">+</span> and <span style="font-style: italic;">*</span>.<br/><br/>It remains only to sketch the proof of time-space tradeoffs for UHFs, which in fact is a manageable argument along classic lines (the basic form of the argument is attributed to earlier work by Borodin and Cook).  The upshot for us will be that for any program computing (any representation of) <span style="font-style: italic;">f(a, b, x) = a*x + b</span> over an n-digit prime modulus, if <span style="font-style: italic;">T</span> denotes worst-case time usage and <span style="font-style: italic;">S</span> worst-case space, <span style="font-style: italic;">T*S = Omega(n^2)</span>.  Very nice! (Although what this actually implies about the larger of the <span style="font-style: italic;">individual</span> time-space products of <span style="font-style: italic;">+</span> and <span style="font-style: italic;">*</span> under this representation is not clear to me at the moment.)<br/><br/>Let's adjourn for today... wouldn't want to pull a muscle.</div><div class="commentbar"><p/><span class="commentbutton" href="http://andysresearch.blogspot.com/feeds/3461807339876779003/comments/default"/><a href="http://andysresearch.blogspot.com/feeds/3461807339876779003/comments/default"><img class="commenticon" src="/images/feed-icon.png"/> Subscribe to comments</a><![CDATA[  | ]]><a href="http://www.blogger.com/comment.g?blogID=19908808&amp;postID=3461807339876779003"><img class="commenticon" src="/images/post-icon.png"/> Post a comment</a></div></content><updated planet:format="May 01, 2008 06:02 PM">2008-05-01T18:02:00Z</updated><published planet:format="May 01, 2008 06:02 PM">2008-05-01T18:02:00Z</published><category scheme="http://www.blogger.com/atom/ns#" term="complexity"/><author><name>Andy D</name><email>noreply@blogger.com</email><uri>http://www.blogger.com/profile/03897281159810085972</uri></author><source><id>tag:blogger.com,1999:blog-19908808</id><category term="general math"/><category term="puzzles"/><category term="complexity"/><category term="geometry"/><category term="computability"/><category term="probability"/><category term="crypto"/><category term="grad life"/><category term="miscellaneous"/><category term="the infinite"/><author><name>Andy D</name><email>noreply@blogger.com</email><uri>http://www.blogger.com/profile/03897281159810085972</uri></author><link href="http://andysresearch.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/19908808/posts/default" rel="self" type="application/atom+xml"/><link href="http://andysresearch.blogspot.com/" rel="alternate" type="text/html"/><link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/><link href="http://www.blogger.com/feeds/19908808/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/><subtitle>Sporadic notes on mathematical and non-mathematical topics, from a student of computational complexity.</subtitle><title>Andy's Math/CS page</title><updated planet:format="September 17, 2018 03:07 AM">2018-09-17T03:07:28Z</updated><planet:module>toc</planet:module><planet:format>atom10</planet:format><planet:http_etag>W/&quot;d66a335240ef742735b44062ca3f549f737aff49db8f80c3136efce6dd24696a&quot;</planet:http_etag><planet:http_last_modified>Mon, 17 Sep 2018 03:07:28 GMT</planet:http_last_modified><planet:bozo>false</planet:bozo><planet:items_per_page>40</planet:items_per_page><planet:css-id>andy-s-math-cs-page</planet:css-id><planet:face>andy.jpeg</planet:face><planet:name>Andy's Math/CS page</planet:name><planet:http_status>200</planet:http_status></source></entry>
