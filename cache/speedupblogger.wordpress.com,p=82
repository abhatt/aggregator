<?xml version="1.0" encoding="utf-8"?><entry xml:lang="en" xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/"><id>http://speedupblogger.wordpress.com/?p=82</id><link href="https://speedupblogger.wordpress.com/2012/09/22/on-the-recent-progress-on-matrix-multiplication-algorithms-guest-post-by-virginia-vassilevska-williams/" rel="alternate" type="text/html"/><title>On the recent progress on matrix multiplication algorithms (guest post by Virginia Vassilevska Williams)</title><summary>A central question in the theory of algorithms is to determine the constant , called the exponent of matrix multiplication. This constant is defined as the infimum of all real numbers such that for all there is an algorithm for multiplying matrices running in time . Until the late 1960s it was believed that , […]<div class="commentbar"><p/></div></summary><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A central question in the theory of algorithms is to determine the constant <img alt="\omega" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\omega"/>, called the exponent of matrix multiplication. This constant is defined as the infimum of all real numbers such that for all <img alt="\varepsilon&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon%3E0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\varepsilon&gt;0"/> there is an algorithm for multiplying <img alt="n\times n" class="latex" src="https://s0.wp.com/latex.php?latex=n%5Ctimes+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n\times n"/> matrices running in time <img alt="O(n^{\omega+\varepsilon})" class="latex" src="https://s0.wp.com/latex.php?latex=O%28n%5E%7B%5Comega%2B%5Cvarepsilon%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="O(n^{\omega+\varepsilon})"/>. Until the late 1960s it was believed that <img alt="\omega=3" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega%3D3&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\omega=3"/>, i.e. that no improvement can be found for the problem. In 1969, <a href="http://www.springerlink.com/content/w71w4445t7m71gm5/" title="Gaussian elimination is not optimal">Strassen</a> surprised everyone by showing that two <img alt="n\times n" class="latex" src="https://s0.wp.com/latex.php?latex=n%5Ctimes+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n\times n"/> matrices can be multiplied in <img alt="O(n^{2.81})" class="latex" src="https://s0.wp.com/latex.php?latex=O%28n%5E%7B2.81%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="O(n^{2.81})"/> time. This discovery spawned a twenty-year-long extremely productive time in which the upper bound on <img alt="\omega" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\omega"/> was gradually lowered to <img alt="2.376" class="latex" src="https://s0.wp.com/latex.php?latex=2.376&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2.376"/>. After a twenty-year stall, some very recent research has brought the upper bound down to <img alt="2.373" class="latex" src="https://s0.wp.com/latex.php?latex=2.373&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2.373"/>.</p>
<p><strong>Bilinear algorithms and recursion.</strong><br/>
Strassen’s approach was to exploit the inherent recursive nature of matrix multiplication: the product of two <img alt="kn\times kn" class="latex" src="https://s0.wp.com/latex.php?latex=kn%5Ctimes+kn&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="kn\times kn"/> matrices can be viewed as the product of two <img alt="k\times k" class="latex" src="https://s0.wp.com/latex.php?latex=k%5Ctimes+k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k\times k"/> matrices, the entries of which are <img alt="n\times n" class="latex" src="https://s0.wp.com/latex.php?latex=n%5Ctimes+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n\times n"/> matrices. Suppose that we have an algorithm <em>ALG</em> that runs in <img alt="o(k^3)" class="latex" src="https://s0.wp.com/latex.php?latex=o%28k%5E3%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="o(k^3)"/> time and multiplies two <img alt="k\times k" class="latex" src="https://s0.wp.com/latex.php?latex=k%5Ctimes+k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k\times k"/> matrices. Then one can envision obtaining a fast recursive algorithm for multiplying <img alt="k^i\times k^i" class="latex" src="https://s0.wp.com/latex.php?latex=k%5Ei%5Ctimes+k%5Ei&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k^i\times k^i"/> matrices (for any integer <img alt="i&gt;1" class="latex" src="https://s0.wp.com/latex.php?latex=i%3E1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i&gt;1"/>) as well: view the <img alt="k^i\times k^i" class="latex" src="https://s0.wp.com/latex.php?latex=k%5Ei%5Ctimes+k%5Ei&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k^i\times k^i"/> matrices as <img alt="k\times k" class="latex" src="https://s0.wp.com/latex.php?latex=k%5Ctimes+k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k\times k"/> matrices the entries of which are <img alt="k^{i-1}\times k^{i-1}" class="latex" src="https://s0.wp.com/latex.php?latex=k%5E%7Bi-1%7D%5Ctimes+k%5E%7Bi-1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k^{i-1}\times k^{i-1}"/> matrices; then multiply the <img alt="k\times k" class="latex" src="https://s0.wp.com/latex.php?latex=k%5Ctimes+k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k\times k"/> matrices using <em>ALG</em> and when <em>ALG</em> requires us to multiply two matrix entries, recurse.</p>
<p>This approach only works, provided that the operations that <em>ALG</em> performs on the matrix entries make sense as matrix operations: e.g. entry multiplication, taking linear combination of entries etc. One very general type of such algorithm is the so called <em>bilinear</em> algorithm: Given two matrices <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/> and <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="B"/>, compute <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r"/> products</p>
<p style="text-align: center;"><img alt="P_l = (\sum_{i,j} u_{ijl} A[i,j])(\sum_{ij} v_{ijl} B[i,j])," class="latex" src="https://s0.wp.com/latex.php?latex=P_l+%3D+%28%5Csum_%7Bi%2Cj%7D+u_%7Bijl%7D+A%5Bi%2Cj%5D%29%28%5Csum_%7Bij%7D+v_%7Bijl%7D+B%5Bi%2Cj%5D%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P_l = (\sum_{i,j} u_{ijl} A[i,j])(\sum_{ij} v_{ijl} B[i,j]),"/></p>
<p>i.e. take <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r"/> possibly different linear combinations of entries of <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/> and multiply each one with a possibly different linear combination of entries of <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="B"/>. Then, compute each entry of the product <img alt="AB" class="latex" src="https://s0.wp.com/latex.php?latex=AB&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="AB"/> as a linear combination of the <img alt="P_l" class="latex" src="https://s0.wp.com/latex.php?latex=P_l&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P_l"/>: <img alt="AB[i,j]=\sum_{l} w_{ijl} P_l" class="latex" src="https://s0.wp.com/latex.php?latex=AB%5Bi%2Cj%5D%3D%5Csum_%7Bl%7D+w_%7Bijl%7D+P_l&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="AB[i,j]=\sum_{l} w_{ijl} P_l"/>.</p>
<p>Given a bilinear algorithm <em>ALG</em> for multiplying two <img alt="k\times k" class="latex" src="https://s0.wp.com/latex.php?latex=k%5Ctimes+k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k\times k"/> matrices (for constant <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>) that computes <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r"/> products <img alt="P_l" class="latex" src="https://s0.wp.com/latex.php?latex=P_l&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P_l"/>, the recursive approach that multiplies <img alt="k^i\times k^i" class="latex" src="https://s0.wp.com/latex.php?latex=k%5Ei%5Ctimes+k%5Ei&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k^i\times k^i"/> matrices using <em>ALG</em> gives a bound <img alt="\omega\leq \log_k r" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega%5Cleq+%5Clog_k+r&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\omega\leq \log_k r"/>. To see this, notice that the number of additions that one has to do is no more than <img alt="3rk^2" class="latex" src="https://s0.wp.com/latex.php?latex=3rk%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="3rk^2"/>: at most <img alt="2k^2" class="latex" src="https://s0.wp.com/latex.php?latex=2k%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2k^2"/> to compute the linear combinations for each <img alt="P_l" class="latex" src="https://s0.wp.com/latex.php?latex=P_l&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P_l"/> and at most <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r"/> for each of the <img alt="k^2" class="latex" src="https://s0.wp.com/latex.php?latex=k%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k^2"/> outputs <img alt="AB[i,j]" class="latex" src="https://s0.wp.com/latex.php?latex=AB%5Bi%2Cj%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="AB[i,j]"/>. Since matrix addition takes linear time in the matrix size, we have a recurrence of the form <img alt="T(k^i) = r T(k^{i-1}) + O(rk^{2i})" class="latex" src="https://s0.wp.com/latex.php?latex=T%28k%5Ei%29+%3D+r+T%28k%5E%7Bi-1%7D%29+%2B+O%28rk%5E%7B2i%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T(k^i) = r T(k^{i-1}) + O(rk^{2i})"/>.</p>
<p>As long as <img alt="r&lt;k^3" class="latex" src="https://s0.wp.com/latex.php?latex=r%3Ck%5E3&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r&lt;k^3"/> we get a nontrivial bound on <img alt="\omega" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\omega"/>. Strassen’s famous algorithm used <img alt="k=2" class="latex" src="https://s0.wp.com/latex.php?latex=k%3D2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k=2"/> and <img alt="r=7" class="latex" src="https://s0.wp.com/latex.php?latex=r%3D7&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r=7"/> thus showing that <img alt="\omega\leq \log_2 7" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega%5Cleq+%5Clog_2+7&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\omega\leq \log_2 7"/>. A lot of work went into getting better and better “base algorithms” for varying constants <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>. Methods such as Pan’s method of trilinear aggregation were developed. This approach culminated in<a href="http://dl.acm.org/citation.cfm?id=1382584" title="Strassen's algorithm is not optimal"> Pan’s algorithm</a> (1978) for multiplying <img alt="70\times 70" class="latex" src="https://s0.wp.com/latex.php?latex=70%5Ctimes+70&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="70\times 70"/> matrices that used <img alt="143,640" class="latex" src="https://s0.wp.com/latex.php?latex=143%2C640&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="143,640"/> products and hence showed that <img alt="\omega\leq \log_{70} 143,640&lt;2.796" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega%5Cleq+%5Clog_%7B70%7D+143%2C640%3C2.796&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\omega\leq \log_{70} 143,640&lt;2.796"/><strong>.</strong></p>
<p><strong>Approximate algorithms and Schonhage’s theorem.</strong><br/>
A further step was to look at more general algorithms, so called <em>approximate</em> bilinear algorithms. In the definition of a bilinear algorithm the coefficients <img alt="u_{ijl},v_{ijl},w_{ijl}" class="latex" src="https://s0.wp.com/latex.php?latex=u_%7Bijl%7D%2Cv_%7Bijl%7D%2Cw_%7Bijl%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="u_{ijl},v_{ijl},w_{ijl}"/> were constants. In an approximate algorithm, these coefficients can be formal linear combinations of integer powers of an indeterminate, <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda"/> (e.g. <img alt="2\lambda^2-5\lambda^{-3}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Clambda%5E2-5%5Clambda%5E%7B-3%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2\lambda^2-5\lambda^{-3}"/>). The entries of the product <img alt="AB" class="latex" src="https://s0.wp.com/latex.php?latex=AB&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="AB"/> are then only “approximately” computed, in the sense that <img alt="AB[i,j]=\sum_{l} w_{ijl} P_l + O(\lambda)" class="latex" src="https://s0.wp.com/latex.php?latex=AB%5Bi%2Cj%5D%3D%5Csum_%7Bl%7D+w_%7Bijl%7D+P_l+%2B+O%28%5Clambda%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="AB[i,j]=\sum_{l} w_{ijl} P_l + O(\lambda)"/>, where the <img alt="O(\lambda)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28%5Clambda%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="O(\lambda)"/> term is a linear combination of <em>positive</em> powers of <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda"/>. The term “approximate” comes from the intuition that if you set <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda"/> to be close to <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0"/>, then the algorithm would get the product almost exactly.</p>
<p>Interestingly enough, <a href="http://epubs.siam.org/doi/abs/10.1137/0209053" title="approximate and exact bilinear algorithms">Bini et al.</a> (1980) showed that when dealing with the asymptotic complexity of matrix multiplication, approximate algorithms suffice for obtaining bounds on <img alt="\omega" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\omega"/>. This is not obvious! What Bini et al. show, in a sense, is that as the size of the matrices grows, the “approximation” part can be replaced by a sort of bookkeeping which does not present an overhead asymptotically. The upshot is that if there is an <em>approximate</em> bilinear algorithm that computes <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r"/> products <img alt="P_l" class="latex" src="https://s0.wp.com/latex.php?latex=P_l&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P_l"/> to compute the product of two <img alt="k\times k" class="latex" src="https://s0.wp.com/latex.php?latex=k%5Ctimes+k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k\times k"/> matrices, then <img alt="\omega\leq \log_k r" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega%5Cleq+%5Clog_k+r&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\omega\leq \log_k r"/>.</p>
<p><a href="http://www.sciencedirect.com/science/article/pii/0020019079901133" title="approximate algorithm by Bini et al">Bini et al. </a>(1979) gave the first approximate bilinear algorithm for a matrix product. Their algorithm used <img alt="10" class="latex" src="https://s0.wp.com/latex.php?latex=10&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="10"/> entry products to multiply a <img alt="2\times 3" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Ctimes+3&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2\times 3"/> matrix with a <img alt="3\times 3" class="latex" src="https://s0.wp.com/latex.php?latex=3%5Ctimes+3&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="3\times 3"/> matrix. Although this algorithm is for rectangular matrices, it can easily be converted into one for square matrices: a <img alt="12\times 12" class="latex" src="https://s0.wp.com/latex.php?latex=12%5Ctimes+12&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="12\times 12"/> matrix is a <img alt="2\times 3" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Ctimes+3&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2\times 3"/> matrix with entries that are <img alt="3\times 2" class="latex" src="https://s0.wp.com/latex.php?latex=3%5Ctimes+2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="3\times 2"/> matrices with entries that are <img alt="2\times 2" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Ctimes+2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2\times 2"/> matrices, and so multiplying <img alt="12\times 12" class="latex" src="https://s0.wp.com/latex.php?latex=12%5Ctimes+12&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="12\times 12"/> matrices can be done recursively using Bini et al.’s algorithm three times, taking <img alt="1,000" class="latex" src="https://s0.wp.com/latex.php?latex=1%2C000&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1,000"/> entry products. Hence <img alt="\omega\leq \log_{12} 1,000&lt;2.78" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega%5Cleq+%5Clog_%7B12%7D+1%2C000%3C2.78&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\omega\leq \log_{12} 1,000&lt;2.78"/>.</p>
<p><a href="http://epubs.siam.org/action/showAbstract?page=434&amp;volume=10&amp;issue=3&amp;journalCode=smjcat" title="tau theorem">Schonhage</a> (1981) developed a sophisticated theory involving the bilinear complexity of rectangular matrix multiplication that showed that approximate bilinear algorithms are even more powerful. His paper culminated in something called the Schonhage <img alt="\tau" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctau&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\tau"/>-theorem, or the asymptotic sum inequality. This theorem is one of the most useful tools in designing and analyzing matrix multiplication algorithms.</p>
<p>Schonhage’s <img alt="\tau" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctau&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\tau"/>-theorem says roughly the following. Suppose we have several instances of matrix multiplication, each involving matrices of possibly different dimensions, and we are somehow able to design an approximate bilinear algorithm that solves all instances and uses fewer products than would be needed when computing each instance separately. Then this bilinear algorithm can be used to multiply (larger) square matrices and would imply a nontrivial bound on <img alt="\omega" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\omega"/>.</p>
<p>What is interesting about Schonhage’s theorem is that it is believed that when it comes to <em>exact</em> bilinear algorithms, one cannot use fewer products to compute several instances than one would use by just computing each instance separately. This is known as Strassen’s additivity conjecture. Schonhage showed that the additivity conjecture is false for <em>approximate</em> bilinear algorithms. In particular, he showed that one can approximately compute the product of a <img alt="3\times 1" class="latex" src="https://s0.wp.com/latex.php?latex=3%5Ctimes+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="3\times 1"/> by a <img alt="1\times 3" class="latex" src="https://s0.wp.com/latex.php?latex=1%5Ctimes+3&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1\times 3"/> vector and the product of a <img alt="1\times 4" class="latex" src="https://s0.wp.com/latex.php?latex=1%5Ctimes+4&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1\times 4"/> by a <img alt="4\times 1" class="latex" src="https://s0.wp.com/latex.php?latex=4%5Ctimes+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="4\times 1"/> vector together using only <img alt="10" class="latex" src="https://s0.wp.com/latex.php?latex=10&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="10"/> entry products, whereas any exact bilinear algorithm would need at least <img alt="3\cdot 3+4 = 13" class="latex" src="https://s0.wp.com/latex.php?latex=3%5Ccdot+3%2B4+%3D+13&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="3\cdot 3+4 = 13"/> products. His theorem then implied <img alt="\omega&lt;2.55" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega%3C2.55&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\omega&lt;2.55"/>, and this was a huge improvement over the previous bound of Bini et al.</p>
<p><strong>Using fast solutions for problems that are not matrix multiplications.</strong><br/>
The next realization was that there is no immediate reason why the “base algorithm” that we use for our recursion has to compute a matrix product at all. Let us focus on the following family of computational problems. We are given two vectors <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/> and <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/> and we want to compute a third vector <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="z"/>. The dependence of <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="z"/> on <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/> and <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/> is given by a three-dimensional tensor <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t"/> as follows: <img alt="z_k = \sum_{ij} t_{ijk} x_i y_j" class="latex" src="https://s0.wp.com/latex.php?latex=z_k+%3D+%5Csum_%7Bij%7D+t_%7Bijk%7D+x_i+y_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="z_k = \sum_{ij} t_{ijk} x_i y_j"/>. The vector <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="z"/> is a bilinear form. The tensor <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t"/> can be arbitrary, but let us focus on the case where <img alt="t_{ijk}\in \{0,1\}" class="latex" src="https://s0.wp.com/latex.php?latex=t_%7Bijk%7D%5Cin+%5C%7B0%2C1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t_{ijk}\in \{0,1\}"/>. Notice that <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t"/> completely determines the computational problem. Some examples of such bilinear problems are polynomial multiplication and of course matrix multiplication. For polynomial multiplication, <img alt="t_{ijk} = 1" class="latex" src="https://s0.wp.com/latex.php?latex=t_%7Bijk%7D+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t_{ijk} = 1"/> if and only if <img alt="j=k-i" class="latex" src="https://s0.wp.com/latex.php?latex=j%3Dk-i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="j=k-i"/>, and for matrix multiplication, <img alt="t_{(i,i'),(j,j'),(k,k')}=1" class="latex" src="https://s0.wp.com/latex.php?latex=t_%7B%28i%2Ci%27%29%2C%28j%2Cj%27%29%2C%28k%2Ck%27%29%7D%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t_{(i,i'),(j,j'),(k,k')}=1"/> if and only if <img alt="i'=j, j'=k" class="latex" src="https://s0.wp.com/latex.php?latex=i%27%3Dj%2C+j%27%3Dk&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i'=j, j'=k"/> and <img alt="k'=i" class="latex" src="https://s0.wp.com/latex.php?latex=k%27%3Di&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k'=i"/>.</p>
<p>The nice thing about these bilinear problems is that one can easily extend the theory of bilinear algorithms to them. A bilinear algorithm computing a problem instance for tensor <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t"/> computes <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r"/> products <img alt="P_l" class="latex" src="https://s0.wp.com/latex.php?latex=P_l&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P_l"/> of the form <img alt="(\sum_{i} u_{il} x_i)(\sum_j v_{jl} y_j)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Csum_%7Bi%7D+u_%7Bil%7D+x_i%29%28%5Csum_j+v_%7Bjl%7D+y_j%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(\sum_{i} u_{il} x_i)(\sum_j v_{jl} y_j)"/> and then sets <img alt="z_k=\sum_k w_{kl} P_l" class="latex" src="https://s0.wp.com/latex.php?latex=z_k%3D%5Csum_k+w_%7Bkl%7D+P_l&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="z_k=\sum_k w_{kl} P_l"/>. Here, an algorithm is nontrivial if the number of products <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r"/> that it computes is less than the number of positions <img alt="t_{ijk}" class="latex" src="https://s0.wp.com/latex.php?latex=t_%7Bijk%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t_{ijk}"/> where the tensor <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t"/> is nonzero.</p>
<p>In order to be able to talk about recursion for general bilinear problems, it is useful to define the tensor product <img alt="t\otimes t'" class="latex" src="https://s0.wp.com/latex.php?latex=t%5Cotimes+t%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t\otimes t'"/> of two tensors <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t"/> and <img alt="t'" class="latex" src="https://s0.wp.com/latex.php?latex=t%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t'"/>: <img alt="(t\otimes t')_{(i,i'),(j,j'),(k,k')} = t_{ijk}\cdot t_{i'j'k'}" class="latex" src="https://s0.wp.com/latex.php?latex=%28t%5Cotimes+t%27%29_%7B%28i%2Ci%27%29%2C%28j%2Cj%27%29%2C%28k%2Ck%27%29%7D+%3D+t_%7Bijk%7D%5Ccdot+t_%7Bi%27j%27k%27%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(t\otimes t')_{(i,i'),(j,j'),(k,k')} = t_{ijk}\cdot t_{i'j'k'}"/>. Thus, the bilinear problem defined by <img alt="t\otimes t'" class="latex" src="https://s0.wp.com/latex.php?latex=t%5Cotimes+t%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t\otimes t'"/> can be viewed as a bilinear problem <img alt="z_k = \sum_{ij} t_{ijk} x_i y_j" class="latex" src="https://s0.wp.com/latex.php?latex=z_k+%3D+%5Csum_%7Bij%7D+t_%7Bijk%7D+x_i+y_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="z_k = \sum_{ij} t_{ijk} x_i y_j"/> defined by <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t"/>, where each product <img alt="x_iy_j" class="latex" src="https://s0.wp.com/latex.php?latex=x_iy_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_iy_j"/> is actually itself a bilinear problem <img alt="z_{ijk'}=\sum_{i'j'} t'_{i'j'k'} x_{ii'}y_{jj'}" class="latex" src="https://s0.wp.com/latex.php?latex=z_%7Bijk%27%7D%3D%5Csum_%7Bi%27j%27%7D+t%27_%7Bi%27j%27k%27%7D+x_%7Bii%27%7Dy_%7Bjj%27%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="z_{ijk'}=\sum_{i'j'} t'_{i'j'k'} x_{ii'}y_{jj'}"/> defined by <img alt="t'" class="latex" src="https://s0.wp.com/latex.php?latex=t%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t'"/>.</p>
<p>This allows one to compute an instance of the problem defined by <img alt="t\otimes t'" class="latex" src="https://s0.wp.com/latex.php?latex=t%5Cotimes+t%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t\otimes t'"/> using an algorithm for <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t"/> and an algorithm for <img alt="t'" class="latex" src="https://s0.wp.com/latex.php?latex=t%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t'"/>. One can similarly define the <img alt="k^{th}" class="latex" src="https://s0.wp.com/latex.php?latex=k%5E%7Bth%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k^{th}"/> tensor power of a tensor <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t"/> as tensor-multiplying <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t"/> by itself <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> times. Then any bilinear algorithm computing an instance defined by <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t"/> using <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r"/> entry products can be used recursively to compute the <img alt="k^{th}" class="latex" src="https://s0.wp.com/latex.php?latex=k%5E%7Bth%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k^{th}"/> tensor power of <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t"/> using <img alt="r^k" class="latex" src="https://s0.wp.com/latex.php?latex=r%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r^k"/> products, just as in the case of matrix multiplication.</p>
<p>A crucial development in the study of matrix multiplication algorithms was the discovery that sometimes algorithms for bilinear problems that do not look at all like matrix products can be converted into matrix multiplication algorithms. This was first shown by Strassen in the development of his<a href="http://www.computer.org/portal/web/csdl/doi/10.1109/SFCS.1986.52" title="Strassen's laser method"> “laser method”</a> and was later exploited in the work of <a href="http://www.sciencedirect.com/science/article/pii/S0747717108800132" title="Coppersmith Winograd algorithm">Coppersmith and Winograd</a> (1987,1990). The basic idea of the approach is as follows.</p>
<p>Consider a bilinear problem <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P"/> for which you have a really nice approximate algorithm <em>ALG</em> that uses <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r"/> entry products. Take the <img alt="n^{th}" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%7Bth%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n^{th}"/> tensor power <img alt="P^n" class="latex" src="https://s0.wp.com/latex.php?latex=P%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P^n"/> of <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P"/> (for large <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/>), and use<em> ALG</em> recursively to compute <img alt="P^n" class="latex" src="https://s0.wp.com/latex.php?latex=P%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P^n"/> using <img alt="r^n" class="latex" src="https://s0.wp.com/latex.php?latex=r%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r^n"/> entry products. <img alt="P^n" class="latex" src="https://s0.wp.com/latex.php?latex=P%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P^n"/> is a bilinear problem that computes a long vector <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="z"/> from two long vectors <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/> and <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/>. Suppose that we can embed the product <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C"/> of two <img alt="N\times N" class="latex" src="https://s0.wp.com/latex.php?latex=N%5Ctimes+N&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N\times N"/> matrices <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/> and <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="B"/> into <img alt="P^n" class="latex" src="https://s0.wp.com/latex.php?latex=P%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P^n"/> as follows: we put each entry of <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/> into some position of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/> and set all other positions of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/> to <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0"/>, we similarly put each entry of <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="B"/> into some position of <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/> and set all other positions of <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/> to <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0"/>, and finally we argue that each entry of the product <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C"/> is in some position of the computed vector <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="z"/> (all other <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="z"/> entries are <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0"/>). Then we would have a bilinear algorithm for computing the product of two <img alt="N\times N" class="latex" src="https://s0.wp.com/latex.php?latex=N%5Ctimes+N&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N\times N"/> matrices using <img alt="r^n" class="latex" src="https://s0.wp.com/latex.php?latex=r%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r^n"/> entry products, and hence <img alt="\omega\leq \log_N r^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega%5Cleq+%5Clog_N+r%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\omega\leq \log_N r^n"/>.</p>
<p>The goal is to make <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N"/> as large of a function of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> as possible, thus minimizing the upper bound on <img alt="\omega" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\omega"/>.</p>
<p>Strassen’s laser method and Coppersmith and Winograd’s paper, and even Schonhage’s <img alt="\tau" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctau&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\tau"/>-theorem, present ways of embedding a matrix product into a large tensor power of a different bilinear problem. The approaches differ in the starting algorithm and in the final matrix product embedding. We’ll give a very brief overview of the Coppersmith-Winograd algorithm.</p>
<p><strong>The Coppersmith-Winograd algorithm.</strong><br/>
The bilinear problem that Coppersmith and Winograd start with is as follows. Let <img alt="q\geq 3" class="latex" src="https://s0.wp.com/latex.php?latex=q%5Cgeq+3&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q\geq 3"/> be an integer. Then we are given two vectors <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/> and <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/> of length <img alt="q+2" class="latex" src="https://s0.wp.com/latex.php?latex=q%2B2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q+2"/> and we want to compute a vector <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="z"/> of length <img alt="q+2" class="latex" src="https://s0.wp.com/latex.php?latex=q%2B2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q+2"/> defined as follows:</p>
<p style="text-align: center;"><img alt="z_0 = \sum_{i=1}^q (x_iy_i+x_0y_{q+1})+x_{q+1}y_0" class="latex" src="https://s0.wp.com/latex.php?latex=z_0+%3D+%5Csum_%7Bi%3D1%7D%5Eq+%28x_iy_i%2Bx_0y_%7Bq%2B1%7D%29%2Bx_%7Bq%2B1%7Dy_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="z_0 = \sum_{i=1}^q (x_iy_i+x_0y_{q+1})+x_{q+1}y_0"/>,</p>
<p style="text-align: center;"><img alt="z_i=x_iy_0+x_0y_i" class="latex" src="https://s0.wp.com/latex.php?latex=z_i%3Dx_iy_0%2Bx_0y_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="z_i=x_iy_0+x_0y_i"/> for <img alt="i\in \{1,\ldots, q\}" class="latex" src="https://s0.wp.com/latex.php?latex=i%5Cin+%5C%7B1%2C%5Cldots%2C+q%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i\in \{1,\ldots, q\}"/>, and <img alt="z_{q+1}=x_0y_0" class="latex" src="https://s0.wp.com/latex.php?latex=z_%7Bq%2B1%7D%3Dx_0y_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="z_{q+1}=x_0y_0"/>.</p>
<p>Notice that <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="z"/> is far from being a matrix product. However, it is related to <img alt="6" class="latex" src="https://s0.wp.com/latex.php?latex=6&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="6"/> matrix products:</p>
<p style="text-align: center;"><img alt="\sum_{i=1}^q x_iy_i" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bi%3D1%7D%5Eq+x_iy_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sum_{i=1}^q x_iy_i"/> which is the inner product of two <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q"/>-length vectors,</p>
<p style="text-align: center;"><img alt="x_0y_{q+1}" class="latex" src="https://s0.wp.com/latex.php?latex=x_0y_%7Bq%2B1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_0y_{q+1}"/>, <img alt="x_{q+1}y_0" class="latex" src="https://s0.wp.com/latex.php?latex=x_%7Bq%2B1%7Dy_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_{q+1}y_0"/>, and <img alt="x_0y_0" class="latex" src="https://s0.wp.com/latex.php?latex=x_0y_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_0y_0"/>, which are three scalar products, and</p>
<p style="text-align: center;">the two matrix products computing <img alt="x_iy_0" class="latex" src="https://s0.wp.com/latex.php?latex=x_iy_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_iy_0"/> and <img alt="x_0y_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_0y_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_0y_i"/> for <img alt="i\in \{1,\ldots, q\}" class="latex" src="https://s0.wp.com/latex.php?latex=i%5Cin+%5C%7B1%2C%5Cldots%2C+q%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i\in \{1,\ldots, q\}"/> which are both products of a vector with a scalar.</p>
<p>If we could somehow convert Coppersmith and Winograd’s bilinear problem into one of computing these <img alt="6" class="latex" src="https://s0.wp.com/latex.php?latex=6&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="6"/> products as <em>independent </em>instances, then we would be able to use Schonhage’s <img alt="\tau" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctau&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\tau"/>-theorem. Unfortunately, however, the <img alt="6" class="latex" src="https://s0.wp.com/latex.php?latex=6&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="6"/> matrix products are merged in a strange way, and it is unclear whether one can get anything meaningful out of an algorithm that solves this bilinear problem.</p>
<p>Coppersmith and Winograd develop a multitude of techniques to show that when one takes a large tensor power of the starting bilinear problem, one can actually decouple these merged matrix products, and one can indeed apply the <img alt="\tau" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctau&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\tau"/>-theorem. The <img alt="\tau" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctau&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\tau"/>-theorem then gives the final embedding of a large matrix product into a tensor power of the original construction, and hence defines a matrix multiplication algorithm.</p>
<p>Their approach combines several impressive ingredients: sets avoiding <img alt="3" class="latex" src="https://s0.wp.com/latex.php?latex=3&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="3"/>-term arithmetic progressions, hashing and the probabilistic method. The algorithm computing their base bilinear problem is also impressive. The number of entry products it computes is <img alt="q+2" class="latex" src="https://s0.wp.com/latex.php?latex=q%2B2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q+2"/>, which is exactly the length of the output vector <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="z"/>! That is, their starting algorithm is optimal for the particular problem that they are solving.</p>
<p>What is not optimal, however, is the analysis of how good of a matrix product algorithm one can obtain from the base algorithm. Coppersmith and Winograd noticed this themselves: They first applied their analysis to the original bilinear algorithm and obtained an embedding of an <img alt="f(n)\times f(n)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28n%29%5Ctimes+f%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(n)\times f(n)"/> matrix product into the <img alt="2n" class="latex" src="https://s0.wp.com/latex.php?latex=2n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2n"/>-tensor power of the bilinear problem for some explicit function <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f"/>. (Then they took <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> to go to infinity and obtained <img alt="\omega&lt; 2.388" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega%3C+2.388&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\omega&lt; 2.388"/>.) Then they noticed, that if one applies the analysis to the second tensor power of the original construction, then one obtains an embedding of an <img alt="f'(n)\times f'(n)" class="latex" src="https://s0.wp.com/latex.php?latex=f%27%28n%29%5Ctimes+f%27%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f'(n)\times f'(n)"/> matrix product into the same <img alt="2n" class="latex" src="https://s0.wp.com/latex.php?latex=2n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2n"/>-tensor power, where <img alt="f'(n)&gt;f(n)" class="latex" src="https://s0.wp.com/latex.php?latex=f%27%28n%29%3Ef%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f'(n)&gt;f(n)"/>. That is, although one is considering embeddings into the same (<img alt="2n" class="latex" src="https://s0.wp.com/latex.php?latex=2n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2n"/>) tensor power of the construction, the analysis crucially depends on which tensor power of the construction you start from! This led to the longstanding bound <img alt="\omega&lt;2.376" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega%3C2.376&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\omega&lt;2.376"/>. Coppersmith and Winograd left as an open problem what bound one can get if one starts from the third or larger tensor powers.</p>
<p><strong>The recent improvements on <img alt="\omega" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\omega"/>.</strong><br/>
It seems that many researchers attempted to apply the analysis to the third tensor power of the construction, but this somehow did not improve bound on <img alt="\omega" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\omega"/>. Because of this and since each new analysis required a lot of work, the approach was abandoned, at least until 2010. In 2010, <a href="http://www.maths.ed.ac.uk/pg/thesis/stothers.pdf" title="Stothers' PhD thesis">Andrew Stothers</a>  carried through the analysis on the fourth tensor power and discovered that the bound on <img alt="\omega" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\omega"/> can be improved to <img alt="2.374" class="latex" src="https://s0.wp.com/latex.php?latex=2.374&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2.374"/>.</p>
<p>As mentioned earlier, the original Coppersmith-Winograd bilinear problem was related to <img alt="6" class="latex" src="https://s0.wp.com/latex.php?latex=6&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="6"/> different matrix multiplication problems that were merged together. The <img alt="k^{th}" class="latex" src="https://s0.wp.com/latex.php?latex=k%5E%7Bth%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k^{th}"/> tensor power of the bilinear problem is similarly composed of <img alt="poly(k)" class="latex" src="https://s0.wp.com/latex.php?latex=poly%28k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="poly(k)"/> merged instances of simpler bilinear ptoblems, however these instances may no longer be matrix multiplications. When applying a Coppersmith-Winograd-like analysis to the <img alt="k^{th}" class="latex" src="https://s0.wp.com/latex.php?latex=k%5E%7Bth%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k^{th}"/> tensor power, there are two main steps.</p>
<p>The first step involves analyzing each of the <img alt="poly(k)" class="latex" src="https://s0.wp.com/latex.php?latex=poly%28k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="poly(k)"/> bilinear problems, intuitively in terms of how close they are to matrix products; there is a formal definition of the similarity measure called the <em>value </em>of the bilinear form. The second step defines a family of matrix product embeddings in the <img alt="n^{th}" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%7Bth%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n^{th}"/> tensor power in terms of the values. These embeddings are defined via some variables and constraints, and each represents some matrix multiplication algorithm. Finally, one solves a nonlinear optimization program to find the best among these embeddings, essentially finding the best matrix multiplication algorithm in the search space.</p>
<p>Both the Coppersmith-Winograd paper and Stothers’ thesis perform an entirely new analysis for each new tensor power <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>. The main goal of my work was to provide a general framework so that the two steps of the analysis do not have to be redone for each new tensor power <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>. <a href="http://www.cs.berkeley.edu/~virgi/matrixmult.pdf" title="Vassilevska Williams (2012)">My paper</a> first shows that the first step, the analysis of each of the values, can be completely automated by solving linear programs and simple systems of linear equations. This means that instead of proving <img alt="poly(k)" class="latex" src="https://s0.wp.com/latex.php?latex=poly%28k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="poly(k)"/> theorems one only needs to solve <img alt="poly(k)" class="latex" src="https://s0.wp.com/latex.php?latex=poly%28k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="poly(k)"/> linear programs and linear systems, a simpler task. My paper then shows that the second step of the analysis, the theorem defining the search space of algorithms, can also be replaced by just solving a simple system of linear equations. (Amusingly, the fact that matrix multiplication algorithms can be used to solve linear equations implies that good matrix multiplication algorithms can be used to search for better matrix multiplication algorithms.) Together with the final nonlinear program, this presents a fully automated approach to performing a Coppersmith-Winograd-like analysis.</p>
<p>After seeing Stothers’ thesis in the summer of last year, I was impressed by a shortcut he had used in the analysis of the values of the fourth tensor power. This shortcut gave a way to use recursion in the analysis, and I was able to incorporate it in my analysis to show that the number of linear programs and linear systems one would need to solve to compute the values for the <img alt="k^{th}" class="latex" src="https://s0.wp.com/latex.php?latex=k%5E%7Bth%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k^{th}"/> tensor power drops down to <img alt="O(k^2)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28k%5E2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="O(k^2)"/>, at least when <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> is a power of <img alt="2" class="latex" src="https://s0.wp.com/latex.php?latex=2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2"/>. This drop in complexity allowed me to analyze the <img alt="8^{th}" class="latex" src="https://s0.wp.com/latex.php?latex=8%5E%7Bth%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="8^{th}"/> tensor power, thus obtaining an improvement in the bound of <img alt="\omega" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\omega"/>: <img alt="\omega&lt;2.3727" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega%3C2.3727&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\omega&lt;2.3727"/>.</p>
<p>There are several lingering open questions. The most natural one is, how does the bound on <img alt="\omega" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\omega"/> change when applying the analysis to higher and higher tensor powers. I am currently working together with a Stanford undergraduate on this problem: we’ll apply the automated approach to several consecutive powers, hoping to uncover a pattern so that one can then mathematically analyze what bounds on <img alt="\omega" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\omega"/> can be proven with this approach.</p>
<p>A second open question is more far reaching: the Coppersmith-Winograd analysis is not optimal– in a sense it computes an approximation to the best embedding of a matrix product in the <img alt="n^{th}" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%7Bth%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n^{th}"/> tensor power of their bilinear problem. What is the optimal embedding? Can one analyze it mathematically? Can one automate the search for it?</p>
<p>Finally, I am fascinated by automating the search for algorithms for problems. In the special case of matrix multiplication we were able to define a search space of algorithms and then use software to optimize over this search space. What other computational problems can one approach in this way?</p></div></content><updated planet:format="September 22, 2012 03:30 AM">2012-09-22T03:30:33Z</updated><published planet:format="September 22, 2012 03:30 AM">2012-09-22T03:30:33Z</published><category term="Matrix multiplication"/><author><name>pinkfloydie</name></author><source><id>https://speedupblogger.wordpress.com</id><logo>https://s0.wp.com/i/buttonw-com.png</logo><link href="https://speedupblogger.wordpress.com/feed/" rel="self" type="application/atom+xml"/><link href="https://speedupblogger.wordpress.com" rel="alternate" type="text/html"/><link href="https://speedupblogger.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/><link href="https://speedupblogger.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/><subtitle>Are there familiar computational problems with no best algorithm?</subtitle><title>Speedup in Computational Complexity</title><updated planet:format="December 17, 2018 05:29 AM">2018-12-17T05:29:56Z</updated><planet:module>toc</planet:module><planet:format>atom10</planet:format><planet:http_last_modified>Mon, 13 Aug 2018 19:00:30 GMT</planet:http_last_modified><planet:bozo>false</planet:bozo><planet:items_per_page>40</planet:items_per_page><planet:css-id>speedup-in-computational-complexity</planet:css-id><planet:face>speedup.png</planet:face><planet:name>Speedup in Computational Complexity</planet:name><planet:http_location>https://speedupblogger.wordpress.com/feed/</planet:http_location><planet:http_status>301</planet:http_status></source></entry>
