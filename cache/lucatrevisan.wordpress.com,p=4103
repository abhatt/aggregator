<?xml version="1.0" encoding="utf-8"?><entry xml:lang="en" xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/"><id>http://lucatrevisan.wordpress.com/?p=4103</id><link href="https://lucatrevisan.wordpress.com/2017/10/31/beyond-worst-case-analysis-lecture-11/" rel="alternate" type="text/html"/><title>Beyond Worst-Case Analysis: Lecture 11</title><summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Scribed by Neng Huang In which we use the SDP relaxation of the infinity-to-one norm and Grothendieck inequality to give an approximation reconstruction of the stochastic block model. 1. A Brief Review of the Model First, let’s briefly review the … <a href="https://lucatrevisan.wordpress.com/2017/10/31/beyond-worst-case-analysis-lecture-11/">Continue reading <span class="meta-nav">→</span></a></div><div class="commentbar"><p/></div></summary><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p/><p>
 Scribed by Neng Huang </p>
<p/><p>
<em>In which we use the SDP relaxation of the infinity-to-one norm and Grothendieck inequality to give an approximation reconstruction of the stochastic block model.</em></p>
<p>
</p><p><b>1. A Brief Review of the Model </b></p>
<p> First, let’s briefly review the model. We have a random graph <img alt="{G = (V, E)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG+%3D+%28V%2C+E%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G = (V, E)}"/> with an unknown partition of the vertices into two equal parts <img alt="{V_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{V_1}"/> and <img alt="{V_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{V_2}"/>. Edges across the partition are generated independently with probability <img alt="{q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q}"/>, and edges inside the partition are generated independently with probability <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/>. To abbreviate the notation, we let <img alt="{a = pn/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba+%3D+pn%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a = pn/2}"/>, which is the average internal degree, and <img alt="{b = qn/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb+%3D+qn%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b = qn/2}"/>, which is the average external degree. Intuitively, the closer are <img alt="{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a}"/> and <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b}"/>, the more difficult it is to reconstruct the partition. We assume <img alt="{a &gt; b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba+%3E+b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a &gt; b}"/>, although there are also similar results in the complementary model where <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b}"/> is larger than <img alt="{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a}"/>. We also assume <img alt="{b &gt; 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb+%3E+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b &gt; 1}"/> so that the graph is not almost empty.</p>
<p>
We will prove the following two results, the first of which will be proved using Grothendieck inequality. </p>
<ol>
<li> For every <img alt="{\epsilon &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon &gt; 0}"/>, there exists a constant <img alt="{c_\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c_\epsilon}"/> such that if <img alt="{a - b &gt; c_\epsilon\sqrt{a+b}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba+-+b+%3E+c_%5Cepsilon%5Csqrt%7Ba%2Bb%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a - b &gt; c_\epsilon\sqrt{a+b}}"/>, then we can reconstruct the partition up to less than <img alt="{\epsilon n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon n}"/> misclassified vertices.
</li><li> There exists a constant <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> such that if <img alt="{a-b \geq c \sqrt{\log n}\sqrt{a+b}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba-b+%5Cgeq+c+%5Csqrt%7B%5Clog+n%7D%5Csqrt%7Ba%2Bb%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a-b \geq c \sqrt{\log n}\sqrt{a+b}}"/>, then we can do exact reconstruct.
</li></ol>
<p> We note that the first result is essentially tight in the sense that for every <img alt="{\epsilon &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon &gt; 0}"/>, there also exists a constant <img alt="{c_\epsilon'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_%5Cepsilon%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c_\epsilon'}"/> such that if <img alt="{a-b &lt; c_\epsilon'\sqrt{a+b}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba-b+%3C+c_%5Cepsilon%27%5Csqrt%7Ba%2Bb%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a-b &lt; c_\epsilon'\sqrt{a+b}}"/>, then it will be impossible to reconstruct the partition even if an <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/> fraction of misclassified vertices is allowed. Also, the constant <img alt="{c_\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c_\epsilon}"/> will go to infinity as <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/> goes to 0, so if we want more and more accuracy, <img alt="{a-b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba-b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a-b}"/> needs to be a bigger and bigger constant times <img alt="{\sqrt{a+b}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Ba%2Bb%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sqrt{a+b}}"/>. When the constant becomes <img alt="{O(\sqrt{\log n})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt%7B%5Clog+n%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\sqrt{\log n})}"/>, we will get an exact reconstruction as stated in the second result.</p>
<p>
<span id="more-4103"/></p>
<p>
</p><p><b>2. The Algorithm </b></p>
<p> Our algorithm will be based on semi-definite programming. Intuitively, the problem of reconstructing the partition is essentially the same as min-bisection problem, which is to find a balanced cut with the fewest edges. This is because the balanced cut with the fewest expected edges is exactly our hidden cut. Unfortunately, the min-bisection problem is <img alt="{{\bf NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\bf NP}}"/>-hard, so we will use semi-definite programming. The min-bisection problem can be stated as the following program: \begin{equation*}  &amp; {\text{minimize}} &amp; &amp; \sum_{(u, v) \in E} \frac{1}{4}(x_u – x_v)^2<br/>
 &amp; \text{subject to} &amp; &amp; x_v^2 = 1, \forall v \in V <br/>
 &amp;&amp;&amp; \sum_{v \in V}x_v = 0.  \end{equation*} Its semi-definite programming relaxation will be</p>
<p>
<a name="sdp1"/></p><a name="sdp1">
<p align="center"><img alt="\displaystyle  \begin{aligned} &amp; {\text{minimize}} &amp; &amp; \sum_{(u, v) \in E} \frac{1}{4}\|{\bf x}_u - {\bf x}_v\|^2\\ &amp; \text{subject to} &amp; &amp; \|{\bf x}_v\|^2 = 1, \forall v \in V \\ &amp;&amp;&amp; \|\sum_{v \in V}{\bf x}_v\|^2 = 0. \end{aligned} \ \ \ \ \ (1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Baligned%7D+%26+%7B%5Ctext%7Bminimize%7D%7D+%26+%26+%5Csum_%7B%28u%2C+v%29+%5Cin+E%7D+%5Cfrac%7B1%7D%7B4%7D%5C%7C%7B%5Cbf+x%7D_u+-+%7B%5Cbf+x%7D_v%5C%7C%5E2%5C%5C+%26+%5Ctext%7Bsubject+to%7D+%26+%26+%5C%7C%7B%5Cbf+x%7D_v%5C%7C%5E2+%3D+1%2C+%5Cforall+v+%5Cin+V+%5C%5C+%26%26%26+%5C%7C%5Csum_%7Bv+%5Cin+V%7D%7B%5Cbf+x%7D_v%5C%7C%5E2+%3D+0.+%5Cend%7Baligned%7D+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{aligned} &amp; {\text{minimize}} &amp; &amp; \sum_{(u, v) \in E} \frac{1}{4}\|{\bf x}_u - {\bf x}_v\|^2\\ &amp; \text{subject to} &amp; &amp; \|{\bf x}_v\|^2 = 1, \forall v \in V \\ &amp;&amp;&amp; \|\sum_{v \in V}{\bf x}_v\|^2 = 0. \end{aligned} \ \ \ \ \ (1)"/></p>
</a><p><a name="sdp1"/></p>
<p>
Our algorithm will be as follows. </p>
<ul>
<li> Solve the semi-definite programming above.
</li><li> Let <img alt="{{\bf x}_1^\ast, \ldots, {\bf x}_n^\ast}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+x%7D_1%5E%5Cast%2C+%5Cldots%2C+%7B%5Cbf+x%7D_n%5E%5Cast%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\bf x}_1^\ast, \ldots, {\bf x}_n^\ast}"/> be the optimal solution and <img alt="{X = (X_{ij})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX+%3D+%28X_%7Bij%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X = (X_{ij})}"/> such that <img alt="{X_{ij} = \langle{\bf x}_i^\ast, {\bf x}_j^\ast\rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_%7Bij%7D+%3D+%5Clangle%7B%5Cbf+x%7D_i%5E%5Cast%2C+%7B%5Cbf+x%7D_j%5E%5Cast%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X_{ij} = \langle{\bf x}_i^\ast, {\bf x}_j^\ast\rangle}"/>.
</li><li> Find <img alt="{{\bf z} = (z_1, \ldots, z_n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+z%7D+%3D+%28z_1%2C+%5Cldots%2C+z_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\bf z} = (z_1, \ldots, z_n)}"/>, which is the eigenvector corresponding to the largest eigenvalue of <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/>.
</li><li> Let <img alt="{S = \{i : z_i &gt; 0\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS+%3D+%5C%7Bi+%3A+z_i+%3E+0%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S = \{i : z_i &gt; 0\}}"/>, <img alt="{V - S = \{i : z_i \leq 0\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV+-+S+%3D+%5C%7Bi+%3A+z_i+%5Cleq+0%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{V - S = \{i : z_i \leq 0\}}"/>.
</li><li> Output <img alt="{(S, V-S)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28S%2C+V-S%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(S, V-S)}"/> as our partition.
</li></ul>
<p>
Ideally, we want half of the <img alt="{{\bf x}_i^\ast}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+x%7D_i%5E%5Cast%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\bf x}_i^\ast}"/>‘s pointing to one direction, and the other half pointing to the opposite direction. In this ideal case we will have </p>
<p align="center"><img alt="\displaystyle  X = \left(\begin{array}{c | c} {\bf 1} &amp; {\bf -1}\\ {\bf -1} &amp; {\bf 1} \end{array}\right) = \left(\begin{array}{c} 1 \\ \vdots \\1 \\ -1 \\ \vdots \\ -1 \end{array}\right) (1, \ldots, 1, -1, \ldots, -1). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++X+%3D+%5Cleft%28%5Cbegin%7Barray%7D%7Bc+%7C+c%7D+%7B%5Cbf+1%7D+%26+%7B%5Cbf+-1%7D%5C%5C+%7B%5Cbf+-1%7D+%26+%7B%5Cbf+1%7D+%5Cend%7Barray%7D%5Cright%29+%3D+%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+1+%5C%5C+%5Cvdots+%5C%5C1+%5C%5C+-1+%5C%5C+%5Cvdots+%5C%5C+-1+%5Cend%7Barray%7D%5Cright%29+%281%2C+%5Cldots%2C+1%2C+-1%2C+%5Cldots%2C+-1%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  X = \left(\begin{array}{c | c} {\bf 1} &amp; {\bf -1}\\ {\bf -1} &amp; {\bf 1} \end{array}\right) = \left(\begin{array}{c} 1 \\ \vdots \\1 \\ -1 \\ \vdots \\ -1 \end{array}\right) (1, \ldots, 1, -1, \ldots, -1). "/></p>
<p> Then <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> will be a rank-one matrix and <img alt="{(1, \ldots, 1, -1, \ldots, -1)^T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281%2C+%5Cldots%2C+1%2C+-1%2C+%5Cldots%2C+-1%29%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(1, \ldots, 1, -1, \ldots, -1)^T}"/>, which is the indicator vector of the hidden cut, will be its eigenvector with eigenvalue <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>. The remaining eigenvalues of <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> will be all zeros. So finding the largest eigenvector of <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> will reveal the hidden cut. In reality, if <img alt="{a - b &gt; c_\epsilon\sqrt{a+b}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba+-+b+%3E+c_%5Cepsilon%5Csqrt%7Ba%2Bb%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a - b &gt; c_\epsilon\sqrt{a+b}}"/>, then our solution will be almost the same as that in the ideal case, so the cut we get will be almost the same as the hidden cut. Furthermore, if <img alt="{a-b \geq c \sqrt{\log n}\sqrt{a+b}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba-b+%5Cgeq+c+%5Csqrt%7B%5Clog+n%7D%5Csqrt%7Ba%2Bb%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a-b \geq c \sqrt{\log n}\sqrt{a+b}}"/>, then the unique optimal solution of the SDP will be the combinatorial solution of min-bisection problem, that is, in the vector language, the one-dimensional solution.\footnote{“A miracle”, said Luca.}</p>
<p>
</p><p><b>3. Analysis of the Algorithm </b></p>
<p> First, we rearrange the SDP to make it slightly simpler. We have the following SDP:</p>
<p>
<a name="sdp2"/></p><a name="sdp2">
<p align="center"><img alt="\displaystyle  \begin{aligned} &amp; {\text{maximize}} &amp; &amp; \sum_{u,v} A_{uv}\langle {\bf x}_u, {\bf x}_v \rangle\\ &amp; \text{subject to} &amp; &amp; \|{\bf x}_v\|^2 = 1, \forall v \in V \\ &amp;&amp;&amp; \|\sum_{v \in V}{\bf x}_v\|^2 = 0. \end{aligned} \ \ \ \ \ (2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Baligned%7D+%26+%7B%5Ctext%7Bmaximize%7D%7D+%26+%26+%5Csum_%7Bu%2Cv%7D+A_%7Buv%7D%5Clangle+%7B%5Cbf+x%7D_u%2C+%7B%5Cbf+x%7D_v+%5Crangle%5C%5C+%26+%5Ctext%7Bsubject+to%7D+%26+%26+%5C%7C%7B%5Cbf+x%7D_v%5C%7C%5E2+%3D+1%2C+%5Cforall+v+%5Cin+V+%5C%5C+%26%26%26+%5C%7C%5Csum_%7Bv+%5Cin+V%7D%7B%5Cbf+x%7D_v%5C%7C%5E2+%3D+0.+%5Cend%7Baligned%7D+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{aligned} &amp; {\text{maximize}} &amp; &amp; \sum_{u,v} A_{uv}\langle {\bf x}_u, {\bf x}_v \rangle\\ &amp; \text{subject to} &amp; &amp; \|{\bf x}_v\|^2 = 1, \forall v \in V \\ &amp;&amp;&amp; \|\sum_{v \in V}{\bf x}_v\|^2 = 0. \end{aligned} \ \ \ \ \ (2)"/></p>
</a><p><a name="sdp2"/></p>
<p>
We note that SDP<a href="https://lucatrevisan.wordpress.com/feed/#sdp1">1</a> and SDP<a href="https://lucatrevisan.wordpress.com/feed/#sdp2">2</a> have the same optimal solution, because the cost function of SDP<a href="https://lucatrevisan.wordpress.com/feed/#sdp1">1</a> is </p>
<p align="center"><img alt="\displaystyle  \begin{aligned} &amp; \sum_{(u, v) \in E} \frac{1}{4}\|{\bf x}_u - {\bf x}_v\|^2 \\ = &amp; \sum_{u,v} \frac{1}{4}A_{uv}\|{\bf x}_u - {\bf x}_v\|^2 \\ = &amp; \sum_{u,v} \frac{1}{4}A_{uv}(2 - \langle {\bf x}_u, {\bf x}_v \rangle) \\ = &amp; (\sum_{u,v} \frac{1}{2}A_{uv}) - \frac{1}{4}(\sum_{u,v}A_{uv}\langle {\bf x}_u, {\bf x}_v \rangle). \end{aligned} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Baligned%7D+%26+%5Csum_%7B%28u%2C+v%29+%5Cin+E%7D+%5Cfrac%7B1%7D%7B4%7D%5C%7C%7B%5Cbf+x%7D_u+-+%7B%5Cbf+x%7D_v%5C%7C%5E2+%5C%5C+%3D+%26+%5Csum_%7Bu%2Cv%7D+%5Cfrac%7B1%7D%7B4%7DA_%7Buv%7D%5C%7C%7B%5Cbf+x%7D_u+-+%7B%5Cbf+x%7D_v%5C%7C%5E2+%5C%5C+%3D+%26+%5Csum_%7Bu%2Cv%7D+%5Cfrac%7B1%7D%7B4%7DA_%7Buv%7D%282+-+%5Clangle+%7B%5Cbf+x%7D_u%2C+%7B%5Cbf+x%7D_v+%5Crangle%29+%5C%5C+%3D+%26+%28%5Csum_%7Bu%2Cv%7D+%5Cfrac%7B1%7D%7B2%7DA_%7Buv%7D%29+-+%5Cfrac%7B1%7D%7B4%7D%28%5Csum_%7Bu%2Cv%7DA_%7Buv%7D%5Clangle+%7B%5Cbf+x%7D_u%2C+%7B%5Cbf+x%7D_v+%5Crangle%29.+%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{aligned} &amp; \sum_{(u, v) \in E} \frac{1}{4}\|{\bf x}_u - {\bf x}_v\|^2 \\ = &amp; \sum_{u,v} \frac{1}{4}A_{uv}\|{\bf x}_u - {\bf x}_v\|^2 \\ = &amp; \sum_{u,v} \frac{1}{4}A_{uv}(2 - \langle {\bf x}_u, {\bf x}_v \rangle) \\ = &amp; (\sum_{u,v} \frac{1}{2}A_{uv}) - \frac{1}{4}(\sum_{u,v}A_{uv}\langle {\bf x}_u, {\bf x}_v \rangle). \end{aligned} "/></p>
<p> The first term is a constant and the second is the cost function of SDP<a href="https://lucatrevisan.wordpress.com/feed/#sdp2">2</a> with a factor of -1/4.</p>
<p>
Now, consider the cost of SDP<a href="https://lucatrevisan.wordpress.com/feed/#sdp2">2</a> of <img alt="{\chi = (\chi_v)_{v \in V}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cchi+%3D+%28%5Cchi_v%29_%7Bv+%5Cin+V%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\chi = (\chi_v)_{v \in V}}"/> where </p>
<p align="center"><img alt="\displaystyle  \chi_v = \left\{\begin{array}{ll} +1 &amp; \text{if } v \in V_1, \\ -1 &amp; \text{if } v \in V_2. \end{array} \right. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cchi_v+%3D+%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bll%7D+%2B1+%26+%5Ctext%7Bif+%7D+v+%5Cin+V_1%2C+%5C%5C+-1+%26+%5Ctext%7Bif+%7D+v+%5Cin+V_2.+%5Cend%7Barray%7D+%5Cright.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \chi_v = \left\{\begin{array}{ll} +1 &amp; \text{if } v \in V_1, \\ -1 &amp; \text{if } v \in V_2. \end{array} \right. "/></p>
<p> The expected cost will be </p>
<p align="center"><img alt="\displaystyle  \mathop{\mathbb E}_{\text{choice of graph}}(\text{cost of }\chi) = \frac{n(n-1)}{2}p - \frac{n^2}{2}q = n(a-b)-a. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D_%7B%5Ctext%7Bchoice+of+graph%7D%7D%28%5Ctext%7Bcost+of+%7D%5Cchi%29+%3D+%5Cfrac%7Bn%28n-1%29%7D%7B2%7Dp+-+%5Cfrac%7Bn%5E2%7D%7B2%7Dq+%3D+n%28a-b%29-a.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathop{\mathbb E}_{\text{choice of graph}}(\text{cost of }\chi) = \frac{n(n-1)}{2}p - \frac{n^2}{2}q = n(a-b)-a. "/></p>
<p> Since each edge is chosen independently, with high probability our cost will be at least <img alt="{n(a - b) - a - \sqrt{n(a+b)} \geq n(a-b) - O(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%28a+-+b%29+-+a+-+%5Csqrt%7Bn%28a%2Bb%29%7D+%5Cgeq+n%28a-b%29+-+O%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n(a - b) - a - \sqrt{n(a+b)} \geq n(a-b) - O(n)}"/>, which implies that the optimal solution of SDP<a href="https://lucatrevisan.wordpress.com/feed/#sdp2">2</a> will be at least <img alt="{n(a-b) - O(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%28a-b%29+-+O%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n(a-b) - O(n)}"/>. Let <img alt="{{\bf x}_1^\ast, \ldots, {\bf x}_n^\ast}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+x%7D_1%5E%5Cast%2C+%5Cldots%2C+%7B%5Cbf+x%7D_n%5E%5Cast%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\bf x}_1^\ast, \ldots, {\bf x}_n^\ast}"/> be the optimal solution of the SDP, then we have</p>
<p>
<a name="eq1"/> n(a-b) – O(n) &amp; \leq cost(<b>x</b>_1^\ast, \ldots, <b>x</b>_n^\ast)\nonumber <br/>
 &amp; = \sum_{u,v}A_{uv}\langle<b>x</b>_u^\ast, <b>x</b>_v^\ast\rangle\nonumber <br/>
 &amp; = \sum_{u,v}\left(A_{uv} – \frac{a+b}{n}\right)\langle<b>x</b>_u^\ast, <b>x</b>_v^\ast\rangle </p>
<p>
In the last equality we used the fact that <img alt="{\sum_{u,v} \langle{\bf x}_u^\ast, {\bf x}_v^\ast\rangle = \|\sum_u {\bf x}_u^\ast\|^2 = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bu%2Cv%7D+%5Clangle%7B%5Cbf+x%7D_u%5E%5Cast%2C+%7B%5Cbf+x%7D_v%5E%5Cast%5Crangle+%3D+%5C%7C%5Csum_u+%7B%5Cbf+x%7D_u%5E%5Cast%5C%7C%5E2+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sum_{u,v} \langle{\bf x}_u^\ast, {\bf x}_v^\ast\rangle = \|\sum_u {\bf x}_u^\ast\|^2 = 0}"/></p>
<p>
When we used the spectral method last week, we said that the largest eigenvalue of <img alt="{A - \frac{d}{n}J}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA+-+%5Cfrac%7Bd%7D%7Bn%7DJ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A - \frac{d}{n}J}"/> is large, where <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{d}"/> is the average degree. This is because the hidden cut will give us a vector with large Rayleigh quotient. But <img alt="{A - \mathop{\mathbb E} A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA+-+%5Cmathop%7B%5Cmathbb+E%7D+A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A - \mathop{\mathbb E} A}"/> has a relatively small spectral norm, so everything should come from <img alt="{\mathop{\mathbb E} A - \frac{d}{n}J}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathop%7B%5Cmathbb+E%7D+A+-+%5Cfrac%7Bd%7D%7Bn%7DJ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathop{\mathbb E} A - \frac{d}{n}J}"/>, which when simplified will be 1 for entries representing vertices on the same side and -1 for entries representing vertices on different sides. We will redo this argument with SDP norm in place of spectral norm and every step appropriately adjusted.</p>
<p>
Recall that the SDP norm of a matrix <img alt="{M}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{M}"/> is defined to be </p>
<p align="center"><img alt="\displaystyle  \|M\|_{SDP} := \max_{\substack{|{\bf x}_1| = \cdots = |{\bf x}_n| = 1 \\|{\bf y}_1| = \cdots = |{\bf y}_n| = 1}}\sum_{u, v}M_{uv}\langle{\bf x}_u, {\bf y}_v\rangle. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5C%7CM%5C%7C_%7BSDP%7D+%3A%3D+%5Cmax_%7B%5Csubstack%7B%7C%7B%5Cbf+x%7D_1%7C+%3D+%5Ccdots+%3D+%7C%7B%5Cbf+x%7D_n%7C+%3D+1+%5C%5C%7C%7B%5Cbf+y%7D_1%7C+%3D+%5Ccdots+%3D+%7C%7B%5Cbf+y%7D_n%7C+%3D+1%7D%7D%5Csum_%7Bu%2C+v%7DM_%7Buv%7D%5Clangle%7B%5Cbf+x%7D_u%2C+%7B%5Cbf+y%7D_v%5Crangle.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \|M\|_{SDP} := \max_{\substack{|{\bf x}_1| = \cdots = |{\bf x}_n| = 1 \\|{\bf y}_1| = \cdots = |{\bf y}_n| = 1}}\sum_{u, v}M_{uv}\langle{\bf x}_u, {\bf y}_v\rangle. "/></p>
<p> Let <img alt="{R = \left( \begin{array}{c | c} {\bf p} &amp; {\bf q} \\  {\bf q} &amp; {\bf p} \end{array} \right)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR+%3D+%5Cleft%28+%5Cbegin%7Barray%7D%7Bc+%7C+c%7D+%7B%5Cbf+p%7D+%26+%7B%5Cbf+q%7D+%5C%5C++%7B%5Cbf+q%7D+%26+%7B%5Cbf+p%7D+%5Cend%7Barray%7D+%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R = \left( \begin{array}{c | c} {\bf p} &amp; {\bf q} \\  {\bf q} &amp; {\bf p} \end{array} \right)}"/>, then by Grothendieck inequality we have</p>
<p/><p align="center"><img alt="\displaystyle  \|A-R\|_{SDP} \leq c\|A-R\|_{\infty \rightarrow 1}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5C%7CA-R%5C%7C_%7BSDP%7D+%5Cleq+c%5C%7CA-R%5C%7C_%7B%5Cinfty+%5Crightarrow+1%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \|A-R\|_{SDP} \leq c\|A-R\|_{\infty \rightarrow 1}. "/></p>
<p> We proved in the previous lecture that <img alt="{\|A-R\|_{\infty \rightarrow 1} \leq O(n\sqrt{a+b})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7CA-R%5C%7C_%7B%5Cinfty+%5Crightarrow+1%7D+%5Cleq+O%28n%5Csqrt%7Ba%2Bb%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\|A-R\|_{\infty \rightarrow 1} \leq O(n\sqrt{a+b})}"/> with high probability, so we know that the SDP norm <img alt="{\|A-R\|_{SDP} \leq O(n\sqrt{a+b})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7CA-R%5C%7C_%7BSDP%7D+%5Cleq+O%28n%5Csqrt%7Ba%2Bb%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\|A-R\|_{SDP} \leq O(n\sqrt{a+b})}"/> with high probability as well. By definition, this means <a name="eq2"/></p><a name="eq2">
<p align="center"><img alt="\displaystyle  \sum_{u,v}(A_{uv}- R_{uv})\langle {\bf x}_u^\ast, {\bf x}_y^\ast \rangle \leq \|A-R\|_{SDP} \leq O(n\sqrt{a+b}). \ \ \ \ \ (3)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bu%2Cv%7D%28A_%7Buv%7D-+R_%7Buv%7D%29%5Clangle+%7B%5Cbf+x%7D_u%5E%5Cast%2C+%7B%5Cbf+x%7D_y%5E%5Cast+%5Crangle+%5Cleq+%5C%7CA-R%5C%7C_%7BSDP%7D+%5Cleq+O%28n%5Csqrt%7Ba%2Bb%7D%29.+%5C+%5C+%5C+%5C+%5C+%283%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{u,v}(A_{uv}- R_{uv})\langle {\bf x}_u^\ast, {\bf x}_y^\ast \rangle \leq \|A-R\|_{SDP} \leq O(n\sqrt{a+b}). \ \ \ \ \ (3)"/></p>
</a><p><a name="eq2"/> Substracting <a href="https://lucatrevisan.wordpress.com/feed/#eq2">3</a> from <a href="https://lucatrevisan.wordpress.com/feed/#eq1">3</a>, we obtain <a name="eq3"/></p><a name="eq3">
<p align="center"><img alt="\displaystyle  \sum_{u,v}\left(A_{uv}- \frac{a+b}{n} -A_{uv} + R_{uv}\right)\langle {\bf x}_u^\ast, {\bf x}_y^\ast \rangle \geq n(a-b) - O(n\sqrt{a+b}). \ \ \ \ \ (4)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bu%2Cv%7D%5Cleft%28A_%7Buv%7D-+%5Cfrac%7Ba%2Bb%7D%7Bn%7D+-A_%7Buv%7D+%2B+R_%7Buv%7D%5Cright%29%5Clangle+%7B%5Cbf+x%7D_u%5E%5Cast%2C+%7B%5Cbf+x%7D_y%5E%5Cast+%5Crangle+%5Cgeq+n%28a-b%29+-+O%28n%5Csqrt%7Ba%2Bb%7D%29.+%5C+%5C+%5C+%5C+%5C+%284%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{u,v}\left(A_{uv}- \frac{a+b}{n} -A_{uv} + R_{uv}\right)\langle {\bf x}_u^\ast, {\bf x}_y^\ast \rangle \geq n(a-b) - O(n\sqrt{a+b}). \ \ \ \ \ (4)"/></p>
</a><p><a name="eq3"/></p>
<p>
Observe that <a name="eq4"/></p><a name="eq4">
<p align="center"><img alt="\displaystyle  R = \left( \begin{array}{c | c} {\bf p} &amp; {\bf q} \\  {\bf q} &amp; {\bf p} \end{array} \right) = \frac{p+q}{2}J + \frac{p-q}{2}C = \frac{a+b}{n}J + \frac{a-b}{n}C, \ \ \ \ \ (5)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R+%3D+%5Cleft%28+%5Cbegin%7Barray%7D%7Bc+%7C+c%7D+%7B%5Cbf+p%7D+%26+%7B%5Cbf+q%7D+%5C%5C++%7B%5Cbf+q%7D+%26+%7B%5Cbf+p%7D+%5Cend%7Barray%7D+%5Cright%29+%3D+%5Cfrac%7Bp%2Bq%7D%7B2%7DJ+%2B+%5Cfrac%7Bp-q%7D%7B2%7DC+%3D+%5Cfrac%7Ba%2Bb%7D%7Bn%7DJ+%2B+%5Cfrac%7Ba-b%7D%7Bn%7DC%2C+%5C+%5C+%5C+%5C+%5C+%285%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  R = \left( \begin{array}{c | c} {\bf p} &amp; {\bf q} \\  {\bf q} &amp; {\bf p} \end{array} \right) = \frac{p+q}{2}J + \frac{p-q}{2}C = \frac{a+b}{n}J + \frac{a-b}{n}C, \ \ \ \ \ (5)"/></p>
</a><p><a name="eq4"/> where <img alt="{J}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BJ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{J}"/> is the all-one matrix and <img alt="{C = \left(\begin{array}{c | c} {\bf 1} &amp; {\bf -1}\\ {\bf -1} &amp; {\bf 1} \end{array}\right)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC+%3D+%5Cleft%28%5Cbegin%7Barray%7D%7Bc+%7C+c%7D+%7B%5Cbf+1%7D+%26+%7B%5Cbf+-1%7D%5C%5C+%7B%5Cbf+-1%7D+%26+%7B%5Cbf+1%7D+%5Cend%7Barray%7D%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C = \left(\begin{array}{c | c} {\bf 1} &amp; {\bf -1}\\ {\bf -1} &amp; {\bf 1} \end{array}\right)}"/>. Plugging <a href="https://lucatrevisan.wordpress.com/feed/#eq4">5</a> into <a href="https://lucatrevisan.wordpress.com/feed/#eq3">4</a>, we get </p>
<p align="center"><img alt="\displaystyle  \sum_{u,v} \frac{a-b}{n} C_{uv}\langle {\bf x}_u^\ast, {\bf x}_y^\ast \rangle \geq n(a-b) - O(n\sqrt{a+b}), " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bu%2Cv%7D+%5Cfrac%7Ba-b%7D%7Bn%7D+C_%7Buv%7D%5Clangle+%7B%5Cbf+x%7D_u%5E%5Cast%2C+%7B%5Cbf+x%7D_y%5E%5Cast+%5Crangle+%5Cgeq+n%28a-b%29+-+O%28n%5Csqrt%7Ba%2Bb%7D%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{u,v} \frac{a-b}{n} C_{uv}\langle {\bf x}_u^\ast, {\bf x}_y^\ast \rangle \geq n(a-b) - O(n\sqrt{a+b}), "/></p>
<p> which can be simplified to </p>
<p align="center"><img alt="\displaystyle  \sum_{u,v} C_{uv}\langle {\bf x}_u^\ast, {\bf x}_y^\ast \rangle \geq n^2\left(1 - \frac{O(\sqrt{a+b})}{a-b}\right). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bu%2Cv%7D+C_%7Buv%7D%5Clangle+%7B%5Cbf+x%7D_u%5E%5Cast%2C+%7B%5Cbf+x%7D_y%5E%5Cast+%5Crangle+%5Cgeq+n%5E2%5Cleft%281+-+%5Cfrac%7BO%28%5Csqrt%7Ba%2Bb%7D%29%7D%7Ba-b%7D%5Cright%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{u,v} C_{uv}\langle {\bf x}_u^\ast, {\bf x}_y^\ast \rangle \geq n^2\left(1 - \frac{O(\sqrt{a+b})}{a-b}\right). "/></p>
<p> For simplicity, in the following analysis the term <img alt="{\frac{O(\sqrt{a+b})}{a-b}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7BO%28%5Csqrt%7Ba%2Bb%7D%29%7D%7Ba-b%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{O(\sqrt{a+b})}{a-b}}"/> will be called <img alt="{1/c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2Fc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/c}"/>. Notice that <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> is a matrix with 1 for nodes from the same side of the cut and -1 for nodes from different sides of the cut, and <img alt="{\langle {\bf x}_u^\ast, {\bf x}_y^\ast \rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+%7B%5Cbf+x%7D_u%5E%5Cast%2C+%7B%5Cbf+x%7D_y%5E%5Cast+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\langle {\bf x}_u^\ast, {\bf x}_y^\ast \rangle}"/> is an inner product of two unit vectors. If <img alt="{1/c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2Fc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/c}"/> is very close to zero, then the sum will be very close to <img alt="{n^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^2}"/>. This means that <img alt="{C_{uv}\langle {\bf x}_u^\ast, {\bf x}_y^\ast \rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_%7Buv%7D%5Clangle+%7B%5Cbf+x%7D_u%5E%5Cast%2C+%7B%5Cbf+x%7D_y%5E%5Cast+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_{uv}\langle {\bf x}_u^\ast, {\bf x}_y^\ast \rangle}"/> should be 1 for almost every pair of <img alt="{(u, v)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28u%2C+v%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(u, v)}"/>, which shows that <img alt="{X = \langle {\bf x}_u^\ast, {\bf x}_y^\ast \rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX+%3D+%5Clangle+%7B%5Cbf+x%7D_u%5E%5Cast%2C+%7B%5Cbf+x%7D_y%5E%5Cast+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X = \langle {\bf x}_u^\ast, {\bf x}_y^\ast \rangle}"/> is actually very close to <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. Now, we will make this argument robust. To achieve this, we introduce the Frobenius norm of a matrix. </p>
<blockquote><p><b>Definition 1 (Frobenius norm)</b> <em> Let <img alt="{M = (M_{ij})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM+%3D+%28M_%7Bij%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{M = (M_{ij})}"/> be a matrix. The Frobenius norm of <img alt="{M}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{M}"/> is </em></p><em>
<p align="center"><img alt="\displaystyle  \|M\|_F := \sqrt{\sum_{i,j}M_{ij}^2}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5C%7CM%5C%7C_F+%3A%3D+%5Csqrt%7B%5Csum_%7Bi%2Cj%7DM_%7Bij%7D%5E2%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \|M\|_F := \sqrt{\sum_{i,j}M_{ij}^2}. "/></p>
</em><p><em> </em></p></blockquote>
<p> The following fact is a good exercise. </p>
<blockquote><p><b>Fact 2</b> <em> Let <img alt="{M = (M_{ij})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM+%3D+%28M_%7Bij%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{M = (M_{ij})}"/> be a matrix. Then </em></p><em>
<p align="center"><img alt="\displaystyle  \|M\| \leq \|M\|_F, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5C%7CM%5C%7C+%5Cleq+%5C%7CM%5C%7C_F%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \|M\| \leq \|M\|_F, "/></p>
</em><p><em> where <img alt="{\|\cdot\|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7C%5Ccdot%5C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\|\cdot\|}"/> denotes the spectral norm. </em></p></blockquote>
<p> To see how close are <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> and <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/>, we calculate the Frobenius norm of <img alt="{C - X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC+-+X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C - X}"/>, which will be </p>
<p align="center"><img alt="\displaystyle  \begin{aligned} \|C-X\|_F^2 &amp; = \sum_{u,v}C_{uv}^2 + \sum_{u,v}X_{uv}^2 - 2\sum_{u,v}C_{uv}X_{uv} \\ &amp; \leq 2n^2 - 2n^2\left(1 - \frac{1}{c}\right) = \frac{2}{c}n^2. \end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Baligned%7D+%5C%7CC-X%5C%7C_F%5E2+%26+%3D+%5Csum_%7Bu%2Cv%7DC_%7Buv%7D%5E2+%2B+%5Csum_%7Bu%2Cv%7DX_%7Buv%7D%5E2+-+2%5Csum_%7Bu%2Cv%7DC_%7Buv%7DX_%7Buv%7D+%5C%5C+%26+%5Cleq+2n%5E2+-+2n%5E2%5Cleft%281+-+%5Cfrac%7B1%7D%7Bc%7D%5Cright%29+%3D+%5Cfrac%7B2%7D%7Bc%7Dn%5E2.+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{aligned} \|C-X\|_F^2 &amp; = \sum_{u,v}C_{uv}^2 + \sum_{u,v}X_{uv}^2 - 2\sum_{u,v}C_{uv}X_{uv} \\ &amp; \leq 2n^2 - 2n^2\left(1 - \frac{1}{c}\right) = \frac{2}{c}n^2. \end{aligned}"/></p>
<p> This gives us a bound on the spectral norm of <img alt="{C-X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC-X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C-X}"/>, namely </p>
<p align="center"><img alt="\displaystyle  \|C-X\| \leq \|C-X\|_F \leq \sqrt{\frac{2}{c}}n. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5C%7CC-X%5C%7C+%5Cleq+%5C%7CC-X%5C%7C_F+%5Cleq+%5Csqrt%7B%5Cfrac%7B2%7D%7Bc%7D%7Dn.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \|C-X\| \leq \|C-X\|_F \leq \sqrt{\frac{2}{c}}n. "/></p>
<p> Let <img alt="{{\bf z} = (z_1, \ldots, z_n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cbf+z%7D+%3D+%28z_1%2C+%5Cldots%2C+z_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\bf z} = (z_1, \ldots, z_n)}"/> be the unit eigenvector of <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> corresponding to its largest eigenvalue, then by Davis-Kahan theorem we have\footnote{When we apply Davis-Kahan theorem, what we get is actually an upper bound on <img alt="{\min\{\|{\bf z} - \chi/\sqrt{n}\|, \|-{\bf z} - \chi/\sqrt{n}\|\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmin%5C%7B%5C%7C%7B%5Cbf+z%7D+-+%5Cchi%2F%5Csqrt%7Bn%7D%5C%7C%2C+%5C%7C-%7B%5Cbf+z%7D+-+%5Cchi%2F%5Csqrt%7Bn%7D%5C%7C%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\min\{\|{\bf z} - \chi/\sqrt{n}\|, \|-{\bf z} - \chi/\sqrt{n}\|\}}"/>. We have assumed here that the bound holds for <img alt="{\|{\bf z} - \chi/\sqrt{n}\|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7C%7B%5Cbf+z%7D+-+%5Cchi%2F%5Csqrt%7Bn%7D%5C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\|{\bf z} - \chi/\sqrt{n}\|}"/>, but the exact same proof will also work in the other case.} </p>
<p align="center"><img alt="\displaystyle  \left\lVert{\bf z} - \frac{1}{\sqrt{n}}\chi\right\rVert \leq \sqrt{2} \cdot \frac{\sqrt{\frac{2}{c}}n}{n - \sqrt{\frac{2}{c}}n}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%5ClVert%7B%5Cbf+z%7D+-+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7D%5Cchi%5Cright%5CrVert+%5Cleq+%5Csqrt%7B2%7D+%5Ccdot+%5Cfrac%7B%5Csqrt%7B%5Cfrac%7B2%7D%7Bc%7D%7Dn%7D%7Bn+-+%5Csqrt%7B%5Cfrac%7B2%7D%7Bc%7D%7Dn%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \left\lVert{\bf z} - \frac{1}{\sqrt{n}}\chi\right\rVert \leq \sqrt{2} \cdot \frac{\sqrt{\frac{2}{c}}n}{n - \sqrt{\frac{2}{c}}n}. "/></p>
<p> For any <img alt="{\epsilon &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon &gt; 0}"/>, if <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> is a large enough constant then we will have <img alt="{\left\lVert {\bf z} - \frac{1}{\sqrt{n}}\chi\right\rVert \leq \sqrt{\epsilon}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cleft%5ClVert+%7B%5Cbf+z%7D+-+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7D%5Cchi%5Cright%5CrVert+%5Cleq+%5Csqrt%7B%5Cepsilon%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\left\lVert {\bf z} - \frac{1}{\sqrt{n}}\chi\right\rVert \leq \sqrt{\epsilon}}"/>. Now we have the following standard argument: </p>
<p align="center"><img alt="\displaystyle  \begin{aligned} \epsilon n &amp; \geq \|\sqrt{n}{\bf z}- \chi\|^2 \\ &amp; = \sum_i(\sqrt{n} z_i - \chi_i)^2 \\ &amp; \geq \#\{i:\text{sign}(\sqrt{n}z_i) \neq \chi_i\}. \end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Baligned%7D+%5Cepsilon+n+%26+%5Cgeq+%5C%7C%5Csqrt%7Bn%7D%7B%5Cbf+z%7D-+%5Cchi%5C%7C%5E2+%5C%5C+%26+%3D+%5Csum_i%28%5Csqrt%7Bn%7D+z_i+-+%5Cchi_i%29%5E2+%5C%5C+%26+%5Cgeq+%5C%23%5C%7Bi%3A%5Ctext%7Bsign%7D%28%5Csqrt%7Bn%7Dz_i%29+%5Cneq+%5Cchi_i%5C%7D.+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{aligned} \epsilon n &amp; \geq \|\sqrt{n}{\bf z}- \chi\|^2 \\ &amp; = \sum_i(\sqrt{n} z_i - \chi_i)^2 \\ &amp; \geq \#\{i:\text{sign}(\sqrt{n}z_i) \neq \chi_i\}. \end{aligned}"/></p>
<p> The last inequality is because every <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> with <img alt="{\text{sign}(\sqrt{n}z_i) \neq \chi_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7Bsign%7D%28%5Csqrt%7Bn%7Dz_i%29+%5Cneq+%5Cchi_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{sign}(\sqrt{n}z_i) \neq \chi_i}"/> will contribute at least 1 in the sum <img alt="{\sum_i(\sqrt{n} z_i - \chi_i)^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_i%28%5Csqrt%7Bn%7D+z_i+-+%5Cchi_i%29%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sum_i(\sqrt{n} z_i - \chi_i)^2}"/>. This shows that our algorithm will misclassify at most <img alt="{\epsilon n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon n}"/> vertices.</p>
<p/></div></content><updated planet:format="November 01, 2017 01:34 AM">2017-11-01T01:34:17Z</updated><published planet:format="November 01, 2017 01:34 AM">2017-11-01T01:34:17Z</published><category term="BWCA"/><category term="Grothendieck Inequality"/><category term="stochastic block model"/><author><name>luca</name></author><source><id>https://lucatrevisan.wordpress.com</id><logo>https://s0.wp.com/i/buttonw-com.png</logo><link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/><link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/><link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/><link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/><subtitle>&quot;Marge, I agree with you - in theory. In theory, communism works. In theory.&quot; -- Homer Simpson</subtitle><title>in   theory</title><updated planet:format="December 17, 2018 05:28 AM">2018-12-17T05:28:59Z</updated><planet:module>toc</planet:module><planet:format>atom10</planet:format><planet:http_last_modified>Thu, 06 Dec 2018 16:46:05 GMT</planet:http_last_modified><planet:bozo>false</planet:bozo><planet:items_per_page>40</planet:items_per_page><planet:css-id>luca-trevisan</planet:css-id><planet:face>trevisan.jpeg</planet:face><planet:name>Luca Trevisan</planet:name><planet:http_location>https://lucatrevisan.wordpress.com/feed/</planet:http_location><planet:http_status>301</planet:http_status></source></entry>
