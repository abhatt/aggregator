<?xml version="1.0" ?><entry xml:lang="en" xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/"><id>http://mittheory.wordpress.com/?p=393</id><link href="https://mittheory.wordpress.com/2014/07/03/an-encore-more-learning-and-testing-stoc-2014-recaps-part-8/" rel="alternate" type="text/html"/><title>An Encore: More Learning and Testing – STOC 2014 Recaps (Part 8)</title><summary>Thought you were rid of us? Not quite: in a last hurrah, Clément and I come back with a final pair of distribution estimation recaps — this time on results from the actual conference! Gautam Kamath on Efficient Density Estimation via Piecewise Polynomial Approximation by Siu-On Chan, Ilias Diakonikolas, Rocco A. Servedio, and Xiaorui Sun Density estimation is the question on […]</summary><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Thought you were rid of us? Not quite: in a last hurrah, Clément and I come back with a final pair of distribution estimation recaps — this time on results from the actual conference!</p>
<hr/>
<p><span style="text-decoration: underline;">Gautam Kamath on<b> <a href="http://arxiv.org/abs/1305.3207">Efficient Density Estimation via Piecewise Polynomial Approximation</a></b> by <a href="http://research.microsoft.com/en-us/people/siuonc/">Siu-On Chan</a>, <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>, <a href="http://www.cs.columbia.edu/~rocco/">Rocco A. Servedio</a>, and <a href="http://www.cs.columbia.edu/~xiaoruisun/">Xiaorui Sun</a></span></p>
<p>Density estimation is the question on everyone’s mind. It’s as simple as it gets – we receive samples from a distribution and want to figure out what the distribution looks like. The problem rears its head in almost every setting you can imagine — fields as diverse as medicine, advertising, and compiler design, to name a few. Given its ubiquity, it’s embarrassing to admit that we didn’t have a provably good algorithm for this problem until just now.</p>
<p>Let’s get more precise. We’ll deal with the <a href="http://en.wikipedia.org/wiki/Total_variation_distance_of_probability_measures">total variation distance metric</a> (AKA statistical distance). Given distributions with PDFs <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f"/> and <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="g"/>, their total variation distance is <img alt="d_{\mathrm{TV}}(f,g) = \frac12\|f - g\|_1" class="latex" src="https://s0.wp.com/latex.php?latex=d_%7B%5Cmathrm%7BTV%7D%7D%28f%2Cg%29+%3D+%5Cfrac12%5C%7Cf+-+g%5C%7C_1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="d_{\mathrm{TV}}(f,g) = \frac12\|f - g\|_1"/>. Less formally but more intuitively, it upper bounds the difference in probabilities for any given event. With this metric in place, we can define what it means to learn a distribution: given sample access to a distribution <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="X"/>, we would like to output a distribution <img alt="\hat X" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat+X&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\hat X"/> such that <img alt="d_{\mathrm{TV}}(f_X,f_{\hat X}) \leq \varepsilon" class="latex" src="https://s0.wp.com/latex.php?latex=d_%7B%5Cmathrm%7BTV%7D%7D%28f_X%2Cf_%7B%5Chat+X%7D%29+%5Cleq+%5Cvarepsilon&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="d_{\mathrm{TV}}(f_X,f_{\hat X}) \leq \varepsilon"/>.</p>
<p>This paper presents an algorithm for learning <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="t"/>-piecewise degree-<img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="d"/> polynomials. Wow, that’s a mouthful — what does it mean? A <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="t"/>-piecewise degree-<img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="d"/> polynomial is a function where the domain can be partitioned into <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="t"/> intervals, such that the function is a degree-<img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="d"/> polynomial on each of these intervals. The main result says that a distribution with a PDF described by a <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="t"/>-piecewise degree-<img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="d"/> polynomial can be learned to accuracy <img alt="\varepsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\varepsilon"/> using <img alt="O((d+1)kt/\varepsilon^2)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28%28d%2B1%29kt%2F%5Cvarepsilon%5E2%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="O((d+1)kt/\varepsilon^2)"/> samples and polynomial time. Moreover, the sample complexity is optimal up to logarithmic factors.</p>
<div class="wp-caption aligncenter" id="attachment_426" style="width: 310px;"><a href="https://mittheory.files.wordpress.com/2014/06/screenshot-from-2014-06-24-014935.png"><img alt="Piecewise Polynomial" class="wp-image-426 size-medium" height="142" src="https://mittheory.files.wordpress.com/2014/06/screenshot-from-2014-06-24-014935.png?w=300&amp;h=142" width="300"/></a><p class="wp-caption-text" id="caption-attachment-426"/>
<p style="text-align: center;">A 4-piecewise degree-3 polynomial. <br/>Lifted from <a href="http://www.iliasdiakonikolas.org/stoc14-workshop/diakonikolas.pdf">Ilias’ slides</a>.</p>
<p/></div>
<p>Now this is great and all, but what good are piecewise polynomials? How many realistic distributions are described by something like “<img alt="-x^2 + 1" class="latex" src="https://s0.wp.com/latex.php?latex=-x%5E2+%2B+1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="-x^2 + 1"/> for <img alt="x \in [0,1)" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5B0%2C1%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="x \in [0,1)"/> but <img alt="\frac12x - 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac12x+-+1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\frac12x - 1"/> for <img alt="x \in [1,2)" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5B1%2C2%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="x \in [1,2)"/> and <img alt="2x^4 - x^2 +" class="latex" src="https://s0.wp.com/latex.php?latex=2x%5E4+-+x%5E2+%2B&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="2x^4 - x^2 +"/>…”? The answer turns out to be a <i>ton</i> of distributions — as long as you squint at them hard enough.</p>
<p>The wonderful thing about this result is that it’s <i>semi-agnostic</i>. Many algorithms in the literature are God-fearing subroutines, and will sacrifice their first-born child to make sure they receive samples from the class of distributions they’re promised — otherwise, you can’t make any guarantees about the quality of their output. But our friend here is a bit more skeptical. He deals with a funny class of distributions, and knows true piecewise polynomial distributions are few and far between — if you get one on the streets, who knows if it’s pure? Our friend is resourceful: no matter the quality, he makes it work.</p>
<p>Let’s elaborate, in slightly less blasphemous terms. Suppose you’re given sample access to a distribution <img alt="\mathcal{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathcal{D}"/> which is at total variation distance <img alt="\leq \tau" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleq+%5Ctau&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\leq \tau"/> from <i>some</i> <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="t"/>-piecewise degree-<img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="d"/> polynomial (you don’t need to know which one). Then the algorithm will output a <img alt="(2t-1)" class="latex" src="https://s0.wp.com/latex.php?latex=%282t-1%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="(2t-1)"/>-piecewise degree-<img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="d"/> polynomial which is at distance <img alt="\leq 4\tau + O(\varepsilon)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleq+4%5Ctau+%2B+O%28%5Cvarepsilon%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\leq 4\tau + O(\varepsilon)"/> from <img alt="\mathcal{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathcal{D}"/>. In English: even if the algorithm isn’t given a piecewise polynomial, it’ll still produce something that’s (almost) as good as you could hope for.</p>
<p>With this insight under our cap, let’s ask again — where do we see piecewise polynomials? They’re everywhere: this algorithm can handle distributions which are log-concave, bounded monotone, Gaussian, <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="t"/>-modal, monotone hazard rate, and Poisson Binomial. And the kicker is that it can handle <i>mixtures</i> of these distributions too. Usually, algorithms fail catastrophically when considering mixtures, but this algorithm keeps chugging and handles them all — and near optimally, most of the time.</p>
<p>The analysis is tricky, but I’ll try to give a taste of some of the techniques. One of the key tools is the <a href="http://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory">Vapnik-Chervonenkis (VC) inequality</a>. Without getting into the details, the punchline is that if we output a piecewise polynomial which is “close” to the empirical distribution (under a weaker metric than total variation distance), it’ll give us our desired learning result. In this setting, “close” means (roughly) that the CDFs don’t stray too far from each (though in a sense that is stronger than the Kolmogorov distance metric).</p>
<p>Let’s start with an easy case – what if the distribution is a <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="1"/>-piecewise polynomial? By the VC inequality, we just have to match the empirical CDF. We can do this by setting up a linear program which outputs a linear combination of the <a href="http://en.wikipedia.org/wiki/Chebyshev_polynomials">Chebyshev polynomials</a>, constrained to resemble the empirical distribution.</p>
<p>It turns out that this subroutine is the hardest part of the algorithm. In order to deal with multiple pieces, we first discretize the support into small intervals which are roughly equal in probability mass. Next, in order to discover a good partition of these intervals, we run a dynamic program. This program uses the subroutine from the previous paragraph to compute the best polynomial approximation over each contiguous set of the intervals. Then, it stitches the solutions together in the minimum cost way, with the constraint that it uses fewer than <img alt="2t - 1" class="latex" src="https://s0.wp.com/latex.php?latex=2t+-+1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="2t - 1"/> pieces.</p>
<p>In short, this result essentially closes the problem of density estimation for an enormous class of distributions — they turn existential approximations (by piecewise polynomials) into approximation algorithms. But there’s still a lot more work to do — while this result gives us <i>improper</i> learning, we yearn for <i>proper</i> learning algorithms. For example, this algorithm lets us approximate a mixture of Gaussians using a piecewise polynomial, but can we output a mixture of Gaussians as our hypothesis instead? Looking at the sample complexity, the answer is yes, but we don’t know of any computationally efficient way to solve this problem yet. Regardless, there’s many exciting directions to go — I’m looking forward to where the authors will take this line of work!</p>
<p>-G</p>
<hr/>
<p><span style="text-decoration: underline;">Clément Canonne on <b><a href="http://grigory.github.io/files/publications/BRY14-Lp-Testing.pdf"><img alt="L_p" class="latex" src="https://s0.wp.com/latex.php?latex=L_p&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L_p"/>-Testing</a></b>, by <a href="http://www.cse.psu.edu/~berman/">Piotr Berman</a>, <a href="http://www.cse.psu.edu/~sofya/">Sofya Raskhodnikova</a>, and <a href="http://www.grigory.us/">Grigory Yaroslavtsev</a> [1])</span></p>
<p>Almost every — if not all — work in property testing of functions are concerned with the <a href="http://en.wikipedia.org/wiki/Hamming_distance"><i>Hamming distance</i></a> between functions, that is the fraction of inputs on which they disagree. Very natural when we deal for instance with Boolean functions <img alt="f\colon\{0,1\}^d\to\{0,1\}" class="latex" src="https://s0.wp.com/latex.php?latex=f%5Ccolon%5C%7B0%2C1%5C%7D%5Ed%5Cto%5C%7B0%2C1%5C%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f\colon\{0,1\}^d\to\{0,1\}"/>, this distance becomes highly arguable when the codomain is, say, the real line: sure, <img alt="f\colon x\mapsto x^2-\frac{\sin^2 x}{2^{32}}" class="latex" src="https://s0.wp.com/latex.php?latex=f%5Ccolon+x%5Cmapsto+x%5E2-%5Cfrac%7B%5Csin%5E2+x%7D%7B2%5E%7B32%7D%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f\colon x\mapsto x^2-\frac{\sin^2 x}{2^{32}}"/> and <img alt="g\colon x\mapsto x^2" class="latex" src="https://s0.wp.com/latex.php?latex=g%5Ccolon+x%5Cmapsto+x%5E2&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="g\colon x\mapsto x^2"/> technically disagree on almost every single input, but should they be considered two completely different functions?</p>
<p>This question, Grigory answered by the negative; and presented (joint work with Piotr Berman and Sofya Raskhodnikova [2]) a new framework for testing real-valued functions <img alt="f\colon X^d\to[0,1]" class="latex" src="https://s0.wp.com/latex.php?latex=f%5Ccolon+X%5Ed%5Cto%5B0%2C1%5D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f\colon X^d\to[0,1]"/>, less sensitive to this sort of annoying “technicalities” (i.e., noise). Instead of the usual Hamming/<img alt="L_0" class="latex" src="https://s0.wp.com/latex.php?latex=L_0&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L_0"/> distance between function, they suggest the more robust <img alt="L_p" class="latex" src="https://s0.wp.com/latex.php?latex=L_p&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L_p"/> (<img alt="p\geq 1" class="latex" src="https://s0.wp.com/latex.php?latex=p%5Cgeq+1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p\geq 1"/>) distance</p>
<p style="text-align: center;"><img alt="\mathrm{d}_p(f,g) = \frac{ \left(\int_{X^d} \lvert f(x)-g(x) \rvert^p dx\right)^{\frac{1}{p}} }{ \left(\int_{X^d} \mathbf{1} dx \right)^{\frac{1}{p}} } = \mathbb{E}_{x\sim\mathcal{U}(X^d)}\left[\lvert f(x)-g(x) \rvert^p\right]^{\frac{1}{p}}\in [0,1]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bd%7D_p%28f%2Cg%29+%3D+%5Cfrac%7B+%5Cleft%28%5Cint_%7BX%5Ed%7D+%5Clvert+f%28x%29-g%28x%29+%5Crvert%5Ep+dx%5Cright%29%5E%7B%5Cfrac%7B1%7D%7Bp%7D%7D+%7D%7B+%5Cleft%28%5Cint_%7BX%5Ed%7D+%5Cmathbf%7B1%7D+dx+%5Cright%29%5E%7B%5Cfrac%7B1%7D%7Bp%7D%7D+%7D+%3D+%5Cmathbb%7BE%7D_%7Bx%5Csim%5Cmathcal%7BU%7D%28X%5Ed%29%7D%5Cleft%5B%5Clvert+f%28x%29-g%28x%29+%5Crvert%5Ep%5Cright%5D%5E%7B%5Cfrac%7B1%7D%7Bp%7D%7D%5Cin+%5B0%2C1%5D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathrm{d}_p(f,g) = \frac{ \left(\int_{X^d} \lvert f(x)-g(x) \rvert^p dx\right)^{\frac{1}{p}} }{ \left(\int_{X^d} \mathbf{1} dx \right)^{\frac{1}{p}} } = \mathbb{E}_{x\sim\mathcal{U}(X^d)}\left[\lvert f(x)-g(x) \rvert^p\right]^{\frac{1}{p}}\in [0,1]"/></p>
<p>(think of <img alt="X^d" class="latex" src="https://s0.wp.com/latex.php?latex=X%5Ed&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="X^d"/> as being the hypercube <img alt="\{0,1\}^d" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B0%2C1%5C%7D%5Ed&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\{0,1\}^d"/> or the hypergrid <img alt="[n]^d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Bn%5D%5Ed&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="[n]^d"/>, and <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p"/> being 1 or 2. In this case, the denominator is just a normalizing factor <img alt="\lvert X\rvert" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clvert+X%5Crvert&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\lvert X\rvert"/> or <img alt="\sqrt{\lvert X\rvert}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B%5Clvert+X%5Crvert%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\sqrt{\lvert X\rvert}"/>)</p>
<p>Now, erm… why?</p>
<ul>
<li>because it is much more robust to noise in the data;</li>
<li>because it is much more robust to outliers;</li>
<li>because it plays well (as a preprocessing step for model selection) with existing variants of PAC-learning under <img alt="L_p" class="latex" src="https://s0.wp.com/latex.php?latex=L_p&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L_p"/> norms;</li>
<li>because <img alt="L_1" class="latex" src="https://s0.wp.com/latex.php?latex=L_1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L_1"/> and <img alt="L_2" class="latex" src="https://s0.wp.com/latex.php?latex=L_2&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L_2"/> are pervasive in (machine) learning;</li>
<li>because they can.</li>
</ul>
<p>Their results and methods turn out to be very elegant: to outline only a few, they</p>
<ul>
<li>give the first example of testing monotonicity testing (de facto, for the <img alt="L_1" class="latex" src="https://s0.wp.com/latex.php?latex=L_1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L_1"/> distance) when <i>adaptivity provably helps</i>; that is, a testing algorithm that selects its future queries as a function of the answers it previously got <strong>can</strong> outperform any tester that commits in advance to all its queries. This settles a longstanding question for testing monotonicity with respect to Hamming distance;</li>
<li>improve several general results for property testing, <strong>also applicable to Hamming testing</strong> (e.g. Levin’s investment strategy [3]);</li>
<li>provide general relations between sample complexity of testing (and tolerant testing) for various norms (<img alt="L_0, L_1,L_2,L_p" class="latex" src="https://s0.wp.com/latex.php?latex=L_0%2C+L_1%2CL_2%2CL_p&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L_0, L_1,L_2,L_p"/>);</li>
<li>have quite nice and beautiful algorithms (e.g., testing via partial learning) for testing monotonicity and Lipschitz property;</li>
<li>give close-to-tight bounds for the problems they consider;</li>
<li>have slides in which the phrase “Big Data” and a mention to stock markets appear (!);</li>
<li>have an incredibly neat reduction between <img alt="L_1" class="latex" src="https://s0.wp.com/latex.php?latex=L_1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L_1"/> and Hamming testing of monotonicity.</li>
</ul>
<p>I will hereafter only focus on the last of these bullets, one which really tickled my fancy (gosh, my fancy is <i>so</i> ticklish) — for the other ones, I urge you to read the paper. It is a cool paper. Really.</p>
<p>Here is the last bullet, in a slightly more formal fashion — recall that a function <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f"/> defined on a partially ordered set is monotone if for all comparable inputs <img alt="x,y" class="latex" src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="x,y"/> such that <img alt="x\preceq y" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Cpreceq+y&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="x\preceq y"/>, one has <img alt="f(x)\leq f(y)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29%5Cleq+f%28y%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f(x)\leq f(y)"/>; and that a one-sided tester is an algorithm which will never reject a “good” function: it can only err on “bad” functions (that is, it may sometimes accept, with small probability, a function far from monotone, but will never reject a monotone function).</p>
<p><b>Theorem.</b><br/>
Suppose one has a one-sided, non-adaptive tester <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="T"/> for monotonicity of Boolean functions <img alt="f\colon X\to \{0,1\}" class="latex" src="https://s0.wp.com/latex.php?latex=f%5Ccolon+X%5Cto+%5C%7B0%2C1%5C%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f\colon X\to \{0,1\}"/> with respect to Hamming distance, with query complexity <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="q"/>. Then the very same <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="T"/> is <i>also</i> a tester for monotonicity of <i>real-valued</i> functions <img alt="f\colon X\to [0,1]" class="latex" src="https://s0.wp.com/latex.php?latex=f%5Ccolon+X%5Cto+%5B0%2C1%5D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f\colon X\to [0,1]"/> with respect to <img alt="L_1" class="latex" src="https://s0.wp.com/latex.php?latex=L_1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L_1"/> distance.</p>
<p>Almost too good to be true: we can recycle testers! How? The idea is to express our real-valued <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f"/> as some “mixture” of Boolean functions, and use <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="T"/> as if we were accessing these. More precisely, let <img alt="f\colon [n]^d \to [0,1]" class="latex" src="https://s0.wp.com/latex.php?latex=f%5Ccolon+%5Bn%5D%5Ed+%5Cto+%5B0%2C1%5D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f\colon [n]^d \to [0,1]"/> be a function which one intends to test for monotonicity. For all thresholds <img alt="t\in[0,1]" class="latex" src="https://s0.wp.com/latex.php?latex=t%5Cin%5B0%2C1%5D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="t\in[0,1]"/>, the authors define the Boolean function <img alt="f_t" class="latex" src="https://s0.wp.com/latex.php?latex=f_t&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f_t"/> by</p>
<p style="text-align: center;"><img alt="f_t(x) = \begin{cases}  1 &amp; \text{ if } f(x) \geq t\\  0 &amp; \text{ if } f(x) &lt; t  \end{cases}  " class="latex" src="https://s0.wp.com/latex.php?latex=f_t%28x%29+%3D+%5Cbegin%7Bcases%7D++1+%26+%5Ctext%7B+if+%7D+f%28x%29+%5Cgeq+t%5C%5C++0+%26+%5Ctext%7B+if+%7D+f%28x%29+%3C+t++%5Cend%7Bcases%7D++&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f_t(x) = \begin{cases}  1 &amp; \text{ if } f(x) \geq t\\  0 &amp; \text{ if } f(x) &lt; t  \end{cases}  "/></p>
<p>All these <img alt="f_t" class="latex" src="https://s0.wp.com/latex.php?latex=f_t&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f_t"/> are Boolean; and one can verify that for all <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="x"/>, <img alt="f(x)=\int_0^1 f_t(x) dt" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29%3D%5Cint_0%5E1+f_t%28x%29+dt&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f(x)=\int_0^1 f_t(x) dt"/>. Here comes the twist: one can also show that the distance of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f"/> to monotone satisfies</p>
<p style="text-align: center;"><img alt="\mathrm{d}_1(f,\mathcal{M}) = \int_0^1 \mathrm{d}_0(f_t,\mathcal{M}) dt" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bd%7D_1%28f%2C%5Cmathcal%7BM%7D%29+%3D+%5Cint_0%5E1+%5Cmathrm%7Bd%7D_0%28f_t%2C%5Cmathcal%7BM%7D%29+dt&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathrm{d}_1(f,\mathcal{M}) = \int_0^1 \mathrm{d}_0(f_t,\mathcal{M}) dt"/></p>
<p>i.e. the <img alt="L_1" class="latex" src="https://s0.wp.com/latex.php?latex=L_1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L_1"/> distance of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f"/> to monotone is the <i>integral</i> of the Hamming distances of the <img alt="f_t" class="latex" src="https://s0.wp.com/latex.php?latex=f_t&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f_t"/>‘s to monotone. And by a very simple averaging argument, if <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f"/> is far from monotone, then at least one of the <img alt="f_t" class="latex" src="https://s0.wp.com/latex.php?latex=f_t&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f_t"/>‘s must be…<br/>
How does that help? Well, take your favorite Boolean, Hamming one-sided (non-adaptive) tester for monotonicity, <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="T"/>: being one-sided, it can only reject a function if it has some “evidence” it is not monotone — indeed, if it sees some violation: i.e., a pair <img alt="x,y" class="latex" src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="x,y"/> with <img alt="x\prec y" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Cprec+y&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="x\prec y"/> but <img alt="f(x) &gt; f(y)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29+%3E+f%28y%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f(x) &gt; f(y)"/>.</p>
<p>Feed this tester, instead of the Boolean function it expected, our real-valued <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f"/>; as one of the <img alt="f_{t^\ast}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bt%5E%5Cast%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f_{t^\ast}"/>‘s is far from monotone, our tester would reject <img alt="f_{t^\ast}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bt%5E%5Cast%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f_{t^\ast}"/>; so it would find a violation of monotonicity by <img alt="f_{t^\ast}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bt%5E%5Cast%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f_{t^\ast}"/> <i>if it were given access to <img alt="f_{t^\ast}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bt%5E%5Cast%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f_{t^\ast}"/></i>. But being non-adaptive, <i>the tester does exactly the same queries on <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f"/> as it would have done on this <img alt="f_{t^\ast}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bt%5E%5Cast%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f_{t^\ast}"/></i>! And it is not difficult to see that a violation for <img alt="f_{t^\ast}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bt%5E%5Cast%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f_{t^\ast}"/> is still a violation for <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f"/>: so the tester finds a proof that <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f"/> is not monotone, and rejects. <img alt="\square" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csquare&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\square"/></p>
<p>Wow.</p>
<p>— Clément.</p>
<p>Final, small remark: one may notice a similarity between <img alt="L_1" class="latex" src="https://s0.wp.com/latex.php?latex=L_1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L_1"/> testing of functions <img alt="f\colon[n]\to[0,1]" class="latex" src="https://s0.wp.com/latex.php?latex=f%5Ccolon%5Bn%5D%5Cto%5B0%2C1%5D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f\colon[n]\to[0,1]"/> and the “usual” testing (with relation to total variation distance, <img alt="\propto L_1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpropto+L_1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\propto L_1"/>) of distributions <img alt="D\colon [n]\to[0,1]" class="latex" src="https://s0.wp.com/latex.php?latex=D%5Ccolon+%5Bn%5D%5Cto%5B0%2C1%5D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="D\colon [n]\to[0,1]"/>. There is actually a quite important difference, as in the latter the distance is <i>not</i> normalized by <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="n"/> (because distributions have to sum to <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="1"/> anyway). In this sense, there is no direct relation between the two, and the work presented here is indeed novel in every respect.</p>
<p><b>Edit:</b> thanks to Sofya Raskhodnikova for spotting an imprecision in the original review.</p>
<p>[1] Slides available here: <a href="http://grigory.github.io/files/talks/BRY-STOC14.pptm" rel="nofollow">http://grigory.github.io/files/talks/BRY-STOC14.pptm</a><br/>
[2] <a href="http://dl.acm.org/citation.cfm?id=2591887" rel="nofollow">http://dl.acm.org/citation.cfm?id=2591887</a><br/>
[3] See e.g. Appendix A.2 in “On Multiple Input Problems in Property Testing”, Oded Goldreich. 2013. <a href="http://eccc-preview.hpi-web.de/report/2013/067/" rel="nofollow">http://eccc-preview.hpi-web.de/report/2013/067/</a></p></div></content><updated planet:format="July 03, 2014 02:43 PM">2014-07-03T14:43:01Z</updated><published planet:format="July 03, 2014 02:43 PM">2014-07-03T14:43:01Z</published><category term="conferences"/><category term="learning theory"/><category term="STOC"/><author><name>Gautam</name></author><source><id>https://mittheory.wordpress.com</id><logo>https://s0.wp.com/i/buttonw-com.png</logo><link href="https://mittheory.wordpress.com/feed/" rel="self" type="application/atom+xml"/><link href="https://mittheory.wordpress.com" rel="alternate" type="text/html"/><link href="https://mittheory.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/><link href="https://mittheory.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/><subtitle>A student blog of MIT CSAIL Theory of Computation Group</subtitle><title>Not so Great Ideas in Theoretical Computer Science</title><updated planet:format="February 28, 2020 03:25 AM">2020-02-28T03:25:10Z</updated><planet:module>toc</planet:module><planet:format>atom10</planet:format><planet:http_last_modified>Fri, 06 Sep 2019 10:00:22 GMT</planet:http_last_modified><planet:bozo>false</planet:bozo><planet:items_per_page>40</planet:items_per_page><planet:css-id>mit-csail-student-blog</planet:css-id><planet:face>csail.png</planet:face><planet:name>MIT CSAIL student blog</planet:name><planet:http_location>https://mittheory.wordpress.com/feed/</planet:http_location><planet:http_status>301</planet:http_status></source></entry>