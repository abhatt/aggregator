<?xml version="1.0" ?><entry xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/"><id>tag:blogger.com,1999:blog-25562705.post-2172214634745900334</id><link href="http://aaronsadventures.blogspot.com/feeds/2172214634745900334/comments/default" rel="replies" type="application/atom+xml"/><link href="http://www.blogger.com/comment.g?blogID=25562705&amp;postID=2172214634745900334" rel="replies" type="text/html"/><link href="http://www.blogger.com/feeds/25562705/posts/default/2172214634745900334" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/25562705/posts/default/2172214634745900334" rel="self" type="application/atom+xml"/><link href="http://aaronsadventures.blogspot.com/2016/05/fairness-in-learning.html" rel="alternate" type="text/html"/><title>Fairness in Learning</title><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The very real problem of (un)fairness in algorithmic decision making in general, and machine learning in particular seems to have finally reached the forefront of public attention. Every day there is a new popular article about the topic. Just in the last few weeks, we have seen articles in the Times about <a href="http://www.nytimes.com/2016/05/12/technology/facebooks-bias-is-built-in-and-bears-watching.html">built in bias in facebook</a>, and an in-depth <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">ProPublica study about racial bias in statistical models for predicting criminal recidivism.</a> Earlier this month, the <a href="https://www.whitehouse.gov/sites/default/files/microsites/ostp/2016_0504_data_discrimination.pdf">White House released a report on the challenges in promoting fairness in Big Data</a>.<br/><br/>The tricky thing is saying something concrete and technical about this problem -- even defining what &quot;fairness&quot; <i>is</i> is delicate. There has been some good technical work in this area that I have long admired from afar -- see e.g. the<a href="http://www.fatml.org/"> &quot;FATML&quot; (Fairness and Transparency in Machine Learning) Workshop</a> to get an idea of the range of work being done, and the folks doing it. People like <a href="http://research.microsoft.com/en-us/people/dwork/">Cynthia Dwork</a>, <a href="http://www.moritzhardt.com/">Moritz Hardt</a>, <a href="http://solon.barocas.org/">Solon Barocas</a>, <a href="http://www.cs.utah.edu/~suresh/web/">Suresh Venkatasubramanian</a>, <a href="https://www.haverford.edu/users/sfriedle">Sorelle Friedler</a>, <a href="https://mathbabe.org/">Cathy O'Neil</a>, and others have been doing important work thinking about these problems for quite some time. A particularly nice early paper that I recommend everyone interested in the area read is <a href="http://arxiv.org/abs/1104.3913">Fairness Through Awareness</a>, by Dwork, Hardt, Pitassi, Reingold, and Zemel. It was first posted online in 2011(!), and in retrospect is quite prescient in its discussion of algorithmic fairness.<br/><br/>So I'm happy to finally have something interesting to say about the topic! My student <a href="http://www.cis.upenn.edu/~majos">Matthew Joseph</a>, <a href="http://www.cis.upenn.edu/~mkearns/">Michael Kearns</a>, <a href="http://www.cs.cmu.edu/~jamiemmt/">Jamie Morgenstern</a>, and I just posted a new paper online that I'm excited about: <a href="http://arxiv.org/abs/1605.07139">Fairness in Learning: Classic and Contextual Bandits</a>. I'll mostly let the paper speak for itself, but briefly, we write down a simple but (I think) compelling definition of fairness in a stylized general model of sequential decision making called the &quot;contextual bandit setting&quot;. To keep a canonical problem in your mind, imagine the following: There are a bunch of different populations (say racial or socioeconomic groups), and you are a loan officer. Every day, an individual from each population applies for a loan. You get to see the loan application for each person (this is the &quot;context&quot;), and have to decide who to give the loan to. When you give out the loan, you observe some reward (e.g. you see if they paid back the loan), but you don't see what reward you -would- have gotten had you given the loan to someone else. Our fairness condition says roughly that an algorithm is &quot;fair&quot; if it never preferentially gives a loan to a less qualified applicant over a more qualified applicant -- where the quality of an applicant in our setting is precisely the probability that they pay back the loan. (It prohibits discriminating against qualified applicants on an individual basis -- even if  they happen to come from a population that is less credit-worthy on average, or from a population that the bank doesn't understand as well).<br/><br/>It might seem like this definition of fairness is entirely consistent with the profit motivation of a bank -- why would a bank ever want to give a loan to an applicant less likely to pay it back? Indeed, this would be true if the bank had nothing to learn -- i.e. if it already knew the optimal rule mapping loan applications to credit-worthiness. Said another way, <i>implementing</i> the optimal policy is entirely consistent with our fairness definition. Our main conceptual message is that fairness can nevertheless be an obstruction to <i>learning</i> the optimal policy.<br/><br/>What our results say is that &quot;fairness&quot; always has a cost in terms of the optimal learning rate achievable by algorithms in this setting. For some kinds of problems, the cost is mild in that the cost of fairness on the learning rate is only polynomial (e.g. when credit-worthiness is determined by a simple linear regression model on the features of a loan application). On the other hand, for other kinds of problems, the cost of fairness on the learning rate is severe, in that it can slow learning by an exponential factor (e.g. when credit-worthiness is determined by an AND of features in the loan application). Put another way, for the problems in which the cost of fairness is severe, if the bank were to use a fast learning algorithm (absent a fairness constraint), the algorithm might be &quot;unfair&quot; for a very long time, even if in the limit, once it learned the truly optimal policy, it would eventually be fair. One friction to fairness is that we don't live in the limit -- we are always in a state of learning.</div></content><updated planet:format="May 24, 2016 01:18 PM">2016-05-24T13:18:00Z</updated><published planet:format="May 24, 2016 01:18 PM">2016-05-24T13:18:00Z</published><author><name>Aaron</name><email>noreply@blogger.com</email><uri>http://www.blogger.com/profile/09952936358739421126</uri></author><source><id>tag:blogger.com,1999:blog-25562705</id><category term="game theory"/><category term="news"/><author><name>Aaron</name><email>noreply@blogger.com</email><uri>http://www.blogger.com/profile/09952936358739421126</uri></author><link href="http://aaronsadventures.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/25562705/posts/default" rel="self" type="application/atom+xml"/><link href="http://aaronsadventures.blogspot.com/" rel="alternate" type="text/html"/><link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/><link href="http://www.blogger.com/feeds/25562705/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/><title>Adventures in Computation</title><updated planet:format="February 28, 2020 02:50 AM">2020-02-28T02:50:20Z</updated><planet:module>toc</planet:module><planet:format>atom10</planet:format><planet:http_etag>W/&quot;54cfd69ca9a6a1248bbf09d7869e70e6fa99df3b840c0db1bfca393f04fae9e8&quot;</planet:http_etag><planet:http_last_modified>Fri, 28 Feb 2020 02:50:20 GMT</planet:http_last_modified><planet:bozo>false</planet:bozo><planet:items_per_page>40</planet:items_per_page><planet:css-id>aaron-roth</planet:css-id><planet:face>roth.jpeg</planet:face><planet:name>Aaron Roth</planet:name><planet:http_status>200</planet:http_status></source></entry>