<?xml version="1.0" encoding="utf-8"?><entry xml:lang="en" xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/"><id>http://mittheory.wordpress.com/?p=705</id><link href="https://mittheory.wordpress.com/2015/08/09/distribution-testing-do-it-with-class/" rel="alternate" type="text/html"/><title>Distribution Testing: Do It With Class!</title><summary>After a long break since the last time we touched upon distribution testing, we are back to discuss two papers that appeared this month on the arXiv — and hopefully some of their implications. [ADK15] Optimal testing for properties of distributions. [CDRG15] Testing Shape Restrictions of Discrete Distributions. As a warmup, recall that in distribution testing (“property […]<div class="commentbar"><p/></div></summary><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://mittheory.files.wordpress.com/2015/07/stayclassy.png"><img alt="stayclassy" class="  wp-image-706 alignright" height="201" src="https://mittheory.files.wordpress.com/2015/07/stayclassy.png?w=209&amp;h=201" width="209"/></a>After a long break since <a href="https://mittheory.wordpress.com/2014/06/19/efficient-distribution-estimation-stoc-2014-recaps-part-2/">the last time</a> we touched upon distribution testing, we are back to discuss two papers that appeared this month on the arXiv — and hopefully some of their implications.</p>
<ul>
<li><a href="http://arxiv.org/abs/1507.05952">[ADK15]</a> Optimal testing for properties of distributions.</li>
<li><a href="http://arxiv.org/abs/1507.03558">[CDRG15]</a> Testing Shape Restrictions of Discrete Distributions.</li>
</ul>
<p><span id="more-705"/>As a warmup, recall that in <em>distribution testing</em> (“property testing of probability distributions,” but shorter), one is given access to independent samples from an unknown distribution <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p"/> over some fixed domain (say, the set of integers <img alt="[n] = \{1,\dots,n\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Bn%5D+%3D+%5C%7B1%2C%5Cdots%2Cn%5C%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="[n] = \{1,\dots,n\}"/>, and wishes to learn <em>one</em> bit of information about <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p"/>:</p>
<blockquote><p>Does <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p"/> have the property I’m interested in, or is it far from every distribution that has this property?</p></blockquote>
<p>In particular, the question is how to do things more efficiently (in terms of the number of samples required) than by fully learning the distribution <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p"/> (which would “cost” <img alt="\Theta(n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5CTheta%28n%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\Theta(n)"/> samples): surely, one bit of information is not <em>that</em> expensive, is it? Initiated by the seminal work of Batu, Fortnow, Rubinfeld, Smith, and White [BFRSW01], the field of distribution testing has since then aimed at answering that question by a resounding <em>“No, it isn’t! Mostly. Sort of?”</em> (And has had a fair amount of success in doing so.)</p>
<p>For more on property testing and distribution testing in general, we now take the liberty to refer the interested reader to [Ron08] and [Rub12,Can15] respectively.</p>
<h1>Testing Membership: why, how, err… <em>what</em>?</h1>
<p>Let <img alt="\mathcal{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BP%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathcal{P}"/> be a property of distributions, which is a fancy way of saying a subset or class of probability distributions. Think for instance of <img alt="\mathcal{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BP%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathcal{P}"/> as “the single uniform distribution on <img alt="[n]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Bn%5D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="[n]"/> elements,” or “the class of all Binomial distributions.”</p>
<p>We mentioned above that the goal was, given a distance parameter <img alt="\varepsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\varepsilon"/> as well as sample access to an arbitrary, unknown distribution <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p"/> over <img alt="[n]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Bn%5D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="[n]"/>, to determine the (asymptotic) minimum number of samples needed from <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p"/> to distinguish with high probability between the following two cases. <strong>(a) </strong><img alt="p\in\mathcal{P}" class="latex" src="https://s0.wp.com/latex.php?latex=p%5Cin%5Cmathcal%7BP%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p\in\mathcal{P}"/> (it has the property); versus <strong>(b) </strong><img alt="\ell_1(p,\mathcal{P})\geq \varepsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell_1%28p%2C%5Cmathcal%7BP%7D%29%5Cgeq+%5Cvarepsilon&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\ell_1(p,\mathcal{P})\geq \varepsilon"/> (<img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p"/> is at distance at least <img alt="\varepsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\varepsilon"/> from <em>every</em> distribution that has the property).</p>
<p>The metric here is the <img alt="\ell_1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell_1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\ell_1"/> distance, or equivalently the <em>total variation distance.</em> Now, many results, spanning the last decade, have culminated in pinpointing the exact sample complexity for various properties, such as <img alt="\{\mathcal{U}_n\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cmathcal%7BU%7D_n%5C%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\{\mathcal{U}_n\}"/> (being the uniform distribution), or <img alt="\{p^\ast\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bp%5E%5Cast%5C%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\{p^\ast\}"/> (being equal to a fixed distribution, specified in advance), or even <img alt="\mathcal{PBD}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BPBD%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathcal{PBD}"/> (being a Poisson Binomial Distribution, a generalization of the Binomial distributions). There has also been work in testing if a joint distribution is a product distribution (“independence”), or if a distribution has a monotone probability mass function.</p>
<p>Note that in the above, two trends appear: the first is consider a property that is a singleton, that is to test if the distribution <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p"/> is <em>equal</em> to something fixed in advance; while the second looks at a bigger set of distributions, typically a “<strong>class</strong>” of distributions that exhibits some structure; and amounts to testing if <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p"/> <em>belongs</em> to this class. (To be consistent with this change of terminology, we switch from now on from <img alt="\mathcal{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BP%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathcal{P}"/> — property — to <img alt="\mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathcal{C}"/> — class.)</p>
<div class="wp-caption aligncenter" id="attachment_715" style="width: 310px;"><a href="https://mittheory.files.wordpress.com/2015/07/binom.png"><img alt="binom" class="wp-image-715 size-medium" height="226" src="https://mittheory.files.wordpress.com/2015/07/binom.png?w=300&amp;h=226" width="300"/></a><p class="wp-caption-text">“We’re Binomials. Are you one of us?”</p></div>
<p>This latter type of result has many applications, e.g. for hypothesis testing, or model selection (<em>“Is my statistical model completely off if I assume it’s Gaussian?”,</em> or<em> “I know how to deal with histograms. Histograms are nice. Can we say it’s a histogram?”</em>). But for many classes of interest, the optimal sample complexity (as a function of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="n"/> and <img alt="\varepsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\varepsilon"/>) remained either completely open, or some gap subsisted between the known upper and lower bounds. (For instance, the cases of monotone distributions and histograms was addressed in [BKR04] and [ILR12], but the results left room for improvement.)</p>
<p>Up until now, roughly. Two very recent papers tackle this question, and simultaneously solve it for <strong>many</strong> such distribution classes at once. The first, [CDGR15], gives a generic “meta-algorithm” that does ‘quite well’ for a whole range of classes: that is, a <em>one-fits-all</em> tester which has nearly-tight sample complexity (with regard to <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="n"/>). Their main theorem has the following flavor:</p>
<blockquote><p>Let <img alt="\mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathcal{C}"/> be a class of distributions that all exhibit some nice structure. If it can be tested, there is a good chance this algorithm can — maybe not optimally, but close enough.</p></blockquote>
<p>The second, [ADK15] describes and instantiates a general method of “testing-by-learning” for distributions, which yields <em>optimal</em> sample complexity for several of these classes (with respect to both <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="n"/> and <img alt="\varepsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\varepsilon"/>, for a wide range of parameters). <span style="line-height: 1.6;">Interestingly, any such testing-by-learning approach was, until today, thought to be a dead-end for distribution testing — strong known impossibility results seemed to doom it. By coming up with an elegant twist, [ADK15] shows that, well, impossibility can only doom so much. Roughly and not quite accurately speaking, here is what they manage to show:</span></p>
<blockquote><p>Let <img alt="\mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathcal{C}"/> be a class of distributions which all exhibit some nice structure. If one can efficiently <img alt="\chi^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cchi%5E2&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\chi^2"/>-learn it, then one can optimally test it.</p></blockquote>
<h1>A Unified Approach to Things</h1>
<h3>One Tester to Rule Them All</h3>
<p>At the root of the generic tester of [CDGR15] is the observation that the testing algorithm of [BKR04], specifically tailored, built, and analyzed for testing whether a distribution is monotone, <em>actually</em> <em>need not be</em>. Namely, one can abstract and generalize the core idea of Batu, Kumar, and Rubinfeld; and by modifying it carefully obtain an algorithm that can apply to <em>any</em> class <img alt="\mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathcal{C}"/> that satisfies a structural assumption:</p>
<blockquote><p><em><strong>Definition 1.</strong> (Structural Criterion)</em> Any distribution in <img alt="\mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathcal{C}"/> is close (in a specific strong sense) to <em>some</em> piecewise-constant distribution on <img alt="L=L(\mathcal{C})" class="latex" src="https://s0.wp.com/latex.php?latex=L%3DL%28%5Cmathcal%7BC%7D%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L=L(\mathcal{C})"/> ‘pieces’, where <img alt="L" class="latex" src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L"/> is small (say <img alt="\mathrm{poly}\log n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bpoly%7D%5Clog+n&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathrm{poly}\log n"/>).</p></blockquote>
<p>By “close,” here we mean that on each of the <img alt="L" class="latex" src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L"/> “pieces” either the distribution <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p"/> puts very little probability weight, or stays within an <img alt="(1\pm \varepsilon)" class="latex" src="https://s0.wp.com/latex.php?latex=%281%5Cpm+%5Cvarepsilon%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="(1\pm \varepsilon)"/> factor of the piecewise-constant distribution <img alt="\bar{q} =\bar{q}(p)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbar%7Bq%7D+%3D%5Cbar%7Bq%7D%28p%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\bar{q} =\bar{q}(p)"/>. Moreover quite crucially one does <em>not</em> need to be able to compute this decomposition in pieces explicitly: it is sufficient to prove <em>existence</em>, for each <img alt="p\in\mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=p%5Cin%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p\in\mathcal{C}"/>, of a good decomposition.</p>
<blockquote><p><em><strong>Theorem 2.</strong> ([CDGR15, Main Theorem])</em> There exists an algorithm which, given sampling access to an unknown distribution <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p"/> over <img alt="[n]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Bn%5D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="[n]"/> and parameter <img alt="\varepsilon\in(0,1]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon%5Cin%280%2C1%5D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\varepsilon\in(0,1]"/>, can distinguish with probability 2/3 between <strong>(a) </strong><img alt="p\in\mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=p%5Cin%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p\in\mathcal{C}"/> versus <strong>(b) </strong><img alt="\ell_1(p, \mathcal{C}) &gt; \varepsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell_1%28p%2C+%5Cmathcal%7BC%7D%29+%3E+%5Cvarepsilon&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\ell_1(p, \mathcal{C}) &gt; \varepsilon"/>, for <em>any</em> class <img alt="\mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathcal{C}"/> that satisfies the above structural criterion. Moreover, for many such properties this algorithm is computationally efficient, and its sample complexity is optimal (up to logarithmic factors and the exact dependence on <img alt="\varepsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\varepsilon"/>).</p></blockquote>
<p>As it turns out, this assumption is satisfied by many interesting classes of distributions: monotone, <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="t"/>-modal, log-concave, monotone hazard rate, Poisson Binomials, histograms, to cite only few… moreover, it’s easy to see that any mixture of distributions from these “structural” classes will also satisfy the above criterion (for a corresponding value of <img alt="L" class="latex" src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L"/>).</p>
<p>To apply the tester to some class <img alt="\mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathcal{C}"/> you may fancy testing, it then only remains to find the best <img alt="L=L(n,\varepsilon)" class="latex" src="https://s0.wp.com/latex.php?latex=L%3DL%28n%2C%5Cvarepsilon%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L=L(n,\varepsilon)"/> possible for the structural criterion (i.e., to prove an existential result for the class, independent on any algorithmic aspect), and plug this into Theorem 2. This will immediately yield an upper bound — maybe not optimal, but <em>maybe</em> not far from it.</p>
<p>For an intuition of how the algorithm works, we paraphrase the authors’ description:</p>
<blockquote><p>The algorithm proceeds in 3 stages:</p>
<ol>
<li>the <em>decomposition step</em> attempts to recursively construct a partition of the domain in a small number of intervals <em style="font-style: normal;">[that is, <img alt="L(\mathcal{C})" class="latex" src="https://s0.wp.com/latex.php?latex=L%28%5Cmathcal%7BC%7D%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="L(\mathcal{C})"/>]</em>, with a very strong guarantee <em style="font-style: normal;">[the distribution is almost uniform on each piece]</em>. If this succeeds, then the unknown distribution <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p"/> will be close (in <img alt="\ell_1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell_1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\ell_1"/>) to its “flattening” <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="q"/> on the partition; while if it fails (too many intervals have to be created), this serves as evidence that <img alt="p\notin\mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=p%5Cnotin%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p\notin\mathcal{C}"/> and we can reject.</li>
<li>The second stage, the <em> approximation step,</em> learns this <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="q"/> — which can be done with few samples since by construction we do not have many intervals.</li>
<li>The last stage is purely computational, the <em>projection step</em>: we verify that <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="q"/> is indeed close to <img alt="\mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathcal{C}"/>.</li>
</ol>
<p>If all three stages succeed, then by the triangle inequality <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p"/> is close to <img alt="\mathcal{C};" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D%3B&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathcal{C};"/> and by the structural assumption on the class, if <img alt="p\in\mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=p%5Cin%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p\in\mathcal{C}"/> then it will admit succinct enough partitions, and all three stages will go through.</p></blockquote>
<h3>Upper bounds are fine, but lower bounds?</h3>
<p>As a counterpart to generically proving testing is not too hard, it would be nice to have an equally generic way of establishing it is actually not very easy either… Canonne, Diakonikolas, Gouleakis, and Rubinfeld also tackle this question, and provide a general <em>“testing-by-narrowing”</em> reduction that enables to do just that. We won’t dwelve into the details here (the reader is encouraged to read — or skim — the paper to learn more, including slightly more general statements and extensions to <em>tolerant</em> testing), but roughly speaking the theorem goes as follows:</p>
<blockquote><p><em><strong>Theorem 3.</strong> ([CDGR15, Lower Bound Theorem])</em> Let  <img alt="\mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathcal{C}"/> be a class of distributions that can be efficiently (agnostically) learned. Then,  testing <em>membership</em> to <img alt="\mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathcal{C}"/> is at least as hard as testing <em>identity</em> to any fixed distribution in <img alt="\mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathcal{C}"/>.</p></blockquote>
<p>While this may seem intuitive, there is actually no obvious reason why it should hold — and it actually fails if one removes the “efficiently learnable” assumption. (As an extreme case, consider the class <img alt="\mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathcal{C}"/> of <em>all</em> distributions (which is hard to learn). Testing if a distribution is in there can trivially be done with no sample at all, yet there are elements in <img alt="\mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathcal{C}"/> for which identity testing requires <img alt="\Omega(\sqrt{n})" class="latex" src="https://s0.wp.com/latex.php?latex=%5COmega%28%5Csqrt%7Bn%7D%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\Omega(\sqrt{n})"/> samples.)</p>
<p>As examples, [CDGR15] then instantiates Theorem 3 to show lower bounds for all the classes considered — choosing a suitably right “hard instance” in each case.</p>
<h3>So… we’re done?</h3>
<p>Well, not quite. As said above, this paper provides “nearly-tight” results for many classes, basically a Swiss army knife for class testing. Depending on the situation, it may be more than enough; but sometimes more fine-grained tools are needed…  what about an <em>optimal</em> testing algorithm for these classes? What about getting the sample complexity <em>right</em>? And what about testing in higher dimensions?</p>
<p>What about all of these? There is a next section, you know.</p>
<h1>Testing by Learning: “Yes, we can!”</h1>
<h3>A naive first approach</h3>
<p>As mentioned before, learning an <em>arbitrary</em> distribution on <img alt="[n]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Bn%5D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="[n]"/> is expensive, requiring <img alt="\Theta(n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5CTheta%28n%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\Theta(n)"/> samples. However, if we assume the distribution belongs to some class <img alt="\mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathcal{C}"/>, we may be able to do better. In particular, for many classes <img alt="\mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathcal{C}"/>, we can efficiently solve the <em>proper learning</em> problem, in which we output a near distribution in the same class. Formally stated:</p>
<blockquote><p>Given sample access to <img alt="p \in \mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=p+%5Cin+%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p \in \mathcal{C}"/>, output <img alt="q \in \mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=q+%5Cin+%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="q \in \mathcal{C}"/> such that <img alt="\ell_1(p,q) \leq \varepsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell_1%28p%2Cq%29+%5Cleq+%5Cvarepsilon&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\ell_1(p,q) \leq \varepsilon"/>.</p></blockquote>
<p>This leads to the following naive algorithm for testing. First, regardless of whether or not <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p"/> is actually in the class, attempt to properly learn it up to accuracy <img alt="\varepsilon/2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon%2F2&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\varepsilon/2"/>. If <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p"/> is in the class, then we will learn a distribution <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="q"/> which is actually <img alt="\varepsilon/2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon%2F2&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\varepsilon/2"/>-close. If it is not, since <img alt="q \in \mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=q+%5Cin+%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="q \in \mathcal{C}"/>, by assumption, <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p"/> and <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="q"/> are <img alt="\varepsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\varepsilon"/>-far. Thus, we have reduced to the following testing problem:</p>
<blockquote><p>Given sample access to <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p"/> and a description of a distribution <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="q"/>, identify whether they are <img alt="\varepsilon/2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon%2F2&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\varepsilon/2"/>-close in <img alt="\ell_1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell_1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\ell_1"/>-distance or <img alt="\varepsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\varepsilon"/>-far in <img alt="\ell_1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell_1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\ell_1"/>-distance.</p></blockquote>
<p>Unfortunately, this is where we hit a brick wall: Valiant and Valiant showed that this problem requires <img alt="\Omega(n/\log n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5COmega%28n%2F%5Clog+n%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\Omega(n/\log n)"/> samples, even for the simplest distribution <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="q"/> possible, the uniform distribution [VV10]. All this work, only to achieve a <em>barely sublinear</em> tester?</p>
<h3>The solution: relax</h3>
<p>It turns out that one can circumvent this lower bound by considering a relaxation of the previous problem. For this purpose, we shall use the <img alt="\chi^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cchi%5E2&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\chi^2"/>-distance (which can be seen as a pointwise nonuniform rescaling of <img alt="\ell_2^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell_2%5E2&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\ell_2^2"/>): <img alt="\chi^2(p,q) = \sum_{i=1}^n \frac{(p(i)-q(i))^2}{q(i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cchi%5E2%28p%2Cq%29+%3D+%5Csum_%7Bi%3D1%7D%5En+%5Cfrac%7B%28p%28i%29-q%28i%29%29%5E2%7D%7Bq%28i%29%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\chi^2(p,q) = \sum_{i=1}^n \frac{(p(i)-q(i))^2}{q(i)}"/>. The main feature of this distance we require is that <img alt="\chi^2(p,q) \leq \ell_1^2(p,q)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cchi%5E2%28p%2Cq%29+%5Cleq+%5Cell_1%5E2%28p%2Cq%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\chi^2(p,q) \leq \ell_1^2(p,q)"/>. As such, we can now consider the following (easier) testing problem:</p>
<blockquote><p>Given sample access to <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p"/> and a description of a distribution <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="q"/>, identify whether they are <img alt="\varepsilon^2/2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon%5E2%2F2&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\varepsilon^2/2"/>-close in <img alt="\chi^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cchi%5E2&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\chi^2"/>-distance or <img alt="\varepsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\varepsilon"/>-far in <img alt="\ell_1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell_1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\ell_1"/>-distance.</p></blockquote>
<p>Now, the upshot: surprisingly, this new problem <strong>can</strong> be solved using <img alt="O(\sqrt{n}/\varepsilon^2)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28%5Csqrt%7Bn%7D%2F%5Cvarepsilon%5E2%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="O(\sqrt{n}/\varepsilon^2)"/> samples, significantly bypassing the previous <img alt="\Omega(n/\log n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5COmega%28n%2F%5Clog+n%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\Omega(n/\log n)"/> lower bound. The actual test is a slight modification of the <img alt="\chi^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cchi%5E2&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\chi^2"/>-test by Pearson, a statistician so far ahead of his time that he casually introduced techniques which lead to optimal algorithms for major problems over a century later (and <a href="http://blog.mrtz.org/2014/04/22/pearsons-polynomial.html">this isn’t the first time, either</a>).</p>
<div class="wp-caption aligncenter" id="attachment_754" style="width: 310px;"><a href="https://mittheory.files.wordpress.com/2015/07/brickwallbreaking.png"><img alt="Take that, brick wall. (Image from penningtonhennessy.com)" class="wp-image-754 size-medium" height="225" src="https://mittheory.files.wordpress.com/2015/07/brickwallbreaking.png?w=300&amp;h=225" width="300"/></a><p class="wp-caption-text">Another brick off the wall!</p></div>
<h3>The fruits of our labor</h3>
<p>Given this powerful primitive, we have a framework, which is a slight modification of the naive approach described above. Instead of: proper learn, then test; we proper learn <em>in <img alt="\chi^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cchi%5E2&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\chi^2"/>-distance</em>, then test:</p>
<blockquote><p>1. Use samples to obtain an explicit <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="q"/> s.t.<br/>
– if <img alt="p\in \mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=p%5Cin+%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p\in \mathcal{C}"/>, <img alt="\chi^2(p,q)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cchi%5E2%28p%2Cq%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\chi^2(p,q)"/> small<br/>
– if <img alt="p\notin \mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=p%5Cnotin+%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="p\notin \mathcal{C}"/>, <img alt="\ell_1(p,q)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell_1%28p%2Cq%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\ell_1(p,q)"/> large<br/>
2. Perform the <img alt="\chi^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cchi%5E2&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\chi^2"/>-close vs <img alt="\ell_1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell_1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\ell_1"/>-far test.</p></blockquote>
<p>The learning problem is now slightly harder than before, but still requires <img alt="o(\sqrt{n}/\varepsilon^2)" class="latex" src="https://s0.wp.com/latex.php?latex=o%28%5Csqrt%7Bn%7D%2F%5Cvarepsilon%5E2%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="o(\sqrt{n}/\varepsilon^2)"/> samples for a large number of classes in the parameter regime of interest. As a result, this leads to optimal algorithms for testing many classes, including monotonicity, unimodality, log-concavity, and monotone hazard rate.</p>
<p>The story doesn’t end here — this framework also applies for testing multivariate discrete distributions. While the previous literature on testing one-dimensional distributions was quite mature (i.e., for the previously studied classes, we sort of “knew” the right answer to be <img alt="\Theta(\sqrt{n})" class="latex" src="https://s0.wp.com/latex.php?latex=%5CTheta%28%5Csqrt%7Bn%7D%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\Theta(\sqrt{n})"/>, up to log factors and the precise dependence on epsilon), fairly little was known about higher dimensional testing problems. This work manages to give optimal algorithms for testing monotonicity and independence of marginals over the <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="d"/>-dimensional hypergrid. In particular, for monotonicity, it gives a quadratic improvement in the sample complexity, reducing it from <img alt="O(n^{d - \frac12})" class="latex" src="https://s0.wp.com/latex.php?latex=O%28n%5E%7Bd+-+%5Cfrac12%7D%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="O(n^{d - \frac12})"/> to the optimal <img alt="\Theta(n^{\frac{d}{2}})" class="latex" src="https://s0.wp.com/latex.php?latex=%5CTheta%28n%5E%7B%5Cfrac%7Bd%7D%7B2%7D%7D%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\Theta(n^{\frac{d}{2}})"/>.</p>
<h1>Conclusions</h1>
<p>The hope is that these frameworks may find applications for many other problems, in distribution testing and beyond. <a href="https://youtu.be/74Uq-PxH-es?t=7s">The world is yours…</a></p>
<p>— <a href="http://www.cs.columbia.edu/~ccanonne/">Clément Canonne</a> and <a href="http://www.gautamkamath.com/">Gautam Kamath</a></p>
<h1>References</h1>
<p>[ADK15] <a href="http://arxiv.org/abs/1507.05952">Optimal testing for properties of distributions</a>. J. Acharya, C. Daskalakis, and G. Kamath. arXiv, 2015.<br/>
[BFRSW01] <a href="http://arxiv.org/abs/1009.5397">Testing that distributions are close</a>. T. Batu, L. Fortnow, R. Rubinfeld, W. D. Smith, and P. White,  FOCS’01.<br/>
[BKR04] <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.212.1794MOVKOsD71fS7dDfG5rKan8g">Sublinear algorithms for testing monotone and unimodal distributions</a>. T. Batu, R. Kumar, and R. Rubinfeld, STOC’04.<br/>
[Can15] <a href="http://eccc.hpi-web.de/report/2015/063/">A Survey on Distribution Testing: Your Data is Big. But is it Blue?</a>. C. Canonne. ECCC, 2015.<br/>
[CDGR15] <a href="http://arxiv.org/abs/1507.03558">Testing Shape Restrictions of Discrete Distributions</a>. C. Canonne, I. Diakonikolas, T. Gouleakis, and R, Rubinfeld. arXiv, 2015.<br/>
[ILR12] <a href="http://eccc.hpi-web.de/report/2011/171/">Approximating and Testing <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="k"/>-Histogram Distributions in Sub-linear Time</a>. P. Indyk, R. Levi, and R. Rubinfeld, PODS’12.<br/>
[Ron08] <a href="http://www.eng.tau.ac.il/~danar/Public-pdf/fnt-ml.pdf">Property Testing: A Learning Theory Perspective</a>. D. Ron. FTML, 2008.<br/>
[Rub12] <a href="http://people.csail.mit.edu/ronitt/papers/xrds-rr-dist.pdf">Taming Big Probability Distributions</a>. R. Rubinfeld. XRDS, 2012.<br/>
[VV10] <a href="http://eccc.hpi-web.de/report/2010/179/">A CLT and tight lower bounds for estimating entropy</a>. G. Valiant and P. Valiant. ECCC, 2010.</p></div></content><updated planet:format="August 09, 2015 05:14 PM">2015-08-09T17:14:00Z</updated><published planet:format="August 09, 2015 05:14 PM">2015-08-09T17:14:00Z</published><category term="algorithms"/><category term="complexity"/><author><name>mittheory</name></author><source><id>https://mittheory.wordpress.com</id><logo>https://s0.wp.com/i/buttonw-com.png</logo><link href="https://mittheory.wordpress.com/feed/" rel="self" type="application/atom+xml"/><link href="https://mittheory.wordpress.com" rel="alternate" type="text/html"/><link href="https://mittheory.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/><link href="https://mittheory.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/><subtitle>A student blog of MIT CSAIL Theory of Computation Group</subtitle><title>Not so Great Ideas in Theoretical Computer Science</title><updated planet:format="December 17, 2018 05:30 AM">2018-12-17T05:30:11Z</updated><planet:module>toc</planet:module><planet:format>atom10</planet:format><planet:http_last_modified>Thu, 12 Apr 2018 01:31:44 GMT</planet:http_last_modified><planet:bozo>false</planet:bozo><planet:items_per_page>40</planet:items_per_page><planet:css-id>mit-csail-student-blog</planet:css-id><planet:face>csail.png</planet:face><planet:name>MIT CSAIL student blog</planet:name><planet:http_location>https://mittheory.wordpress.com/feed/</planet:http_location><planet:http_status>301</planet:http_status></source></entry>
