<?xml version="1.0" encoding="utf-8"?><entry xml:lang="en" xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/"><id>http://rjlipton.wordpress.com/?p=15499</id><link href="https://rjlipton.wordpress.com/2018/12/09/transposing/" rel="alternate" type="text/html"/><title>Transposing</title><summary>On the arithmetic complexity of the matrix transpose [ KKB ] Michael Kaminski, David Kirkpatrick, and Nader Bshouty are complexity theorists who together and separately have proved many wonderful theorems. They wrote an interesting paper recently—well not quite—in 1988 about the transpose operation. Today we want to discuss an alternative proof of the main result […]<div class="commentbar"><p/></div></summary><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>On the arithmetic complexity of the matrix transpose</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2018/12/09/transposing/kkb/" rel="attachment wp-att-15501"><img alt="" class="alignright wp-image-15501" src="https://rjlipton.files.wordpress.com/2018/12/kkb.png?w=250" width="250"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ KKB ]</font></td>
</tr>
</tbody>
</table>
<p>
Michael Kaminski, David Kirkpatrick, and Nader Bshouty are complexity theorists who together and separately have proved many wonderful theorems. They wrote an interesting <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.3626&amp;rep=rep1&amp;type=pdf">paper</a> recently—well not quite—in 1988 about the transpose operation.</p>
<p>
Today we want to discuss an alternative proof of the main result of that paper.<br/>
<span id="more-15499"/></p>
<p>
</p><p/><h2> The Transpose </h2><p/>
<p/><p>
The operation that maps a <img alt="{n \times n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n \times n}"/> matrix <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> to its transpose <img alt="{A^{T}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%5E%7BT%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A^{T}}"/> is quite important in many aspects of linear algebra. Recall that the transpose is defined by 	</p>
<p align="center"><img alt="\displaystyle  A^{T}(i,j) = A(j,i) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++A%5E%7BT%7D%28i%2Cj%29+%3D+A%28j%2Ci%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  A^{T}(i,j) = A(j,i) "/></p>
<p>for all indices <img alt="{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i,j}"/>. A non-trivial issue arises already in proving this: </p>
<blockquote><p><b> </b> <em> <i>If <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A}"/> has an inverse then so does <img alt="{A^{T}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%5E%7BT%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A^{T}}"/></i>. </em>
</p></blockquote>
<p>There are many proofs of this basic fact about the transpose, but it is not simple to prove it from first principles. For example look at William Wardlaw’s proof <a href="https://www.maa.org/sites/default/files/3004418139737.pdf.bannered.pdf">here</a>. Or <a href="http://www.doc.ic.ac.uk/~ae/papers/lecture04.pdf">here</a> for another.</p>
<p>
</p><p/><h2> A Theorem </h2><p/>
<p/><p>
The paper of Kaminski, Kirkpatrick, and Bshouty (KKB) came up the other day, while I visited the computer science department at University of Buffalo. Atri Rudra told about some of his recent work on various complexity issues around matrix computations. One was an interesting question about the transpose operation on matrices. The result is the following: </p>
<blockquote><p><b>Theorem 1</b> <em> If the arithmetic complexity of <img alt="{x \rightarrow Ax}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%5Crightarrow+Ax%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{x \rightarrow Ax}"/> is <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{S}"/> then the arithmetic complexity of <img alt="{y \rightarrow A^{T}y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By+%5Crightarrow+A%5E%7BT%7Dy%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{y \rightarrow A^{T}y}"/> is at most <img alt="{O(S + n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28S+%2B+n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{O(S + n)}"/>. </em>
</p></blockquote>
<p>KKB had a nice proof of this theorem over forty years ago. Indeed they can get a precise expression for the complexity that is tighter than the above statement. Their proof is a careful examination of the structure of any arithmetic circuit that computes <img alt="{Ax}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Ax}"/>. Essentially they show one can in a sense “run the computation backwards.” </p>
<p>
</p><p/><h2> Another Proof </h2><p/>
<p/><p>
Anyway in my discussion with Atri, the other day, he described another proof of this basic theorem. He said consider the function that maps <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> to <img alt="{Ax}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Ax}"/> for a fixed matrix <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/>. Suppose this linear map has arithmetic complexity <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>: that is the minimum number of arithmetic operations that are needed to compute <img alt="{Ax}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Ax}"/> is <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>. Note, <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> can vary greatly with the structure of <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/>. Even for nonsingular matrices the complexity can vary quite a bit: for a random matrix is likely to be order <img alt="{n^{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^{2}}"/>; for a Fourier transform matrix it is order <img alt="{n\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n\log n}"/>.</p>
<p>
He said it is known that the arithmetic complexity of <img alt="{Ax}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Ax}"/> and <img alt="{A^{T}x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%5E%7BT%7Dx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A^{T}x}"/> are about the same. But proving this, while not hard, requires some care. Atri told me about a quite neat argument that proves it. Further, the proof is “two-lines” as Atri said:</p>
<p>
<em>Proof:</em>  Consider the function <img alt="{f(x)=y^{T}Ax}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%3Dy%5E%7BT%7DAx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x)=y^{T}Ax}"/> as a function of <img alt="{x=(x_{1},\dots,x_{n})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%3D%28x_%7B1%7D%2C%5Cdots%2Cx_%7Bn%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x=(x_{1},\dots,x_{n})}"/>. The gradient <img alt="{ \nabla f(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+%5Cnabla+f%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{ \nabla f(x)}"/> is equal to all the partial derivatives of <img alt="{f(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x)}"/>. This means that it allows us to compute <img alt="{y^{T}A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%5E%7BT%7DA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y^{T}A}"/>, since the function is linear in each variable <img alt="{x_{i}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_{i}}"/>. The famous <a href="https://core.ac.uk/download/pdf/82480031.pdf">Derivative</a> <a href="https://rjlipton.wordpress.com/2010/03/27/fast-matrix-products-and-other-amazing-results/">Lemma</a> of Walter Baur and Volker Strassen shows that a single arithmetical circuit of size order <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> plus the complexity of <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/> can compute <img alt="{\nabla f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cnabla+f%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\nabla f}"/>. It follows that we can compute all the <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> partial derivatives in <img alt="{O(S+n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28S%2Bn%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(S+n)}"/> arithmetic steps. But for our function <img alt="{f(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x)}"/> this is equal to <img alt="{y^{T}A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%5E%7BT%7DA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y^{T}A}"/>. Transposing the resulting vector gives <img alt="{A^{T}y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%5E%7BT%7Dy%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A^{T}y}"/> in the required number of steps. <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
I like this proof quite a bit. Can this argument be used to prove the basic fact about inverses of a matrix and its transpose? </p>
<p/></font></font></div></content><updated planet:format="December 10, 2018 04:00 AM">2018-12-10T04:00:02Z</updated><published planet:format="December 10, 2018 04:00 AM">2018-12-10T04:00:02Z</published><category term="Ideas"/><category term="Oldies"/><category term="People"/><category term="Proofs"/><category term="arithmetic complexity"/><category term="matrix transpose"/><category term="Strassen"/><author><name>rjlipton</name></author><source><id>https://rjlipton.wordpress.com</id><logo>https://s0.wp.com/i/buttonw-com.png</logo><link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/><link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/><link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/><link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/><subtitle>a personal view of the theory of computation</subtitle><title>Gödel’s Lost Letter and P=NP</title><updated planet:format="December 17, 2018 05:29 AM">2018-12-17T05:29:25Z</updated><planet:module>toc</planet:module><planet:format>atom10</planet:format><planet:http_last_modified>Sat, 15 Dec 2018 18:49:29 GMT</planet:http_last_modified><planet:bozo>false</planet:bozo><planet:items_per_page>40</planet:items_per_page><planet:css-id>richard-lipton</planet:css-id><planet:face>lipton.jpeg</planet:face><planet:name>Richard Lipton</planet:name><planet:http_location>https://rjlipton.wordpress.com/feed/</planet:http_location><planet:http_status>301</planet:http_status></source></entry>
