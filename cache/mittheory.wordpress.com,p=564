<?xml version="1.0" ?><entry xml:lang="en" xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/"><id>http://mittheory.wordpress.com/?p=564</id><link href="https://mittheory.wordpress.com/2014/07/11/insensitive-intersections-of-halfspaces-stoc-2014-recaps-part-11/" rel="alternate" type="text/html"/><title>Insensitive Intersections of Halfspaces – STOC 2014 Recaps (Part 11)</title><summary>In the eleventh and final installment of our STOC 2014 recaps, Jerry Li tells us about a spectacularly elegant result by Daniel Kane. It’s an example of what I like to call a “one-page wonder” — this a bit of a misnomer, since Kane’s paper is slightly more than five pages long, but the term refers to any beautiful paper which […]</summary><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In the eleventh and final installment of our STOC 2014 recaps, <a href="http://www.mit.edu/~jerryzli/">Jerry Li</a> tells us about a spectacularly elegant result by Daniel Kane. It’s an example of what I like to call a “one-page wonder” — this a bit of a misnomer, since Kane’s paper is slightly more than five pages long, but the term refers to any beautiful paper which is (relatively) short and readable.</p>
<p>We hope you’ve enjoyed our coverage of this year’s STOC. We were able to write about a few of our favorite results, but there’s a wealth of other interesting papers that deserve your attention. I encourage you to peruse the <a href="http://www.sigact.org/Proceedings/Most_Recent/stoc.html">proceedings</a> and discover some favorites of your own.</p>
<hr/>
<p>Jerry Li on <a href="http://arxiv.org/abs/1309.2987"><strong>The Average Sensitivity of an Intersection of Half Spaces</strong></a>, by <a href="http://math.stanford.edu/~dankane/">Daniel Kane</a></p>
<p>The Monday afternoon sessions kicked off with Daniel Kane presenting his work on the average sensitivity of an intersection of halfspaces. Usually, FOCS/STOC talks can’t even begin to fit all the technical details from the paper, but unusually, Daniel’s talk included a complete proof of his result, without omitting any details. Amazingly, his result is very deep and general, so something incredible is clearly happening somewhere.</p>
<p>At the highest level, Daniel deals with the study of a certain class of Boolean functions. When we classically think about Boolean functions, we think of things such as <a href="http://en.wikipedia.org/wiki/Conjunctive_normal_form">CNFs</a>, <a href="http://en.wikipedia.org/wiki/Disjunctive_normal_form">DNFs</a>, <a href="http://en.wikipedia.org/wiki/Decision_tree">decision trees</a>, etc., which map into things like <img alt="\mathbb{F}_2, \{0, 1\}," class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BF%7D_2%2C+%5C%7B0%2C+1%5C%7D%2C&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathbb{F}_2, \{0, 1\},"/> or <img alt="\{\mbox{True}, \mbox{False}\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cmbox%7BTrue%7D%2C+%5Cmbox%7BFalse%7D%5C%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\{\mbox{True}, \mbox{False}\}"/>, but today, and often in the study of Boolean analysis, we will think of functions as mapping <img alt="\{-1, 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B-1%2C+1%5C%7D%5En&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\{-1, 1\}^n"/> to <img alt="\{-1, 1\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B-1%2C+1%5C%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\{-1, 1\}"/>, which is roughly equivalent for many purposes (O’Donnell has a nice rule of thumb as when to use one convention or the other <a href="http://www.contrib.andrew.cmu.edu/~ryanod/?p=2285">here</a>). Given a function <img alt="f:\{-1, 1\}^n \to \{-1, 1\}" class="latex" src="https://s0.wp.com/latex.php?latex=f%3A%5C%7B-1%2C+1%5C%7D%5En+%5Cto+%5C%7B-1%2C+1%5C%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f:\{-1, 1\}^n \to \{-1, 1\}"/>, we can define two important measures of sensitivity. The first is the <i>average sensitivity</i> (or, for those of you like me who grew up with O’Donnell’s book, the total influence) of the function, namely,</p>
<p style="text-align: center;"><img alt="\mathbb{AS}(f) = \mathbb{E}_{x \sim \{-1, 1\}^n} [| \{i: f(x^{i \to 1}) \neq f(x^{i \to -1}) \} |]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BAS%7D%28f%29+%3D+%5Cmathbb%7BE%7D_%7Bx+%5Csim+%5C%7B-1%2C+1%5C%7D%5En%7D+%5B%7C+%5C%7Bi%3A+f%28x%5E%7Bi+%5Cto+1%7D%29+%5Cneq+f%28x%5E%7Bi+%5Cto+-1%7D%29+%5C%7D+%7C%5D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathbb{AS}(f) = \mathbb{E}_{x \sim \{-1, 1\}^n} [| \{i: f(x^{i \to 1}) \neq f(x^{i \to -1}) \} |]"/></p>
<p>where <img alt="x^{i \to a}" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%7Bi+%5Cto+a%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="x^{i \to a}"/> is simply <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="x"/> with its <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="i"/>th coordinate set to <img alt="a" class="latex" src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="a"/>. The second is the <i>noise sensitivity</i> of the function, which is defined similarly: for a parameter <img alt="\varepsilon \in (0, 1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon+%5Cin+%280%2C+1%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\varepsilon \in (0, 1)"/>, it is the probability that if we sample <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="x"/> uniformly at random from <img alt="\{-1, 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B-1%2C+1%5C%7D%5En&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\{-1, 1\}^n"/>, then independently flip each of its bits with probability <img alt="\varepsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\varepsilon"/>, the value of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f"/> at these two inputs is different. We denote this quantity <img alt="\mathbb{NS}_\varepsilon (f)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BNS%7D_%5Cvarepsilon+%28f%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathbb{NS}_\varepsilon (f)"/>. When we generate a string <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="y"/> from a string <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="x"/> in this way we say they are <img alt="1 - 2\varepsilon" class="latex" src="https://s0.wp.com/latex.php?latex=1+-+2%5Cvarepsilon&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="1 - 2\varepsilon"/>-correlated. The weird function of <img alt="\varepsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\varepsilon"/> in that expression is because often we equivalently think of <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="y"/> being generated from <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="x"/> by independently keeping each coordinate of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="x"/> fixed with probability <img alt="1 - 2\varepsilon" class="latex" src="https://s0.wp.com/latex.php?latex=1+-+2%5Cvarepsilon&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="1 - 2\varepsilon"/>, and uniformly rerandomizing that bit with probability <img alt="2 \varepsilon" class="latex" src="https://s0.wp.com/latex.php?latex=2+%5Cvarepsilon&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="2 \varepsilon"/>.</p>
<p>Why are these two measures important? If we have a concept class of functions <img alt="\mathcal{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathcal{F}"/>, then it turns out that bounds on these two quantities can often be translated directly into learning algorithms for these classes. By Fourier analytic arguments, good bounds on the noise sensitivity of a function immediately imply that the function has good Fourier concentration on low degree terms, which in turn imply that the so-called “low-degree algorithm” can efficiently learn the class of functions in the <a href="http://en.wikipedia.org/wiki/Probably_approximately_correct_learning">PAC model</a>, with random access. Unfortunately, I can’t really give any more detail here without a lot more technical detail, see [2] for a good introduction to the topic.</p>
<p>Now why is the average sensitivity of a Boolean function important? First of all, trust me when I say that it’s a fundamental measure of the robustness of the function. If we identify <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f"/> with <img alt="f^{-1} (1)" class="latex" src="https://s0.wp.com/latex.php?latex=f%5E%7B-1%7D+%281%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f^{-1} (1)"/>, then the average sensitivity is how many edges cross from one subset into another (over <img alt="2^n" class="latex" src="https://s0.wp.com/latex.php?latex=2%5En&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="2^n"/>), so it is fundamentally related to the surface area of subsets of the hypercube, which comes up all over the place in Boolean analysis. Secondly, in some cases, we can translate between one measure and the other by considering restrictions of functions. To the best of my knowledge, this appears to be a technique first introduced by Peres in 1999, though his paper is from 2004 [3]. Let <img alt="f: \{-1, +1\}^n \to \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=f%3A+%5C%7B-1%2C+%2B1%5C%7D%5En+%5Cto+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f: \{-1, +1\}^n \to \mathbb{R}"/>. We wish to bound the noise sensitivity of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f"/>, so we need to see how it behaves when we generate <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="x"/> uniformly at random, then <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="y"/> as <img alt="1 - 2 \varepsilon" class="latex" src="https://s0.wp.com/latex.php?latex=1+-+2+%5Cvarepsilon&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="1 - 2 \varepsilon"/>-correlated to <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="x"/>. Suppose <img alt="\varepsilon = 1/m" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon+%3D+1%2Fm&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\varepsilon = 1/m"/> for some integer <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="m"/> (if not, just round it). Fix a partition of the <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="n"/> coordinates into <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="m"/> bins <img alt="B_1, \ldots, B_m" class="latex" src="https://s0.wp.com/latex.php?latex=B_1%2C+%5Cldots%2C+B_m&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="B_1, \ldots, B_m"/>, and a <img alt="z \in \{-1, 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=z+%5Cin+%5C%7B-1%2C+1%5C%7D%5En&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="z \in \{-1, 1\}^n"/>. Then, for any string <img alt="s \in \{-1, 1\}^m" class="latex" src="https://s0.wp.com/latex.php?latex=s+%5Cin+%5C%7B-1%2C+1%5C%7D%5Em&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="s \in \{-1, 1\}^m"/>, we associate it with the string <img alt="x \in \{-1, 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5C%7B-1%2C+1%5C%7D%5En&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="x \in \{-1, 1\}^n"/> whose <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="i"/>th coordinate is the <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="i"/>th coordinate of <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="z"/> times the <img alt="j" class="latex" src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="j"/>th coordinate of <img alt="s" class="latex" src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="s"/>, if <img alt="i \in B_j" class="latex" src="https://s0.wp.com/latex.php?latex=i+%5Cin+B_j&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="i \in B_j"/>. Why are we doing this? Well, after some thought, it’s not too hard to convince yourself that if we choose the <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="m"/> bins and the strings <img alt="z, s" class="latex" src="https://s0.wp.com/latex.php?latex=z%2C+s&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="z, s"/> uniformly at random, then we get a uniformly random string <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="x"/>. Moreover, to generate a string <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="y"/> which is <img alt="1 - 2 \varepsilon" class="latex" src="https://s0.wp.com/latex.php?latex=1+-+2+%5Cvarepsilon&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="1 - 2 \varepsilon"/>-correlated with <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="x"/>, it suffices to, after having already randomly chosen the bins, <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="z"/>, and <img alt="s" class="latex" src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="s"/>, to randomly pick a coordinate of <img alt="s" class="latex" src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="s"/> and flip its sign to produce a new string <img alt="s'" class="latex" src="https://s0.wp.com/latex.php?latex=s%27&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="s'"/>, and produce a new string <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="y"/> with these choices of the bins, <img alt="z," class="latex" src="https://s0.wp.com/latex.php?latex=z%2C&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="z,"/> and <img alt="s'" class="latex" src="https://s0.wp.com/latex.php?latex=s%27&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="s'"/>. Thus, importantly, we can reduce the process of producing <img alt="1 - 2 \varepsilon" class="latex" src="https://s0.wp.com/latex.php?latex=1+-+2+%5Cvarepsilon&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="1 - 2 \varepsilon"/>-correlated strings to the process of randomly flipping one bit of some weird new function–but this is the process we consider when we consider the average sensitivity! Thus noise sensitivity of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f"/> is exactly equal to the expected (over the random choice of the bins and <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="z"/>) average sensitivity of this weird restricted thing. Why this is useful will (hopefully) become clear later.</p>
<p>Since the title of the paper includes the phrase “intersection of halfspaces,” at some point I should probably define what an intersection of halfspaces is. First of all, a halfspace (or linear threshold function) is a Boolean function of the form <img alt="f(x) = \text{sgn}\left(w \cdot x - \theta\right)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29+%3D+%5Ctext%7Bsgn%7D%5Cleft%28w+%5Ccdot+x+-+%5Ctheta%5Cright%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f(x) = \text{sgn}\left(w \cdot x - \theta\right)"/> where <img alt="w \in \mathbb{R}^n, \theta \in \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=w+%5Cin+%5Cmathbb%7BR%7D%5En%2C+%5Ctheta+%5Cin+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="w \in \mathbb{R}^n, \theta \in \mathbb{R}"/> and for concreteness let’s say <img alt="\text{sgn}\left(0\right) = 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7Bsgn%7D%5Cleft%280%5Cright%29+%3D+1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\text{sgn}\left(0\right) = 1"/> (however, it’s not too hard to see that any halfspace has a representation so that the linear function inside the sign is never zero on the hypercube). Intuitively, take the hyperplane in <img alt="\mathbb{R}^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathbb{R}^n"/> with normal vector <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="w"/>, then assign to all points which are in the same side as <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="w"/> of the hyper plane the value <img alt="+1" class="latex" src="https://s0.wp.com/latex.php?latex=%2B1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="+1"/>, and the rest <img alt="-1" class="latex" src="https://s0.wp.com/latex.php?latex=-1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="-1"/>. Halfspaces are an incredibly rich family of Boolean functions which include arguably some of the important objects in Boolean analysis, such as the dictator functions, the majority function, etc. There is basically a mountain of work on halfspaces, due to their importance in learning theory, and as elementary objects which capture a surprising amount of structure.</p>
<p>Secondly, the intersection of <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="k"/> functions <img alt="f_1, \ldots, f_k" class="latex" src="https://s0.wp.com/latex.php?latex=f_1%2C+%5Cldots%2C+f_k&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f_1, \ldots, f_k"/> is the function which is <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="1"/> at <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="x"/> if and only <img alt="f_i (x) = 1" class="latex" src="https://s0.wp.com/latex.php?latex=f_i+%28x%29+%3D+1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f_i (x) = 1"/> for all <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="i"/>, and <img alt="-1" class="latex" src="https://s0.wp.com/latex.php?latex=-1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="-1"/> otherwise. If we think of each <img alt="f_i" class="latex" src="https://s0.wp.com/latex.php?latex=f_i&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f_i"/> as a <img alt="\{ \mbox{True}, \mbox{False}\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B+%5Cmbox%7BTrue%7D%2C+%5Cmbox%7BFalse%7D%5C%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\{ \mbox{True}, \mbox{False}\}"/> predicate on the boolean cube, then their intersection is simply their AND (or NOR, depending on your map from <img alt="\{-1, 1\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B-1%2C+1%5C%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\{-1, 1\}"/> to <img alt="\{ \mbox{True}, \mbox{False}\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B+%5Cmbox%7BTrue%7D%2C+%5Cmbox%7BFalse%7D%5C%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\{ \mbox{True}, \mbox{False}\}"/>).</p>
<p>Putting these together gives us the family of functions that Daniel’s work concerns. I don’t know what else to say other than they are a natural algebraic generalization of halfspaces. Hopefully you think these functions are interesting, but even if you don’t, it’s (kinda) okay, because, amazingly, it turns out Kane’s main result barely uses any properties of halfspaces! In fact, it only uses the fact that halfspaces are <i>unate</i>, that is, they are either monotone increasing or decreasing in each coordinate. In fact, he proves the following, incredibly general, theorem:</p>
<p><b>Theorem.</b> [Kane14]<br/>
Let <img alt="f:\{-1, 1\}^n \to \{-1, 1\}" class="latex" src="https://s0.wp.com/latex.php?latex=f%3A%5C%7B-1%2C+1%5C%7D%5En+%5Cto+%5C%7B-1%2C+1%5C%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f:\{-1, 1\}^n \to \{-1, 1\}"/> be an intersection of <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="k"/> unate functions. Then</p>
<p style="text-align: center;"><img alt="\mathbb{AS}(f) = O(\sqrt{n \log k})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BAS%7D%28f%29+%3D+O%28%5Csqrt%7Bn+%5Clog+k%7D%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathbb{AS}(f) = O(\sqrt{n \log k})"/>.</p>
<p>I’m not going to go into too much detail about the proof; unfortunately, despite my best efforts there’s not much intuition I can compress out of it (in his talk, Daniel himself admitted that there was a lemma which was mysterious even to him). Plus it’s only roughly two pages of elementary (but extremely deep) calculations, just read it yourself! At a very, very high level, the idea is that intersecting a intersection of <img alt="k - 1" class="latex" src="https://s0.wp.com/latex.php?latex=k+-+1&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="k - 1"/> halfspaces with one more can only increase the average sensitivity by a small factor.</p>
<p>The really attentive reader might have also figured out why I gave that strange reduction between noise sensitivity and average sensitivity. This is because, importantly, when we apply this weird process of randomly choosing bins to an intersection of halfspaces, the resulting function is still an intersection of halfspaces, just over fewer coordinates (besides their unate-ness, this is the only property of intersections of halfspaces that Daniel uses). Thus, since we now know how to bound the average sensitivity of halfspaces, we also know tight bounds for the noise sensitivities of intersection of halfspaces, namely, the following:</p>
<p><b>Theorem.</b> [Kane14]<br/>
Let <img alt="f:\{-1, 1\}^n \to \{-1, 1\}" class="latex" src="https://s0.wp.com/latex.php?latex=f%3A%5C%7B-1%2C+1%5C%7D%5En+%5Cto+%5C%7B-1%2C+1%5C%7D&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="f:\{-1, 1\}^n \to \{-1, 1\}"/> be an intersection of <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="k"/> halfspaces. Then</p>
<p style="text-align: center;"><img alt="\mathbb{NS}_\varepsilon (f) = O(\sqrt{\varepsilon \log k})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BNS%7D_%5Cvarepsilon+%28f%29+%3D+O%28%5Csqrt%7B%5Cvarepsilon+%5Clog+k%7D%29&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathbb{NS}_\varepsilon (f) = O(\sqrt{\varepsilon \log k})"/>.</p>
<p>Finally, this gives us good learning algorithms for intersections of halfspaces.</p>
<p>The paper is remarkable; there had been previous work by Nazarov (see [4]) proving optimal bounds for sensitivities of intersections of halfspaces in the Gaussian setting, which is a more restricted setting than the Boolean setting (intuitively because we can simulate Gaussians by sums of independent Boolean variables), and there were some results in the Boolean setting, but they were fairly suboptimal [5]. Furthermore, all these proofs were scary: they were incredibly involved, used powerful machinery from real analysis, drew heavily on the properties of halfspaces, etc. On the other hand, Daniel’s proof of his main result (which I <em>would</em> say builds on past work in the area, except it doesn’t use anything besides elementary facts), well, I think Erdos would say this proof is from “the Book”.</p>
<p>[1] <a href="http://arxiv.org/abs/1309.2987">The Average Sensitivity of an Intersection of Half Spaces</a>, Kane, 2014.<br/>
[2] <a href="http://analysisofbooleanfunctions.org/">Analysis of Boolean Functions</a>, O’Donnell, 2014.<br/>
[3] <a href="http://arxiv.org/abs/math/0412377">Noise Stability of Weighted Majority</a>, Peres, 2004.<br/>
[4] <a href="http://link.springer.com/chapter/10.1007%2F978-3-540-36428-3_15">On the maximal perimeter of a convex set in <img alt="\mathbb{R}^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&amp;bg=ffffff&amp;fg=404040&amp;s=0" title="\mathbb{R}^n"/> with respect to a Gaussian measure</a>, Nazarov, 2003.<br/>
<span style="line-height: 1.6;">[5] </span><a href="http://arxiv.org/abs/0912.4884" style="line-height: 1.6;">An Invariance Principle for Polytopes</a><span style="line-height: 1.6;">, Harsha, Klivans, Meka, 2010.</span></p></div></content><updated planet:format="July 11, 2014 01:54 PM">2014-07-11T13:54:35Z</updated><published planet:format="July 11, 2014 01:54 PM">2014-07-11T13:54:35Z</published><category term="complexity"/><category term="conferences"/><category term="learning theory"/><category term="STOC"/><author><name>Gautam</name></author><source><id>https://mittheory.wordpress.com</id><logo>https://s0.wp.com/i/buttonw-com.png</logo><link href="https://mittheory.wordpress.com/feed/" rel="self" type="application/atom+xml"/><link href="https://mittheory.wordpress.com" rel="alternate" type="text/html"/><link href="https://mittheory.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/><link href="https://mittheory.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/><subtitle>A student blog of MIT CSAIL Theory of Computation Group</subtitle><title>Not so Great Ideas in Theoretical Computer Science</title><updated planet:format="February 28, 2020 03:25 AM">2020-02-28T03:25:10Z</updated><planet:module>toc</planet:module><planet:format>atom10</planet:format><planet:http_last_modified>Fri, 06 Sep 2019 10:00:22 GMT</planet:http_last_modified><planet:bozo>false</planet:bozo><planet:items_per_page>40</planet:items_per_page><planet:css-id>mit-csail-student-blog</planet:css-id><planet:face>csail.png</planet:face><planet:name>MIT CSAIL student blog</planet:name><planet:http_location>https://mittheory.wordpress.com/feed/</planet:http_location><planet:http_status>301</planet:http_status></source></entry>