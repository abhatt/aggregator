<?xml version="1.0" encoding="utf-8"?><entry xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/"><id>tag:blogger.com,1999:blog-25562705.post-2463721255313980949</id><link href="http://aaronsadventures.blogspot.com/feeds/2463721255313980949/comments/default" rel="replies" type="application/atom+xml"/><link href="http://www.blogger.com/comment.g?blogID=25562705&amp;postID=2463721255313980949" rel="replies" type="text/html"/><link href="http://www.blogger.com/feeds/25562705/posts/default/2463721255313980949" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/25562705/posts/default/2463721255313980949" rel="self" type="application/atom+xml"/><link href="http://aaronsadventures.blogspot.com/2014/03/lecture-9-purchasing-private-data-from.html" rel="alternate" type="text/html"/><title>Lecture 9 -- Purchasing Private Data from Privacy Sensitive Individuals</title><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Yesterday in our<a href="http://www.cis.upenn.edu/~aaroth/courses/gametheoryprivacyS14.html"> privacy and mechanism design course</a>, we were fortunate to have a guest lecture by <a href="http://www.cs.princeton.edu/~dxiao/">David Xiao</a>. David told us his exciting recent paper, with Kobbi Nissim and Salil Vadhan, <a href="http://arxiv-web3.library.cornell.edu/abs/1401.4092">Redrawing the Boundaries on Purchasing Private Data from Privacy-Sensitive Individuals</a>.<br/><br/>Consider the following scenario: An analyst wishes to conduct some medical study about an underlying population, but needs to obtain permission from each individual whose data he uses. On the one hand, he needs to buy data from a representative sample of the population so that his study is <i>accurate</i>. On the other hand, he needs to compensate individuals for their privacy costs, and would like to come up with a payment scheme that incentivizes them to report their true privacy costs, rather than inflating them for selfish gain. Finally, he wants the mechanism to be <i>individually rational</i>: that no rational agent should obtain negative utility by interacting with the analyst.<br/><br/>Because individual's costs for privacy are a function of the method by which their reports are used to compute the outcome of the mechanism, rather than just a function of the outcome itself, this takes us outside of a standard mechanism design setting. What makes the problem tricky is that individual's costs for privacy could quite plausibly be correlated with their private data. Suppose the analyst wishes to estimate the fraction of people in some population who have syphilis. It is reasonable to expect that syphilitics will on the whole want to be compensated more than healthy individuals for a loss of privacy. But this means that even computations on agents reported costs for privacy (and independent of agent's supposedly private data) can lead to privacy loss for those agents, and so must be compensated.<br/><br/>Some years ago <a href="http://www.sciencedirect.com/science/article/pii/S0899825613000961">Arpita Ghosh and I studied this problem</a>, and showed an impossibility result when making some (unreasonably) strong assumptions. One might have hoped that our result could be circumvented with one of several tweaks to the model. But no. David and his coauthors extend this impossibility result to have much wider applicability, making fewer assumptions on what the analyst is able to observe, and far fewer assumptions about the form of the privacy loss function of the agents. Their result is quite robust: Under extremely general circumstances, no truthful individually rational mechanism which makes finite payments can distinguish between two populations, in one of which everyone has syphilis, and in the other of which nobody does. This result says that no mechanism can simultaneously enjoy truthfulness, individual rationality, and non-trivial accuracy properties, and so without drastically relaxing the model of how people might value privacy, you must always give up on one of these.<br/><br/>They do propose one such relaxation, which seems to reduce to something like the assumption that contracting syphilis can only ever cause your costs for privacy to increase, never to decrease. But this is probably not the last word. I think that convincing answers for what to do in the face of their impressive impossibility result are still to be proposed, and is a really interesting question.</div><div class="commentbar"><p/><span class="commentbutton" href="http://aaronsadventures.blogspot.com/feeds/2463721255313980949/comments/default"/><a href="http://aaronsadventures.blogspot.com/feeds/2463721255313980949/comments/default"><img class="commenticon" src="/images/feed-icon.png"/> Subscribe to comments</a><![CDATA[  | ]]><a href="http://www.blogger.com/comment.g?blogID=25562705&amp;postID=2463721255313980949"><img class="commenticon" src="/images/post-icon.png"/> Post a comment</a></div></content><updated planet:format="March 29, 2014 02:01 PM">2014-03-29T14:01:00Z</updated><published planet:format="March 29, 2014 02:01 PM">2014-03-29T14:01:00Z</published><author><name>Aaron Roth</name><email>noreply@blogger.com</email><uri>https://plus.google.com/111805394598997130229</uri></author><source><id>tag:blogger.com,1999:blog-25562705</id><category term="game theory"/><category term="news"/><author><name>Aaron Roth</name><email>noreply@blogger.com</email></author><link href="http://aaronsadventures.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/25562705/posts/default" rel="self" type="application/atom+xml"/><link href="http://aaronsadventures.blogspot.com/" rel="alternate" type="text/html"/><link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/><link href="http://www.blogger.com/feeds/25562705/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/><title>Adventures in Computation</title><updated planet:format="November 09, 2018 05:02 AM">2018-11-09T05:02:47Z</updated><planet:module>toc</planet:module><planet:format>atom10</planet:format><planet:http_etag>W/&quot;dac885350a61ee97f590d9e41b38f0bb2cb440449fe9b0bb2377a0df0747a737&quot;</planet:http_etag><planet:http_last_modified>Fri, 09 Nov 2018 05:02:47 GMT</planet:http_last_modified><planet:bozo>false</planet:bozo><planet:items_per_page>40</planet:items_per_page><planet:css-id>aaron-roth</planet:css-id><planet:face>roth.jpeg</planet:face><planet:name>Aaron Roth</planet:name><planet:http_status>200</planet:http_status></source></entry>
