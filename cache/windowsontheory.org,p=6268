<?xml version="1.0" encoding="utf-8"?><entry xml:lang="en" xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/"><id>http://windowsontheory.org/?p=6268</id><link href="https://windowsontheory.org/2018/10/20/belief-propagation-and-the-stochastic-block-model/" rel="alternate" type="text/html"/><title>Belief Propagation and the Stochastic Block Model</title><summary>[Guest post by Thomas Orton who presented a lecture on this in our  physics and computation seminar –Boaz] Introduction This blog post is a continuation of the CS229R lecture series. Last week, we saw how certain computational problems like 3SAT exhibit a thresholding behavior, similar to a phase transition in a physical system. In this post, […]<div class="commentbar"><p/></div></summary><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>[Guest post by Thomas Orton who presented a lecture on this in our  <a href="https://www.boazbarak.org/fall18seminar/">physics and computation seminar</a> –Boaz]</em></p>
<h2><strong>Introduction</strong></h2>
<p>This blog post is a continuation of the <a href="https://windowsontheory.org/category/physics/">CS229R lecture series</a>. <a href="https://windowsontheory.org/2018/09/15/statistical-physics-an-introduction-in-two-parts/">Last week</a>, we saw how certain computational problems like 3SAT exhibit a thresholding behavior, similar to a phase transition in a physical system. In this post, we’ll continue to look at this phenomenon by exploring a heuristic method, belief propagation (and the cavity method), which has been used to make hardness conjectures, and also has thresholding properties. In particular, we’ll start by looking at belief propagation for approximate inference on sparse graphs as a purely computational problem. After doing this, we’ll switch perspectives and see belief propagation motivated in terms of Gibbs free energy minimization for physical systems. With these two perspectives in mind, we’ll then try to use belief propagation to do inference on the the stochastic block model. We’ll see some heuristic techniques for determining when BP succeeds and fails in inference, as well as some numerical simulation results of belief propagation for this problem. Lastly, we’ll talk about where this all fits into what is currently known about efficient algorithms and information theoretic barriers for the stochastic block model.</p>
<h1/>
<h2>(1) Graphical Models and Belief Propagation</h2>
<h3>(1.0) Motivation</h3>
<p>Suppose someone gives you a probabilistic model on <img alt="\vec{x}=(x_1,...,x_n) \in \chi^{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bx%7D%3D%28x_1%2C...%2Cx_n%29+%5Cin+%5Cchi%5E%7BN%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\vec{x}=(x_1,...,x_n) \in \chi^{N}"/> (think of <img alt="\chi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cchi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\chi"/> as a discrete set) which can be decomposed in a special way, say</p>
<p><img alt="P(\vec{x})\propto \prod_{a \in F} f_{a}(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29%5Cpropto+%5Cprod_%7Ba+%5Cin+F%7D+f_%7Ba%7D%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})\propto \prod_{a \in F} f_{a}(\vec{x})"/></p>
<p>where each <img alt="f_{a}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Ba%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_{a}"/> only depends on the variables <img alt="V_{a}" class="latex" src="https://s0.wp.com/latex.php?latex=V_%7Ba%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="V_{a}"/>. Recall from last week that we can express constraint satisfaction problems in these kinds of models, where each <img alt="f_{a}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Ba%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_{a}"/> is associated with a particular constraint. For example, given a 3SAT formula <img alt="\phi=\land_{a} c_{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi%3D%5Cland_%7Ba%7D+c_%7Ba%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi=\land_{a} c_{a}"/>, we can let <img alt="f_{a}(\vec{x})=1" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Ba%7D%28%5Cvec%7Bx%7D%29%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_{a}(\vec{x})=1"/> if <img alt="c_{a}(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=c_%7Ba%7D%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_{a}(\vec{x})"/> is satisfied, and 0 otherwise. Then each <img alt="f_{a}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Ba%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_{a}"/> only depends on 3 variables, and <img alt="P(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})"/> only has support on satisfying assignments of <img alt="\phi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi"/>.</p>
<p> </p>
<p>A central problem in computer science is trying to find satisfying assignments to constraint satisfaction problems, i.e. finding values <img alt="\vec{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bx%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\vec{x}"/> in the support of <img alt="P(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})"/>. Suppose that we knew that the value of <img alt="P(x_1=1)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28x_1%3D1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(x_1=1)"/> were <img alt="&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=%3E0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="&gt;0"/>. Then we would know that there exists some satisfying assignment where <img alt="x_1=1" class="latex" src="https://s0.wp.com/latex.php?latex=x_1%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_1=1"/>. Using this knowledge, we could recursively try to find <img alt="\vec{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bx%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\vec{x}"/>‘s in the support of <img alt="P(1,x_2,...,x_n)" class="latex" src="https://s0.wp.com/latex.php?latex=P%281%2Cx_2%2C...%2Cx_n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(1,x_2,...,x_n)"/>, and iteratively come up with a satisfying assignment to our constraint satisfaction problem. In fact, we could even sample uniformally from the distribution as follows: randomly assign <img alt="x_1" class="latex" src="https://s0.wp.com/latex.php?latex=x_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_1"/> to <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1"/> with probability <img alt="P(x_1=1)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28x_1%3D1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(x_1=1)"/>, and assign it to <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0"/> otherwise. Now iteratively sample from <img alt="P(x_2|x_1)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28x_2%7Cx_1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(x_2|x_1)"/> for the model where <img alt="x_1" class="latex" src="https://s0.wp.com/latex.php?latex=x_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_1"/> is fixed to the value we assigned to it, and repeat until we’ve assigned values to all of the <img alt="\{x_i\}_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bx_i%5C%7D_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{x_i\}_{i}"/>. A natural question is therefore the following: When can we try to efficiently compute the marginals</p>
<p><img alt="P(x_i):=\sum_{\vec{x}-x_i} P(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28x_i%29%3A%3D%5Csum_%7B%5Cvec%7Bx%7D-x_i%7D+P%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(x_i):=\sum_{\vec{x}-x_i} P(\vec{x})"/></p>
<p>for each <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>?</p>
<p> </p>
<p>A well known efficient algorithm for this problem exists when the corresponding graphical model of <img alt="P(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})"/> (more in this in the next section) is a tree. Even though belief propagation is only guaranteed to work exactly for trees, we might hope that if our factor graph is “tree like”, then BP will still give a useful answer. We might even go further than this, and try to analyze exactly when BP fails for a random constraint satisfaction problem. For example, you can do this for k-SAT when <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> is large, and then learn something about the solution threshold for k-SAT. It therefore might be natural to try and study when BP succeeds and fails for different kinds of problems.</p>
<h3>(1.1) Deriving BP</h3>
<p>We will start by making two simplifying assumptions on our model <img alt="P(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})"/>.</p>
<p> </p>
<p>First, we will assume that <img alt="P(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})"/> can be written in the form <img alt="P(\vec{x})\propto \prod_{(i,j) \in E} f_{i,j}(x_i,x_j) \prod_{i \in [n]} f_i(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29%5Cpropto+%5Cprod_%7B%28i%2Cj%29+%5Cin+E%7D+f_%7Bi%2Cj%7D%28x_i%2Cx_j%29+%5Cprod_%7Bi+%5Cin+%5Bn%5D%7D+f_i%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})\propto \prod_{(i,j) \in E} f_{i,j}(x_i,x_j) \prod_{i \in [n]} f_i(x_i)"/> for some functions <img alt="\{f_{i,j}\}_{i,j}, \{f_i(x_i)\}_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bf_%7Bi%2Cj%7D%5C%7D_%7Bi%2Cj%7D%2C+%5C%7Bf_i%28x_i%29%5C%7D_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{f_{i,j}\}_{i,j}, \{f_i(x_i)\}_{i}"/> and some “edge set” <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E"/> (where the edges are undirected). In other words, we will only consider pairwise constraints. We will see later that this naturally corresponds to a physical interpretation, where each of the “particles” <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> interact with each other via pairwise forces. Belief propagation actually still works without this assumption (which is why we can use it to analyze <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-SAT for <img alt="k&gt;2" class="latex" src="https://s0.wp.com/latex.php?latex=k%3E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k&gt;2"/>), but the pairwise case is all we need for the stochastic block model.</p>
<p> </p>
<p>For the second assumption, notice that there is a natural correspondence between <img alt="P(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})"/> and the graphical model <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> on <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> vertices, where <img alt="(i,j)" class="latex" src="https://s0.wp.com/latex.php?latex=%28i%2Cj%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(i,j)"/> forms an edge in <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> iff <img alt="(i,j) \in E" class="latex" src="https://s0.wp.com/latex.php?latex=%28i%2Cj%29+%5Cin+E&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(i,j) \in E"/>. In order words, edges <img alt="(i,j)" class="latex" src="https://s0.wp.com/latex.php?latex=%28i%2Cj%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(i,j)"/> in <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> correspond to factors of the form <img alt="f_{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_{i,j}"/> in <img alt="P(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})"/>, and vertices in <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> correspond to variables in <img alt="\vec{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bx%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\vec{x}"/>. Our second assumption is that the graphical model <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> is a tree.</p>
<p><img alt="graphical_model_fig_1" class="alignnone size-full wp-image-6270" src="https://windowsontheory.files.wordpress.com/2018/10/graphical_model_fig_1.png?w=600"/></p>
<p>Now, suppose we’re given such a tree <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> which represents our probabilistic model. How to we compute the marginals? Generally speaking, when computer scientists see trees, they begin to get very excited [reference]. “I know! Let’s use recursion!” shouts the student in <a href="http://sites.fas.harvard.edu/~cs124/cs124/">CS124</a>, their heart rate noticeably rising. Imagine that we arbitrarily rooted our tree at vertex <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>. Perhaps, if we could somehow compute the marginals of the children of <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>, we could somehow stitch them together to compute the marginal <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>. In order words, we should think about computing the marginals of roots of subtrees in our graphical model. A quick check shows that the base case is easy: suppose we’re given a graphical model which is a tree consisting of a single node <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>. This corresponds to some PDF <img alt="P(\vec{x})=P(x_i) \propto f_{i}(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29%3DP%28x_i%29+%5Cpropto+f_%7Bi%7D%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})=P(x_i) \propto f_{i}(x_i)"/>. So to compute <img alt="P(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(x_i)"/>, all we have to do is compute the marginalizing constant <img alt="Z=\sum_{x_i \in \chi} f_{i}(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=Z%3D%5Csum_%7Bx_i+%5Cin+%5Cchi%7D+f_%7Bi%7D%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z=\sum_{x_i \in \chi} f_{i}(x_i)"/>, and then we have <img alt="P(x_i)=\frac{1}{Z}f(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28x_i%29%3D%5Cfrac%7B1%7D%7BZ%7Df%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(x_i)=\frac{1}{Z}f(x_i)"/>. With the base case out of the way, let’s try to solve the induction step: given a graphical model which is a tree rooted at <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>, and where we’re given the marginals of the subtrees rooted at the children of <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>, how do we compute the marginal of the tree rooted at <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>? Take a look at figure 2 to see what this looks like graphically. To formalize the induction step, we’ll define some notation that will also be useful to us later on. The main pieces of notation are <img alt="T^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=T%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T^{i \rightarrow j}"/>, which is the subtree rooted at <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> with parent <img alt="x_j" class="latex" src="https://s0.wp.com/latex.php?latex=x_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_j"/>, and the “messages” <img alt="\psi^{k \rightarrow i}_{x_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bk+%5Crightarrow+i%7D_%7Bx_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{k \rightarrow i}_{x_i}"/>, which can be thought of as information which is passed from the child subtrees of <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> to the vertex <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> in order to compute the marginals correctly.</p>
<ol>
<li>We let <img alt="\partial i" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpartial+i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\partial i"/> denote the neighbours of vertex <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/> in <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/>. In general, I’ll interchangeably switch between vertex <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/> and the variable <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> represented by vertex <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>.</li>
<li>We define <img alt="T^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=T%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T^{i \rightarrow j}"/> to be the subtree of <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> rooted at <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>, where <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>‘s parent is <img alt="j" class="latex" src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="j"/>. We need to specify <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>‘s parent in order to give an orientation to our tree (so that we know in which direction we need to do recursion down the tree). Likewise, we let <img alt="V_{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=V_%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="V_{i \rightarrow j}"/> be the set of vertices/variables which occur in subtree <img alt="T^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=T%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T^{i \rightarrow j}"/>.</li>
<li>We define the function <img alt="T^{i \rightarrow j}(V_{i \rightarrow j})" class="latex" src="https://s0.wp.com/latex.php?latex=T%5E%7Bi+%5Crightarrow+j%7D%28V_%7Bi+%5Crightarrow+j%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T^{i \rightarrow j}(V_{i \rightarrow j})"/>, which is a function of the variables in <img alt="V_{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=V_%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="V_{i \rightarrow j}"/>, to be equal to the product of <img alt="T^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=T%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T^{i \rightarrow j}"/>‘s edges and vertices. Specifically, <img alt="T^{i \rightarrow j}(V_{i \rightarrow j})=\prod_{(a,b) \in E'} f_{a,b}(x_a,x_b) \prod_{a \in V_{i \rightarrow j}} f_{a}(x_a)" class="latex" src="https://s0.wp.com/latex.php?latex=T%5E%7Bi+%5Crightarrow+j%7D%28V_%7Bi+%5Crightarrow+j%7D%29%3D%5Cprod_%7B%28a%2Cb%29+%5Cin+E%27%7D+f_%7Ba%2Cb%7D%28x_a%2Cx_b%29+%5Cprod_%7Ba+%5Cin+V_%7Bi+%5Crightarrow+j%7D%7D+f_%7Ba%7D%28x_a%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T^{i \rightarrow j}(V_{i \rightarrow j})=\prod_{(a,b) \in E'} f_{a,b}(x_a,x_b) \prod_{a \in V_{i \rightarrow j}} f_{a}(x_a)"/>, where <img alt="E'" class="latex" src="https://s0.wp.com/latex.php?latex=E%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E'"/> is the set of edges which occur in subtree <img alt="T^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=T%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T^{i \rightarrow j}"/>. We can think of <img alt="T^{i \rightarrow j}(V_{i \rightarrow j})" class="latex" src="https://s0.wp.com/latex.php?latex=T%5E%7Bi+%5Crightarrow+j%7D%28V_%7Bi+%5Crightarrow+j%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T^{i \rightarrow j}(V_{i \rightarrow j})"/> as being the pdf (up to normalizing constant) of the “model” of the subtree <img alt="T^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=T%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T^{i \rightarrow j}"/>.</li>
<li>Lastly, we define <img alt="\psi^{i \rightarrow j}_{x_i}:=\frac{1}{Z^{i \rightarrow j}} \sum_{V_{i \rightarrow j}-x_i} T(V_{i \rightarrow j})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Bx_i%7D%3A%3D%5Cfrac%7B1%7D%7BZ%5E%7Bi+%5Crightarrow+j%7D%7D+%5Csum_%7BV_%7Bi+%5Crightarrow+j%7D-x_i%7D+T%28V_%7Bi+%5Crightarrow+j%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i \rightarrow j}_{x_i}:=\frac{1}{Z^{i \rightarrow j}} \sum_{V_{i \rightarrow j}-x_i} T(V_{i \rightarrow j})"/>, where <img alt="Z^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=Z%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z^{i \rightarrow j}"/> is a normalizing constant chosen such that <img alt="\sum_{x_i \in \chi} \psi^{i \rightarrow j}_{x_i}=1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bx_i+%5Cin+%5Cchi%7D+%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Bx_i%7D%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sum_{x_i \in \chi} \psi^{i \rightarrow j}_{x_i}=1"/>. In particular, <img alt="\psi^{i \rightarrow j}_{x_i}: \chi \rightarrow \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Bx_i%7D%3A+%5Cchi+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i \rightarrow j}_{x_i}: \chi \rightarrow \mathbb{R}"/> is a function defined for each possible value of <img alt="x_i \in \chi" class="latex" src="https://s0.wp.com/latex.php?latex=x_i+%5Cin+%5Cchi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i \in \chi"/>. We can interpret <img alt="\psi^{i \rightarrow j}_{x_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Bx_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i \rightarrow j}_{x_i}"/> in two ways: firstly, it is the marginal of the root of the subtree <img alt="T^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=T%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T^{i \rightarrow j}"/>. Secondly, we can think of it as a “message” from vertex <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/> to vertex <img alt="j" class="latex" src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="j"/>. We’ll see why this is a valid interpretation shortly.</li>
</ol>
<p><img alt="graphical_model_recursion" class="alignnone  wp-image-6269" height="465" src="https://windowsontheory.files.wordpress.com/2018/10/graphical_model_recursion.png?w=507&amp;h=465" width="507"/></p>
<p>Phew! That was a lot of notation. Now that we have that out of the way, let’s see how we can express the marginal of the root of a tree as a function of the marginals of its subtrees. Suppose we’re considering the subtree <img alt="T^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=T%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T^{i \rightarrow j}"/>, so that vertex <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/> has children <img alt="(\partial i)-j" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Cpartial+i%29-j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(\partial i)-j"/>. Then we can compute the marginal <img alt="\psi^{i \rightarrow j}_{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Br%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i \rightarrow j}_{r}"/> directly:</p>
<p><img alt="eqs_1" class="alignnone  wp-image-6279" height="238" src="https://windowsontheory.files.wordpress.com/2018/10/eqs_1.png?w=746&amp;h=238" width="746"/></p>
<p>The non-obvious step in the above is that we’re able to switch around summations and products: we’re able to do this because each of the trees are functions on disjoint sets of variables. So we’re able to express <img alt="\psi^{i \rightarrow j}_{x_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Bx_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i \rightarrow j}_{x_i}"/> as a function of the children values <img alt="\{\psi^{k \rightarrow i}_{x_k}\}_{k \in \partial i-j}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpsi%5E%7Bk+%5Crightarrow+i%7D_%7Bx_k%7D%5C%7D_%7Bk+%5Cin+%5Cpartial+i-j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{\psi^{k \rightarrow i}_{x_k}\}_{k \in \partial i-j}"/>. Looking at the update formula we have derived, we can now see why the <img alt="\{\psi^{k \rightarrow i}_{x_k}\}_{k \in \partial i-j}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpsi%5E%7Bk+%5Crightarrow+i%7D_%7Bx_k%7D%5C%7D_%7Bk+%5Cin+%5Cpartial+i-j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{\psi^{k \rightarrow i}_{x_k}\}_{k \in \partial i-j}"/> are called “messages” to vertex <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>: they send information about the child subtrees to their parent <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>.</p>
<p>The above discussion is a purely algebraic way of deriving belief propagation. A more intuitive way to get this result is as follows: imagine fixing the value of <img alt="x_i=a" class="latex" src="https://s0.wp.com/latex.php?latex=x_i%3Da&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i=a"/> in the the subtree <img alt="T^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=T%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T^{i \rightarrow j}"/>, and then drawing from each of the marginals of the children of <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> conditioned on the value <img alt="x_i=a" class="latex" src="https://s0.wp.com/latex.php?latex=x_i%3Da&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i=a"/>. We can consider the marginals of each of the children independently, because the children are independent of each other when conditioned on the value of <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>. Converting words to equations, this means that if <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> has children <img alt="x_{k_{1}},...,x_{k_{d}}" class="latex" src="https://s0.wp.com/latex.php?latex=x_%7Bk_%7B1%7D%7D%2C...%2Cx_%7Bk_%7Bd%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_{k_{1}},...,x_{k_{d}}"/>, then the marginal probability of <img alt="(x_i,x_{k_{1}},...,x_{k_{d}})" class="latex" src="https://s0.wp.com/latex.php?latex=%28x_i%2Cx_%7Bk_%7B1%7D%7D%2C...%2Cx_%7Bk_%7Bd%7D%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(x_i,x_{k_{1}},...,x_{k_{d}})"/> in the subtree <img alt="T^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=T%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T^{i \rightarrow j}"/> is proportional to <img alt="f_{i}(x_i) \prod_{\substack{k:(i,k) \in E, \\ k \not = j}} \psi^{k \rightarrow i}_{x_k} f_{i,k}(x_i,x_k)" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bi%7D%28x_i%29+%5Cprod_%7B%5Csubstack%7Bk%3A%28i%2Ck%29+%5Cin+E%2C+%5C%5C+k+%5Cnot+%3D+j%7D%7D+%5Cpsi%5E%7Bk+%5Crightarrow+i%7D_%7Bx_k%7D+f_%7Bi%2Ck%7D%28x_i%2Cx_k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_{i}(x_i) \prod_{\substack{k:(i,k) \in E, \\ k \not = j}} \psi^{k \rightarrow i}_{x_k} f_{i,k}(x_i,x_k)"/>. We can then write</p>
<p><img alt="\psi_{x_i}^{i \rightarrow j} \propto \sum_{(\partial i)-j} f_{i}(x_i) \prod_{\substack{k:(i,k) \in E, \\ k \not = j}} \psi^{k \rightarrow i}_{x_k} f_{i,k}(x_i,x_k)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7Bx_i%7D%5E%7Bi+%5Crightarrow+j%7D+%5Cpropto+%5Csum_%7B%28%5Cpartial+i%29-j%7D+f_%7Bi%7D%28x_i%29+%5Cprod_%7B%5Csubstack%7Bk%3A%28i%2Ck%29+%5Cin+E%2C+%5C%5C+k+%5Cnot+%3D+j%7D%7D+%5Cpsi%5E%7Bk+%5Crightarrow+i%7D_%7Bx_k%7D+f_%7Bi%2Ck%7D%28x_i%2Cx_k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi_{x_i}^{i \rightarrow j} \propto \sum_{(\partial i)-j} f_{i}(x_i) \prod_{\substack{k:(i,k) \in E, \\ k \not = j}} \psi^{k \rightarrow i}_{x_k} f_{i,k}(x_i,x_k)"/></p>
<p><img alt="= f_{i}(x_i)\prod_{\substack{k:(i,k) \in E, \\ k \not = j}} \sum_{x_k \in \chi} \psi^{k \rightarrow i}_{x_k} f_{i,k}(x_i,x_k)" class="latex" src="https://s0.wp.com/latex.php?latex=%3D+f_%7Bi%7D%28x_i%29%5Cprod_%7B%5Csubstack%7Bk%3A%28i%2Ck%29+%5Cin+E%2C+%5C%5C+k+%5Cnot+%3D+j%7D%7D+%5Csum_%7Bx_k+%5Cin+%5Cchi%7D+%5Cpsi%5E%7Bk+%5Crightarrow+i%7D_%7Bx_k%7D+f_%7Bi%2Ck%7D%28x_i%2Cx_k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="= f_{i}(x_i)\prod_{\substack{k:(i,k) \in E, \\ k \not = j}} \sum_{x_k \in \chi} \psi^{k \rightarrow i}_{x_k} f_{i,k}(x_i,x_k)"/></p>
<p>And we get back what we had before. We’ll call this last equation our “update” or “message passing” equation. The key assumption we used was that if we condition on <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>, then the children of <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> are independent. It’s useful to keep this assumption in mind when thinking about how BP behaves on more general graphs.</p>
<p>A similar calculation yields that we can calculate the marginal of our original probability distribution <img alt="\psi_{x_i}^{i}:=P(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7Bx_i%7D%5E%7Bi%7D%3A%3DP%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi_{x_i}^{i}:=P(x_i)"/> as the marginal of the subtree with no parent, i.e.</p>
<p><img alt="\psi_{x_i}^{i} \propto f_{i}(x_i)\prod_{k:(i,k) \in E} \sum_{x_k \in \chi} \psi^{k \rightarrow i}_{x_k} f_{i,k}(x_i,x_k)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7Bx_i%7D%5E%7Bi%7D+%5Cpropto+f_%7Bi%7D%28x_i%29%5Cprod_%7Bk%3A%28i%2Ck%29+%5Cin+E%7D+%5Csum_%7Bx_k+%5Cin+%5Cchi%7D+%5Cpsi%5E%7Bk+%5Crightarrow+i%7D_%7Bx_k%7D+f_%7Bi%2Ck%7D%28x_i%2Cx_k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi_{x_i}^{i} \propto f_{i}(x_i)\prod_{k:(i,k) \in E} \sum_{x_k \in \chi} \psi^{k \rightarrow i}_{x_k} f_{i,k}(x_i,x_k)"/></p>
<p> </p>
<p>Great! So now we have an algorithm for computing marginals: recursively compute <img alt="\psi^{i \rightarrow j}_{x_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Bx_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i \rightarrow j}_{x_i}"/> for each <img alt="(i,j) \in E" class="latex" src="https://s0.wp.com/latex.php?latex=%28i%2Cj%29+%5Cin+E&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(i,j) \in E"/> in a dynamic programming fashion with the “message passing” equations we have just derived. Then, compute <img alt="\psi_{x_i}^{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7Bx_i%7D%5E%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi_{x_i}^{i}"/> for each <img alt="i \in [n]" class="latex" src="https://s0.wp.com/latex.php?latex=i+%5Cin+%5Bn%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i \in [n]"/>. If the diameter of our tree <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> is <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/>, then the recursion depth of our algorithm is at most <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/>.</p>
<p>However, instead of computing every <img alt="\psi^{i \rightarrow j}_{x_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Bx_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i \rightarrow j}_{x_i}"/> neatly with recursion, we might try something else: let’s instead randomly initialize each <img alt="\psi^{i \rightarrow j}_{x_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Bx_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i \rightarrow j}_{x_i}"/> <img alt="(x_i \in \chi)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x_i+%5Cin+%5Cchi%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(x_i \in \chi)"/> with anything we want. Then, let’s update each <img alt="\psi^{i \rightarrow j}_{x_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Bx_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i \rightarrow j}_{x_i}"/> in parallel with our update equations. We will keep doing this in successive steps until each <img alt="\psi^{i \rightarrow j}_{x_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Bx_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i \rightarrow j}_{x_i}"/> has converged to a fixed value. By looking at belief propagation as a recursive algorithm, it’s easy to see that all of the <img alt="\psi^{i \rightarrow j}_{x_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Bx_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i \rightarrow j}_{x_i}"/>‘s will have their correct values after at most <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/> steps. This is because (after arbitrarily rooting our tree at any vertex) the leaves of our recursion will initialize to the correct value after 1 step. After two steps, the parents of the leaves will be updated correctly as functions of the leaves, and so they will have the correct values as well. Specifically:<br/>
<strong>Proposition:</strong> Suppose we initialize messages <img alt="\psi_{x_i}^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7Bx_i%7D%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi_{x_i}^{i \rightarrow j}"/> arbitrarily, and update them in parallel according to our update equations. If <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> has diameter <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/>, then after <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/> steps each <img alt="\psi_{r}^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7Br%7D%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi_{r}^{i \rightarrow j}"/> converges, and we recover the correct marginals.</p>
<p>Why would anyone want to do things in this way? In particular, by computing everything in parallel in steps instead of recursively, we’re computing a lot of “garbage” updates which we never use. However, the advantage of doing things in this way is that this procedure is now well defined for general graphs. In particular, suppose <img alt="P(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})"/> violated assumption (2), so that the corresponding graph were not a tree. Then we could still try to compute the messages <img alt="\psi_{x_i}^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7Bx_i%7D%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi_{x_i}^{i \rightarrow j}"/> with parallel updates. We are also able to do this in a local “message passing” kind of way, which some people may find physically intuitive. Maybe if we’re lucky, the messages will converge after a reasonable amount of iterations. Maybe if we’re even luckier, they will converge to something which gives us information about the marginals <img alt="\{P(x_i)\}_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7BP%28x_i%29%5C%7D_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{P(x_i)\}_{i}"/>. In fact, we’ll see that just such a thing happens in the stochastic block model. More on that later. For now, let’s shift gears and look at belief propagation from a physics perspective.</p>
<p> </p>
<h2>(2) Free Energy Minimization</h2>
<h3>(2.0) Potts model (Refresh of last week)</h3>
<p>We’ve just seen a statistical/algorithmic view of how to compute marginals in a graphical model. It turns out that there’s also a physical way to think about this, which leads to a qualitatively similar algorithm. Recall from last week that another interpretation of a pairwise factor-able PDF is that of particles interacting with each other via pairwise forces. In particular, we can imagine each particle <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> interacting with <img alt="x_j" class="latex" src="https://s0.wp.com/latex.php?latex=x_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_j"/> via a force of strength</p>
<p><img alt="J_{(i,j)}(x_i,x_j)" class="latex" src="https://s0.wp.com/latex.php?latex=J_%7B%28i%2Cj%29%7D%28x_i%2Cx_j%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J_{(i,j)}(x_i,x_j)"/></p>
<p>and in addition, interacting with an external field</p>
<p><img alt="h_{i}(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=h_%7Bi%7D%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_{i}(x_i)"/></p>
<p>We imagine that each of our particles take values from a discrete set <img alt="\chi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cchi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\chi"/>. When <img alt="\chi=\{0,1\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cchi%3D%5C%7B0%2C1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\chi=\{0,1\}"/>, we recover the Ising model, and in general we have a Potts model. The energy function of this system is then</p>
<p><img alt="E[\vec{x}]=-\sum_{(i,j)}J_{(i,j)}(x_i,x_j)-\sum_{i}h_{i}(x)" class="latex" src="https://s0.wp.com/latex.php?latex=E%5B%5Cvec%7Bx%7D%5D%3D-%5Csum_%7B%28i%2Cj%29%7DJ_%7B%28i%2Cj%29%7D%28x_i%2Cx_j%29-%5Csum_%7Bi%7Dh_%7Bi%7D%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E[\vec{x}]=-\sum_{(i,j)}J_{(i,j)}(x_i,x_j)-\sum_{i}h_{i}(x)"/></p>
<p>with probability distribution given by</p>
<p><img alt="P(\vec{x})=\frac{1}{Z} e^{-E[\vec{x}]/T}" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29%3D%5Cfrac%7B1%7D%7BZ%7D+e%5E%7B-E%5B%5Cvec%7Bx%7D%5D%2FT%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})=\frac{1}{Z} e^{-E[\vec{x}]/T}"/></p>
<p>Now, for <img alt="\chi=\{0,1\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cchi%3D%5C%7B0%2C1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\chi=\{0,1\}"/>, computing the marginals <img alt="P(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(x_i)"/> corresponds to the equivalent physical problem of computing the “magnetizations” <img alt="P(x_i=1)-P(x_i=0)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28x_i%3D1%29-P%28x_i%3D0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(x_i=1)-P(x_i=0)"/>.</p>
<p>How does this setup relate to the previous section, where we thought about constraint satisfaction problems and probability distributions? If we could set <img alt="e^{-J_{(i,j)}(x_i,x_j)/T}=f_{i,j}(x_i,x_j)" class="latex" src="https://s0.wp.com/latex.php?latex=e%5E%7B-J_%7B%28i%2Cj%29%7D%28x_i%2Cx_j%29%2FT%7D%3Df_%7Bi%2Cj%7D%28x_i%2Cx_j%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e^{-J_{(i,j)}(x_i,x_j)/T}=f_{i,j}(x_i,x_j)"/> and <img alt="e^{-h_{i}(x_i)/T}=f_{i}(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=e%5E%7B-h_%7Bi%7D%28x_i%29%2FT%7D%3Df_%7Bi%7D%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e^{-h_{i}(x_i)/T}=f_{i}(x_i)"/>, we would recover exactly the probability distribution <img alt="P(\vec{x})=\prod_{i,j} f_{i,j}(\vec{x}) \prod_{i} f_{i}(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29%3D%5Cprod_%7Bi%2Cj%7D+f_%7Bi%2Cj%7D%28%5Cvec%7Bx%7D%29+%5Cprod_%7Bi%7D+f_%7Bi%7D%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})=\prod_{i,j} f_{i,j}(\vec{x}) \prod_{i} f_{i}(x_i)"/> from the previous section. From a constraint satisfaction perspective, if we set <img alt="J_{(i,j)}(x_i,x_j)=1" class="latex" src="https://s0.wp.com/latex.php?latex=J_%7B%28i%2Cj%29%7D%28x_i%2Cx_j%29%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J_{(i,j)}(x_i,x_j)=1"/> if constraint <img alt="(i,j)" class="latex" src="https://s0.wp.com/latex.php?latex=%28i%2Cj%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(i,j)"/> is satisfied and <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0"/> otherwise, then as <img alt="T \rightarrow 0" class="latex" src="https://s0.wp.com/latex.php?latex=T+%5Crightarrow+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T \rightarrow 0"/> (our system becomes colder), <img alt="P(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})"/>‘s probability mass becomes concentrated only on the satisfying assignments of the constraint satisfaction problem.</p>
<h3>(2.1) Free energy minimization</h3>
<p>We’re now going to try a different approach to computing the marginals: let’s define a distribution <img alt="b(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=b%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b(\vec{x})"/>, which we will hope to be a good approximation to <img alt="P(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})"/>. If you like, you can think about the marginal <img alt="b(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=b%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b(x_i)"/> as being the “belief” about the state of variable <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>. We can measure the “distance” between <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P"/> and <img alt="b" class="latex" src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b"/> by the KL divergence</p>
<p> </p>
<p><img alt="KL(b(\vec{x})||p(\vec{x})) =\sum_{\vec{x}}b(\vec{x})\ln\frac{b(\vec{x})}{P(\vec{x})}" class="latex" src="https://s0.wp.com/latex.php?latex=KL%28b%28%5Cvec%7Bx%7D%29%7C%7Cp%28%5Cvec%7Bx%7D%29%29+%3D%5Csum_%7B%5Cvec%7Bx%7D%7Db%28%5Cvec%7Bx%7D%29%5Cln%5Cfrac%7Bb%28%5Cvec%7Bx%7D%29%7D%7BP%28%5Cvec%7Bx%7D%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="KL(b(\vec{x})||p(\vec{x})) =\sum_{\vec{x}}b(\vec{x})\ln\frac{b(\vec{x})}{P(\vec{x})}"/></p>
<p><img alt="=\sum_{\vec{x}}b(\vec{x})E(\vec{x})+\sum_{\vec{x}}b(\vec{x})\ln b(\vec{x})+\ln Z" class="latex" src="https://s0.wp.com/latex.php?latex=%3D%5Csum_%7B%5Cvec%7Bx%7D%7Db%28%5Cvec%7Bx%7D%29E%28%5Cvec%7Bx%7D%29%2B%5Csum_%7B%5Cvec%7Bx%7D%7Db%28%5Cvec%7Bx%7D%29%5Cln+b%28%5Cvec%7Bx%7D%29%2B%5Cln+Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="=\sum_{\vec{x}}b(\vec{x})E(\vec{x})+\sum_{\vec{x}}b(\vec{x})\ln b(\vec{x})+\ln Z"/></p>
<p>which equals 0 iff the two distributions are equal. Let’s define the Gibbs free energy as<br/>
<img alt="G(b(\vec{x}))=\sum_{\vec{x}}b(\vec{x})E(\vec{x})+\sum_{\vec{x}}b(\vec{x})\ln b(\vec{x})=:U(b(\vec{x}))-S(b(\vec{x}))" class="latex" src="https://s0.wp.com/latex.php?latex=G%28b%28%5Cvec%7Bx%7D%29%29%3D%5Csum_%7B%5Cvec%7Bx%7D%7Db%28%5Cvec%7Bx%7D%29E%28%5Cvec%7Bx%7D%29%2B%5Csum_%7B%5Cvec%7Bx%7D%7Db%28%5Cvec%7Bx%7D%29%5Cln+b%28%5Cvec%7Bx%7D%29%3D%3AU%28b%28%5Cvec%7Bx%7D%29%29-S%28b%28%5Cvec%7Bx%7D%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G(b(\vec{x}))=\sum_{\vec{x}}b(\vec{x})E(\vec{x})+\sum_{\vec{x}}b(\vec{x})\ln b(\vec{x})=:U(b(\vec{x}))-S(b(\vec{x}))"/></p>
<p> </p>
<p>So the minimum value of <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> is <img alt="F=-\ln Z" class="latex" src="https://s0.wp.com/latex.php?latex=F%3D-%5Cln+Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="F=-\ln Z"/> which is called the Helmholz free energy, <img alt="U" class="latex" src="https://s0.wp.com/latex.php?latex=U&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="U"/> is the “average energy” and <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/> is the “entropy”.</p>
<p> </p>
<p>Now for the “free energy minimization part”. We want to choose <img alt="b" class="latex" src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b"/> to minimize <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/>, so that we can have that <img alt="b" class="latex" src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b"/> is a good approximation of <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P"/>. If this happens, then maybe we can hope to “read out” the marginals of <img alt="b" class="latex" src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b"/> directly. How do we do this in a way which makes it easy to “read out” the marginals? Here’s one idea: let’s try to write <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> as a function of only the marginals <img alt="b(x_i,x_j)" class="latex" src="https://s0.wp.com/latex.php?latex=b%28x_i%2Cx_j%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b(x_i,x_j)"/> and <img alt="b(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=b%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b(x_i)"/> of <img alt="b" class="latex" src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b"/>. If we could do this, then maybe we could try to minimize <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> by only optimizing over values for “variables” <img alt="\{b(x_i,x_j)\}_{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bb%28x_i%2Cx_j%29%5C%7D_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{b(x_i,x_j)\}_{i,j}"/> and <img alt="\{b(x_i)\}_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bb%28x_i%29%5C%7D_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{b(x_i)\}_{i}"/>. However, we need to remember that <img alt="\{b(x_i,x_j)\}_{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bb%28x_i%2Cx_j%29%5C%7D_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{b(x_i,x_j)\}_{i,j}"/> and <img alt="\{b(x_i)\}_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bb%28x_i%29%5C%7D_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{b(x_i)\}_{i}"/> are actually meant to represent marginals for some real probability distribution <img alt="b" class="latex" src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b"/>. So at the very least, we should add the consistency constraints <img alt="b(x_i)=\sum_{x_j \in \chi} b(x_i,x_j)" class="latex" src="https://s0.wp.com/latex.php?latex=b%28x_i%29%3D%5Csum_%7Bx_j+%5Cin+%5Cchi%7D+b%28x_i%2Cx_j%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b(x_i)=\sum_{x_j \in \chi} b(x_i,x_j)"/> and <img alt="\sum_{x_i \in \chi} b(x_i)=1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bx_i+%5Cin+%5Cchi%7D+b%28x_i%29%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sum_{x_i \in \chi} b(x_i)=1"/> to our optimization problem. We can then think of <img alt="b(x_i,x_j)" class="latex" src="https://s0.wp.com/latex.php?latex=b%28x_i%2Cx_j%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b(x_i,x_j)"/> and <img alt="b(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=b%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b(x_i)"/> as “pseudo-marginals” which obey degree-2 Sherali-Adams constraints.</p>
<p> </p>
<p>Recall that we’ve written <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> as a sum of both the average energy <img alt="U" class="latex" src="https://s0.wp.com/latex.php?latex=U&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="U"/> and the entropy <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/>. It turns out that we <em>can</em> actually write <img alt="U" class="latex" src="https://s0.wp.com/latex.php?latex=U&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="U"/> as only a function of the pariwise marginals of <img alt="b" class="latex" src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b"/>:</p>
<p><img alt="U(b(\vec{x})) =-\sum_{i,j}\sum_{x_i,x_j} b(x_i,x_j)J_{i,j}(x_i,x_j)-\sum_{i}\sum_{x_i}b(x_i) h_{i}(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=U%28b%28%5Cvec%7Bx%7D%29%29+%3D-%5Csum_%7Bi%2Cj%7D%5Csum_%7Bx_i%2Cx_j%7D+b%28x_i%2Cx_j%29J_%7Bi%2Cj%7D%28x_i%2Cx_j%29-%5Csum_%7Bi%7D%5Csum_%7Bx_i%7Db%28x_i%29+h_%7Bi%7D%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="U(b(\vec{x})) =-\sum_{i,j}\sum_{x_i,x_j} b(x_i,x_j)J_{i,j}(x_i,x_j)-\sum_{i}\sum_{x_i}b(x_i) h_{i}(x_i)"/></p>
<p>which follows just because the sums marginalize out the variables which don’t form part of the pairwise interactions:<br/>
<img alt="\sum_{\vec{x}}b(\vec{x})\left(-\sum_{i,j}J_{i,j}(x_i,x_j)-\sum_{i} h_{i}(x_i)\right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_%7B%5Cvec%7Bx%7D%7Db%28%5Cvec%7Bx%7D%29%5Cleft%28-%5Csum_%7Bi%2Cj%7DJ_%7Bi%2Cj%7D%28x_i%2Cx_j%29-%5Csum_%7Bi%7D+h_%7Bi%7D%28x_i%29%5Cright%29+&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sum_{\vec{x}}b(\vec{x})\left(-\sum_{i,j}J_{i,j}(x_i,x_j)-\sum_{i} h_{i}(x_i)\right) "/></p>
<p><img alt="=-\sum_{i,j}\sum_{x_i,x_j} b(x_i,x_j)J_{i,j}(x_i,x_j)-\sum_{i}\sum_{x_i}b(x_i) h_{i}(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=%3D-%5Csum_%7Bi%2Cj%7D%5Csum_%7Bx_i%2Cx_j%7D+b%28x_i%2Cx_j%29J_%7Bi%2Cj%7D%28x_i%2Cx_j%29-%5Csum_%7Bi%7D%5Csum_%7Bx_i%7Db%28x_i%29+h_%7Bi%7D%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="=-\sum_{i,j}\sum_{x_i,x_j} b(x_i,x_j)J_{i,j}(x_i,x_j)-\sum_{i}\sum_{x_i}b(x_i) h_{i}(x_i)"/></p>
<p>This is good news: since <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P"/> only depends on pairwise interactions, the average energy component of <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> only depends on <img alt="b(x_i,x_j)" class="latex" src="https://s0.wp.com/latex.php?latex=b%28x_i%2Cx_j%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b(x_i,x_j)"/> and <img alt="b(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=b%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b(x_i)"/>. However, it is not so clear how to express the entropy as a function of one node and two node beliefs. However, maybe we can try to pretend that our model is really a “tree”. In this case, the following is true:</p>
<p> </p>
<p><strong>Claim:</strong> If our model is a tree, and <img alt="\{b(x_i,x_j)\}_{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bb%28x_i%2Cx_j%29%5C%7D_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{b(x_i,x_j)\}_{i,j}"/> and <img alt="\{b(x_i)\}_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bb%28x_i%29%5C%7D_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{b(x_i)\}_{i}"/> are the associated marginals of our probabilistic model <img alt="b(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=b%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b(\vec{x})"/>, then we have<br/>
<img alt="b(\vec{x})=\frac{\prod_{(i,j) \in E}b(x_i,x_j)}{\prod_{i \in [n]} b(x_i)^{q_i-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=b%28%5Cvec%7Bx%7D%29%3D%5Cfrac%7B%5Cprod_%7B%28i%2Cj%29+%5Cin+E%7Db%28x_i%2Cx_j%29%7D%7B%5Cprod_%7Bi+%5Cin+%5Bn%5D%7D+b%28x_i%29%5E%7Bq_i-1%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b(\vec{x})=\frac{\prod_{(i,j) \in E}b(x_i,x_j)}{\prod_{i \in [n]} b(x_i)^{q_i-1}}"/></p>
<p>where <img alt="q_i" class="latex" src="https://s0.wp.com/latex.php?latex=q_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q_i"/> is the degree of vertex <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> in the tree.</p>
<p> </p>
<p>It’s not too difficult to see why this is the case: imagine a tree rooted at <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>, with children <img alt="\partial i" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpartial+i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\partial i"/>. We can think of sampling from this tree as first sampling from <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> via its marginal <img alt="b(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=b%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b(x_i)"/>, and then by recursively sampling the children conditioned on <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>. Associate with <img alt="\{T_k\}_{k \in \partial i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7BT_k%5C%7D_%7Bk+%5Cin+%5Cpartial+i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{T_k\}_{k \in \partial i}"/> the subtrees of the children of <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>, i.e. <img alt="T_j(V_{j})" class="latex" src="https://s0.wp.com/latex.php?latex=T_j%28V_%7Bj%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T_j(V_{j})"/> is equal to the probability of the occurrence <img alt="V_{j}" class="latex" src="https://s0.wp.com/latex.php?latex=V_%7Bj%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="V_{j}"/> on the probabilistic model of the tree rooted at vertex <img alt="j" class="latex" src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="j"/>. Then we have</p>
<p><img alt="b(\vec{x}) =b(x_i) \prod_{k \in \partial i} b(x_k|x_i) \prod_{k \in \partial i} T_k(V_{k}|x_k)" class="latex" src="https://s0.wp.com/latex.php?latex=b%28%5Cvec%7Bx%7D%29+%3Db%28x_i%29+%5Cprod_%7Bk+%5Cin+%5Cpartial+i%7D+b%28x_k%7Cx_i%29+%5Cprod_%7Bk+%5Cin+%5Cpartial+i%7D+T_k%28V_%7Bk%7D%7Cx_k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b(\vec{x}) =b(x_i) \prod_{k \in \partial i} b(x_k|x_i) \prod_{k \in \partial i} T_k(V_{k}|x_k)"/></p>
<p><img alt="=b(x_i)\frac{1}{b(x_i)^{q_i}} \prod_{k \in \partial i} b(x_i,x_k) \prod_{k \in \partial i}\frac{1}{b(x_k)} \prod_{k \in \partial i}T_k(V_{k})" class="latex" src="https://s0.wp.com/latex.php?latex=%3Db%28x_i%29%5Cfrac%7B1%7D%7Bb%28x_i%29%5E%7Bq_i%7D%7D+%5Cprod_%7Bk+%5Cin+%5Cpartial+i%7D+b%28x_i%2Cx_k%29+%5Cprod_%7Bk+%5Cin+%5Cpartial+i%7D%5Cfrac%7B1%7D%7Bb%28x_k%29%7D+%5Cprod_%7Bk+%5Cin+%5Cpartial+i%7DT_k%28V_%7Bk%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="=b(x_i)\frac{1}{b(x_i)^{q_i}} \prod_{k \in \partial i} b(x_i,x_k) \prod_{k \in \partial i}\frac{1}{b(x_k)} \prod_{k \in \partial i}T_k(V_{k})"/><br/>
<img alt="=\frac{\prod_{(i,j) \in E}b(x_i,x_j)}{\prod_{i \in [N]}b(x_i)^{q_i-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%3D%5Cfrac%7B%5Cprod_%7B%28i%2Cj%29+%5Cin+E%7Db%28x_i%2Cx_j%29%7D%7B%5Cprod_%7Bi+%5Cin+%5BN%5D%7Db%28x_i%29%5E%7Bq_i-1%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="=\frac{\prod_{(i,j) \in E}b(x_i,x_j)}{\prod_{i \in [N]}b(x_i)^{q_i-1}}"/></p>
<p>where the last line follows inductively, since each <img alt="T_k" class="latex" src="https://s0.wp.com/latex.php?latex=T_k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T_k"/> only sees <img alt="q_{k}-1" class="latex" src="https://s0.wp.com/latex.php?latex=q_%7Bk%7D-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q_{k}-1"/> edges of <img alt="x_k" class="latex" src="https://s0.wp.com/latex.php?latex=x_k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_k"/>.</p>
<p> </p>
<p>If we make the assumption that our model is a tree, then we can write the Bethe approximation entropy as</p>
<p><img alt="S_{Bethe}=-\sum_{i,j}\sum_{x_i,x_j} b(x_i,x_j)\ln(b(x_i,x_j))+\sum_{i}(q_i-1)\sum_{x_i}b(x_i)\ln b(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=S_%7BBethe%7D%3D-%5Csum_%7Bi%2Cj%7D%5Csum_%7Bx_i%2Cx_j%7D+b%28x_i%2Cx_j%29%5Cln%28b%28x_i%2Cx_j%29%29%2B%5Csum_%7Bi%7D%28q_i-1%29%5Csum_%7Bx_i%7Db%28x_i%29%5Cln+b%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S_{Bethe}=-\sum_{i,j}\sum_{x_i,x_j} b(x_i,x_j)\ln(b(x_i,x_j))+\sum_{i}(q_i-1)\sum_{x_i}b(x_i)\ln b(x_i)"/></p>
<p>Where the <img alt="\{q_i\}_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bq_i%5C%7D_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{q_i\}_{i}"/>‘s are the degrees of the variables <img alt="\{x_i\}_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bx_i%5C%7D_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{x_i\}_{i}"/> in the graphical model defined by <img alt="P(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\vec{x})"/>. We then define the Bethe free energy as <img alt="U+S_{Bethe}" class="latex" src="https://s0.wp.com/latex.php?latex=U%2BS_%7BBethe%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="U+S_{Bethe}"/>. The Bethe free energy is in general not an upper bound on the true free energy. Note that if we make the assignments <img alt="E_{i}(x_i)=h_{i}(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=E_%7Bi%7D%28x_i%29%3Dh_%7Bi%7D%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E_{i}(x_i)=h_{i}(x_i)"/>, <img alt="E_{i,j}(x_i,x_j)=-J_{i,j}(x_i,x_j)-h_{i}(x_i)-h_{j}(x_j)" class="latex" src="https://s0.wp.com/latex.php?latex=E_%7Bi%2Cj%7D%28x_i%2Cx_j%29%3D-J_%7Bi%2Cj%7D%28x_i%2Cx_j%29-h_%7Bi%7D%28x_i%29-h_%7Bj%7D%28x_j%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E_{i,j}(x_i,x_j)=-J_{i,j}(x_i,x_j)-h_{i}(x_i)-h_{j}(x_j)"/>, then we can rewrite <img alt="U" class="latex" src="https://s0.wp.com/latex.php?latex=U&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="U"/> as<br/>
<img alt="U=\sum_{i,j}\sum_{x_i,x_j} b(x_i,x_j)E_{i,j}(x_i,x_j)+\sum_{i}(q_i-1)\sum_{x_i}b(x_i)E_{i}(x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=U%3D%5Csum_%7Bi%2Cj%7D%5Csum_%7Bx_i%2Cx_j%7D+b%28x_i%2Cx_j%29E_%7Bi%2Cj%7D%28x_i%2Cx_j%29%2B%5Csum_%7Bi%7D%28q_i-1%29%5Csum_%7Bx_i%7Db%28x_i%29E_%7Bi%7D%28x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="U=\sum_{i,j}\sum_{x_i,x_j} b(x_i,x_j)E_{i,j}(x_i,x_j)+\sum_{i}(q_i-1)\sum_{x_i}b(x_i)E_{i}(x_i)"/></p>
<p> </p>
<p>which is similar in form to the Bethe approximation entropy. In general, we have</p>
<p><img alt="G_{Bethe}(b(x_i),b(x_i,x_j))=" class="latex" src="https://s0.wp.com/latex.php?latex=G_%7BBethe%7D%28b%28x_i%29%2Cb%28x_i%2Cx_j%29%29%3D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G_{Bethe}(b(x_i),b(x_i,x_j))="/></p>
<p><img alt="\sum_{i,j}\sum_{x_i,x_j} b(x_i,x_j)(E_{i,j}(x_i,x_j)+\ln(b(x_i,x_j)))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bi%2Cj%7D%5Csum_%7Bx_i%2Cx_j%7D+b%28x_i%2Cx_j%29%28E_%7Bi%2Cj%7D%28x_i%2Cx_j%29%2B%5Cln%28b%28x_i%2Cx_j%29%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sum_{i,j}\sum_{x_i,x_j} b(x_i,x_j)(E_{i,j}(x_i,x_j)+\ln(b(x_i,x_j)))"/></p>
<p><img alt="-\sum_{i}(q_i-1)\sum_{x_i}b(x_i)( E_{i}(x_i)+\ln b(x_i))" class="latex" src="https://s0.wp.com/latex.php?latex=-%5Csum_%7Bi%7D%28q_i-1%29%5Csum_%7Bx_i%7Db%28x_i%29%28+E_%7Bi%7D%28x_i%29%2B%5Cln+b%28x_i%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="-\sum_{i}(q_i-1)\sum_{x_i}b(x_i)( E_{i}(x_i)+\ln b(x_i))"/></p>
<p>which is exactly the Gibbs free energy for a probabilistic model whose associated graph is a tree. Since BP gives the correct marginals on trees, we can say that the BP beliefs are the global minima of the Bethe free energy. However, the following is also true:</p>
<p><strong>Proposition:</strong> A set of beliefs gives a BP fixed point in any graph (not necessarily a tree) iff they correspond to local stationary points of the Bethe free energy.</p>
<p>(For a proof, see e.g. page 20 of [4])</p>
<p>So trying to minimize the Bethe free energy is in some sense the same thing as doing belief propagation. Apparently, one typically finds that when belief propagation fails to converge on a graph, the optimization program which is trying to minimize <img alt="G_{Bethe}" class="latex" src="https://s0.wp.com/latex.php?latex=G_%7BBethe%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G_{Bethe}"/> also runs into problems in similar parameter regions, and vice versa.</p>
<p> </p>
<h2>(3) The Block Model</h2>
<h3>(3.0) Definitions</h3>
<p>Now that we’ve seen Belief Propagation from two different perspectives, let’s try to apply this technique of computing marginals to analyzing the behavior of the stochastic block model. This section will heavily follow the paper [2].</p>
<p>The stochastic block model is designed to capture a variety of interesting problems, depending on its settings of parameters. The question we’ll be looking at is the following: suppose we generate a random graph, where each vertex of the graph comes from one of <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q"/> groups each with probability <img alt="n_{1},...,n_{q}" class="latex" src="https://s0.wp.com/latex.php?latex=n_%7B1%7D%2C...%2Cn_%7Bq%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n_{1},...,n_{q}"/>. We add an edge between vertices <img alt="(i,j)" class="latex" src="https://s0.wp.com/latex.php?latex=%28i%2Cj%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(i,j)"/> in groups <img alt="a,b" class="latex" src="https://s0.wp.com/latex.php?latex=a%2Cb&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a,b"/> resp. with probability <img alt="p_{a,b}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Ba%2Cb%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{a,b}"/>. For sparse graphs, we define <img alt="c_{i,j}:=Np_{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=c_%7Bi%2Cj%7D%3A%3DNp_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_{i,j}:=Np_{i,j}"/>, where we think of <img alt="p_{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{i,j}"/> as <img alt="\mathcal{O}(\frac{1}{N})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BO%7D%28%5Cfrac%7B1%7D%7BN%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{O}(\frac{1}{N})"/>. The problem is the following: given such a random graph, can you label the vertices so that, up to permutation, the labels you choose have high correlation to the true hidden labels which were used to generate the graph? Here are some typical settings of parameters which represent different problems:</p>
<ol>
<li> Community detection, where we have <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q"/> groups. We set <img alt="n_{i}=\frac{1}{q}" class="latex" src="https://s0.wp.com/latex.php?latex=n_%7Bi%7D%3D%5Cfrac%7B1%7D%7Bq%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n_{i}=\frac{1}{q}"/>, and <img alt="c_{i,j}=c_{in}" class="latex" src="https://s0.wp.com/latex.php?latex=c_%7Bi%2Cj%7D%3Dc_%7Bin%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_{i,j}=c_{in}"/> if <img alt="i=j" class="latex" src="https://s0.wp.com/latex.php?latex=i%3Dj&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i=j"/> and <img alt="c_{out}" class="latex" src="https://s0.wp.com/latex.php?latex=c_%7Bout%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_{out}"/> otherwise, with <img alt="c_{in}&gt;c_{out}" class="latex" src="https://s0.wp.com/latex.php?latex=c_%7Bin%7D%3Ec_%7Bout%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_{in}&gt;c_{out}"/> (assortative structure).</li>
<li>Planted graph partitioning, where <img alt="p_{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{i,j}"/> may not necessarily be <img alt="\mathcal{O}(\frac{1}{N})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BO%7D%28%5Cfrac%7B1%7D%7BN%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{O}(\frac{1}{N})"/>.</li>
<li>Planted graph colouring, where <img alt="p_{in}=0" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bin%7D%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{in}=0"/>, <img alt="p_{a,b}=p_{out}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Ba%2Cb%7D%3Dp_%7Bout%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{a,b}=p_{out}"/> and <img alt="n_{a}=\frac{1}{q}" class="latex" src="https://s0.wp.com/latex.php?latex=n_%7Ba%7D%3D%5Cfrac%7B1%7D%7Bq%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n_{a}=\frac{1}{q}"/> for all groups. We know how to efficiently find a planted coloring which is strongly correlated with the true coloring when the average degree <img alt="c=(q-1)p_{out}N/q" class="latex" src="https://s0.wp.com/latex.php?latex=c%3D%28q-1%29p_%7Bout%7DN%2Fq&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c=(q-1)p_{out}N/q"/> is greater than <img alt="\mathcal{O}(q^2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BO%7D%28q%5E2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{O}(q^2)"/>.</li>
</ol>
<p>We’ll concern ourselves with the case where our graph is sparse, and we need to try and come up with an assignment for the vertices such that we have high correlation with the true labeling of vertices. How might we measure how well we solve this task? Ideally, a labeling which is identical to the true labeling (up to permutation) should get a score of 1. Conversely, a labeling which naively guesses that every vertex comes from the largest group <img alt="\arg \max_{i \in [q]} n_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Carg+%5Cmax_%7Bi+%5Cin+%5Bq%5D%7D+n_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\arg \max_{i \in [q]} n_{i}"/> should get a score of 0.  Here’s one metric which satisfies these properties: if we come up with a labeling <img alt="\{t_{i}\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bt_%7Bi%7D%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{t_{i}\}"/>, and the true labeling is <img alt="\{q_i\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bq_i%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{q_i\}"/>, then we’ll measure our performance by</p>
<p> </p>
<p><img alt="Q(\{t_{i}\},\{q_{i}\}):=\max_{\pi \in S_q} \frac{\frac{1}{N}\sum_{i \in [N]} \delta_{t_i,\pi(q_i)}-max_{a \in [q]} n_a}{1-\max_{a \in [q]} n_a}" class="latex" src="https://s0.wp.com/latex.php?latex=Q%28%5C%7Bt_%7Bi%7D%5C%7D%2C%5C%7Bq_%7Bi%7D%5C%7D%29%3A%3D%5Cmax_%7B%5Cpi+%5Cin+S_q%7D+%5Cfrac%7B%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi+%5Cin+%5BN%5D%7D+%5Cdelta_%7Bt_i%2C%5Cpi%28q_i%29%7D-max_%7Ba+%5Cin+%5Bq%5D%7D+n_a%7D%7B1-%5Cmax_%7Ba+%5Cin+%5Bq%5D%7D+n_a%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q(\{t_{i}\},\{q_{i}\}):=\max_{\pi \in S_q} \frac{\frac{1}{N}\sum_{i \in [N]} \delta_{t_i,\pi(q_i)}-max_{a \in [q]} n_a}{1-\max_{a \in [q]} n_a}"/></p>
<p> </p>
<p>where we maximize over all permutations <img alt="\pi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\pi"/>. When we choose a labeling which (up to permutation) agrees with the true labeling, then the numerator of <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q"/> will equal the denominator, and <img alt="Q=1" class="latex" src="https://s0.wp.com/latex.php?latex=Q%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q=1"/>. Likewise, when we trivially guess that every vertex belongs to the largest group, then the numerator of <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q"/> is <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0"/> and <img alt="Q=0" class="latex" src="https://s0.wp.com/latex.php?latex=Q%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q=0"/>.</p>
<p>Given <img alt="\{c_{a,b}\},\{n_a\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bc_%7Ba%2Cb%7D%5C%7D%2C%5C%7Bn_a%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{c_{a,b}\},\{n_a\}"/> and a set of observed edges <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E"/>, we can write down the probability of a labeling <img alt="\{q_i\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bq_i%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{q_i\}"/> as<br/>
<img alt="P(\{q_i\}_{i})=\prod_{\substack{(i,j) \not \in E \\ i \not = j}}(1-p_{q_{i},q_{j}})\prod_{\substack{(i,j) \in E }}p_{q_{i},q_{j}} \prod_{i \in [q]}n_{q_i}" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5C%7Bq_i%5C%7D_%7Bi%7D%29%3D%5Cprod_%7B%5Csubstack%7B%28i%2Cj%29+%5Cnot+%5Cin+E+%5C%5C+i+%5Cnot+%3D+j%7D%7D%281-p_%7Bq_%7Bi%7D%2Cq_%7Bj%7D%7D%29%5Cprod_%7B%5Csubstack%7B%28i%2Cj%29+%5Cin+E+%7D%7Dp_%7Bq_%7Bi%7D%2Cq_%7Bj%7D%7D+%5Cprod_%7Bi+%5Cin+%5Bq%5D%7Dn_%7Bq_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\{q_i\}_{i})=\prod_{\substack{(i,j) \not \in E \\ i \not = j}}(1-p_{q_{i},q_{j}})\prod_{\substack{(i,j) \in E }}p_{q_{i},q_{j}} \prod_{i \in [q]}n_{q_i}"/></p>
<p>How might we try to infer <img alt="\{q_i\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bq_i%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{q_i\}"/> such that we have maximum correlation (up to permutation) with the true labeling? It turns out that the answer is to use the maximum likelihood estimator of the marginal distribution of each <img alt="q_i" class="latex" src="https://s0.wp.com/latex.php?latex=q_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q_i"/>, up to a caveat. In particular, we should label <img alt="q_i" class="latex" src="https://s0.wp.com/latex.php?latex=q_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q_i"/> with the <img alt="r \in [q]" class="latex" src="https://s0.wp.com/latex.php?latex=r+%5Cin+%5Bq%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r \in [q]"/> such that <img alt="P(q_i=r)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28q_i%3Dr%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(q_i=r)"/> is maximized. The caveat comes in when <img alt="P(\{q_i\}_{i})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5C%7Bq_i%5C%7D_%7Bi%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\{q_i\}_{i})"/> is invariant under permutations of the labelings <img alt="\{q_i\}_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bq_i%5C%7D_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{q_i\}_{i}"/>, so that each marginal <img alt="P(q_i)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28q_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(q_i)"/> is actually the uniform distribution. For example, this happens in community detection, when all the group sizes <img alt="n_{1},...,n_{q}" class="latex" src="https://s0.wp.com/latex.php?latex=n_%7B1%7D%2C...%2Cn_%7Bq%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n_{1},...,n_{q}"/> are equal. In this case, the correct thing to do is to still use the marginals, but only after we have “broken the symmetry” of the problem by randomly fixing certain values of the vertices to have particular labels. There’s actually a way belief propagation does this implicitly: recall that we start belief propagation by randomly initializing the messages. This random initialization can be interpreted as “symmetry breaking” of the problem, in a way that we’ll see shortly.</p>
<h2/>
<h3>(3.1) Belief Propagation</h3>
<p>We’ve just seen from the previous section that in order to maximize the correlation of the labeling we come up with, we should pick the labelings which maximize the marginals of <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P"/>. So we have some marginals that we want to compute. Let’s proceed by applying BP to this problem in the “sparse” regime where <img alt="c_{a,b}=Np_{a,b}=\mathcal{O}(1)" class="latex" src="https://s0.wp.com/latex.php?latex=c_%7Ba%2Cb%7D%3DNp_%7Ba%2Cb%7D%3D%5Cmathcal%7BO%7D%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_{a,b}=Np_{a,b}=\mathcal{O}(1)"/> (other algorithms, like approximate message passing, can be used for “dense” graph problems). Suppose we’re given a random graph with edge list <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E"/>. What does does graph associated with our probabilistic model look like? Well, in this case, every variable is actually connected to every other variable because <img alt="P(\{q_i\}_{i})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5C%7Bq_i%5C%7D_%7Bi%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(\{q_i\}_{i})"/> includes a factor <img alt="f_{i,j}(x_i,x_j)" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bi%2Cj%7D%28x_i%2Cx_j%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_{i,j}(x_i,x_j)"/> for every <img alt="(i,j) \in [n] \times [n]" class="latex" src="https://s0.wp.com/latex.php?latex=%28i%2Cj%29+%5Cin+%5Bn%5D+%5Ctimes+%5Bn%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(i,j) \in [n] \times [n]"/>, so we actually have a complete graph. However, some of the connections between variables are much weaker than others. In full, our BP update equations are</p>
<p><img alt="eqs_2" class="alignnone size-full wp-image-6280" src="https://windowsontheory.files.wordpress.com/2018/10/eqs_2.png?w=600"/></p>
<p>Likewise<br/>
<img alt="\psi^{i}_{t_{i}}=\frac{1}{Z^{i}} n_{t_{i}} \prod_{\substack{(i,k) \not \in E}}\left[ 1-\frac{1}{N}\sum_{t_k \in [q]}c_{t_1,t_k}\psi^{k \rightarrow i}_{t_k} \right]\prod_{\substack{(i,k) \in E}}\left[\sum_{t_k \in [q]}c_{t_i,t_k} \psi^{k \rightarrow i}_{t_k} \right]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi%7D_%7Bt_%7Bi%7D%7D%3D%5Cfrac%7B1%7D%7BZ%5E%7Bi%7D%7D+n_%7Bt_%7Bi%7D%7D+%5Cprod_%7B%5Csubstack%7B%28i%2Ck%29+%5Cnot+%5Cin+E%7D%7D%5Cleft%5B+1-%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bt_k+%5Cin+%5Bq%5D%7Dc_%7Bt_1%2Ct_k%7D%5Cpsi%5E%7Bk+%5Crightarrow+i%7D_%7Bt_k%7D+%5Cright%5D%5Cprod_%7B%5Csubstack%7B%28i%2Ck%29+%5Cin+E%7D%7D%5Cleft%5B%5Csum_%7Bt_k+%5Cin+%5Bq%5D%7Dc_%7Bt_i%2Ct_k%7D+%5Cpsi%5E%7Bk+%5Crightarrow+i%7D_%7Bt_k%7D+%5Cright%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i}_{t_{i}}=\frac{1}{Z^{i}} n_{t_{i}} \prod_{\substack{(i,k) \not \in E}}\left[ 1-\frac{1}{N}\sum_{t_k \in [q]}c_{t_1,t_k}\psi^{k \rightarrow i}_{t_k} \right]\prod_{\substack{(i,k) \in E}}\left[\sum_{t_k \in [q]}c_{t_i,t_k} \psi^{k \rightarrow i}_{t_k} \right]"/></p>
<p> </p>
<p>what we want to do is approximate these equations so that we only have to pass messages along the edges <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E"/>, instead of the complete graph. This will make our analysis simpler, and also allow the belief propagation algorithm to run more efficiently. The first observation is the following: Suppose we have two nodes <img alt="j,j' \in [N]" class="latex" src="https://s0.wp.com/latex.php?latex=j%2Cj%27+%5Cin+%5BN%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="j,j' \in [N]"/> such that <img alt="(i,j),(i,j') \not \in E" class="latex" src="https://s0.wp.com/latex.php?latex=%28i%2Cj%29%2C%28i%2Cj%27%29+%5Cnot+%5Cin+E&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(i,j),(i,j') \not \in E"/>. Then we see that <img alt="\psi^{i \rightarrow j}_{t_{i}}=\psi^{i \rightarrow j'}_{t_{i}}+\mathcal{O}\left(\frac{1}{N}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Bt_%7Bi%7D%7D%3D%5Cpsi%5E%7Bi+%5Crightarrow+j%27%7D_%7Bt_%7Bi%7D%7D%2B%5Cmathcal%7BO%7D%5Cleft%28%5Cfrac%7B1%7D%7BN%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i \rightarrow j}_{t_{i}}=\psi^{i \rightarrow j'}_{t_{i}}+\mathcal{O}\left(\frac{1}{N}\right)"/>, since the only difference between these two variables are two factors of order <img alt="(1-\mathcal{O}\left(\frac{1}{N}\right))" class="latex" src="https://s0.wp.com/latex.php?latex=%281-%5Cmathcal%7BO%7D%5Cleft%28%5Cfrac%7B1%7D%7BN%7D%5Cright%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(1-\mathcal{O}\left(\frac{1}{N}\right))"/> which appear in the first product of the BP equations. Thus, we send essentially the same messages to non-neighbours <img alt="j" class="latex" src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="j"/> of <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/> in our random graph. In general though, we have:</p>
<p><img alt="eqs_3.png" class="alignnone size-full wp-image-6281" src="https://windowsontheory.files.wordpress.com/2018/10/eqs_3.png?w=600"/></p>
<p>The first approximation comes from dropping non-edge constraints on the first product, and is reasonable because we expect the number of neighbours of <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/> to be constant. We’ve also defined a variable</p>
<p><img alt="h_{t_i}:=\frac{1}{N}\sum_{k \in [N]}\sum_{t_k \in [q]}c_{t_i,t_k} \psi^{k \rightarrow i}_{t_k}" class="latex" src="https://s0.wp.com/latex.php?latex=h_%7Bt_i%7D%3A%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bk+%5Cin+%5BN%5D%7D%5Csum_%7Bt_k+%5Cin+%5Bq%5D%7Dc_%7Bt_i%2Ct_k%7D+%5Cpsi%5E%7Bk+%5Crightarrow+i%7D_%7Bt_k%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_{t_i}:=\frac{1}{N}\sum_{k \in [N]}\sum_{t_k \in [q]}c_{t_i,t_k} \psi^{k \rightarrow i}_{t_k}"/></p>
<p>and we’ve used the approximation <img alt="e^{-x} \approx 1-x" class="latex" src="https://s0.wp.com/latex.php?latex=e%5E%7B-x%7D+%5Capprox+1-x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e^{-x} \approx 1-x"/> for small <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/>. We think of the term <img alt="h_{t_i}" class="latex" src="https://s0.wp.com/latex.php?latex=h_%7Bt_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_{t_i}"/> as defining an “auxiliary external field”. We’ll use this approximate BP equation to find solutions for our problem. This has the advantage that the computation time is <img alt="\mathcal{O}(|E|)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BO%7D%28%7CE%7C%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{O}(|E|)"/> instead of <img alt="\mathcal{O}(N^2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BO%7D%28N%5E2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{O}(N^2)"/>, so we can deal with large sparse graphs computationally. It also allows us to see how a large dense graphical model with only sparse strong connections still behaves like a sparse tree-like graphical model from the perspective of Belief Propagation. In particular, we might have reason to hope that the BP equations will actually converge and give us good approximations to the marginals.</p>
<p>From now on, we’ll only consider factored block models, which in some sense represent a “hard” setting of parameters. These are models which satisfy the condition that each group has the same average degree <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c"/>. In particular, we require</p>
<p><img alt="\sum_{d=1}^{q} c_{a,d} n_d = \sum_{d=1}^{q} c_{b,d}n_d =c" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bd%3D1%7D%5E%7Bq%7D+c_%7Ba%2Cd%7D+n_d+%3D+%5Csum_%7Bd%3D1%7D%5E%7Bq%7D+c_%7Bb%2Cd%7Dn_d+%3Dc&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sum_{d=1}^{q} c_{a,d} n_d = \sum_{d=1}^{q} c_{b,d}n_d =c"/></p>
<p>An important observation for this setting of parameters is that</p>
<p><img alt="\psi^{i \rightarrow j}_{t_i}=n_{t_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bi+%5Crightarrow+j%7D_%7Bt_i%7D%3Dn_%7Bt_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{i \rightarrow j}_{t_i}=n_{t_i}"/></p>
<p>is always a fixed point of our BP equations, which is known as a <em>factored fixed point</em> (this can be seen by inspection by plugging the fixed point conditions into the belief propagation equations we derived). When BP ever reaches such a fixed point, we get that <img alt="Q=0" class="latex" src="https://s0.wp.com/latex.php?latex=Q%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q=0"/> and the algorithm fails. However, we might hope that if we randomly initialize <img alt="\{\psi^{i \rightarrow j}\}_{(i,j) \in E}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpsi%5E%7Bi+%5Crightarrow+j%7D%5C%7D_%7B%28i%2Cj%29+%5Cin+E%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{\psi^{i \rightarrow j}\}_{(i,j) \in E}"/>, then BP might converge to some non-trivial fixed point which gives us some information about the original labeling of the vertices.</p>
<h3>(3.2) Numerical Results</h3>
<p>Now that we have our BP equations, we can run numerical simulations to try and get a feel of when BP works. Let’s consider the problem of community detection. In particular, we’ll set our parameters with all group sizes <img alt="n_{a}" class="latex" src="https://s0.wp.com/latex.php?latex=n_%7Ba%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n_{a}"/> being equal, and with <img alt="c_{a,a}=c_{in}&gt;c_{out}=c_{a,b}" class="latex" src="https://s0.wp.com/latex.php?latex=c_%7Ba%2Ca%7D%3Dc_%7Bin%7D%3Ec_%7Bout%7D%3Dc_%7Ba%2Cb%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_{a,a}=c_{in}&gt;c_{out}=c_{a,b}"/> for <img alt="a \not = b" class="latex" src="https://s0.wp.com/latex.php?latex=a+%5Cnot+%3D+b&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a \not = b"/> and vary the ratio <img alt="\epsilon:=c_{out}/c_{in}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon%3A%3Dc_%7Bout%7D%2Fc_%7Bin%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon:=c_{out}/c_{in}"/>, and see when BP finds solutions which are correlated “better than guessing” to the original labeling used to generate the graph. When we do this, we get images which look like this:</p>
<p><img alt="convergence_and_overlap.png" class="alignnone size-full wp-image-6274" src="https://windowsontheory.files.wordpress.com/2018/10/convergence_and_overlap.png?w=600"/></p>
<p>It should be mentioned that the point at which the dashed red line occurs depends on the parameters of the stochastic block model. We get a few interesting observations from numerical experiments:</p>
<ol>
<li>The point at which BP fails (<img alt="Q=0" class="latex" src="https://s0.wp.com/latex.php?latex=Q%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q=0"/>) happens at a certain value of <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon"/> independent of <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N"/>. In particular, when <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon"/> gets too large, BP converges to the trivial factored fixed point. We’ll see in the next section that this is a “stable” fixed point for certain settings of parameters.</li>
<li>Using the approximate BP equations gives the same performance as using the exact BP equations. This numerically justifies the approximations we made.</li>
<li>If we try to estimate the marginals with other tools from our statistical inference toolbox, we find (perhaps surprisingly) that Markov Chain Monte Carlo methods give the same performance as Belief Propagation, even though they take significantly longer to run and have strong asymptotic guarantees on their correctness (if you run MCMC long enough, you will eventually get the correct marginals, but this may take exponential time). This naturally suggests the conjecture that belief propagation is optimal for this task.</li>
<li>As we get closer to the region where BP fails and converges to the trivial fixed point, the number of iterations required for BP to converge increases significantly, and diverges at the critical point <img alt="\epsilon_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_{c}"/> where BP fails and converges to the factored fixed point. The same kind of behavior is seen when using Gibbs sampling.</li>
</ol>
<h3>(3.3) Heuristic Stability Calculations</h3>
<p>How might we analytically try to determine when BP fails for certain settings of <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q"/> and <img alt="\{c_{a,b}\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bc_%7Ba%2Cb%7D%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{c_{a,b}\}"/>? One way we might heuristically try to do this, is to calculate the stability of the factored fixed point. If the fixed point is stable, this suggests that BP will converge to a factored point. If however it is unstable, then we might hope that BP converges to something informative. In particular, suppose we run BP, and we converge to a factored fixed point, so we have that for all our messages <img alt="\psi_{t_i}^{i \rightarrow j}=n_{t_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7Bt_i%7D%5E%7Bi+%5Crightarrow+j%7D%3Dn_%7Bt_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi_{t_i}^{i \rightarrow j}=n_{t_i}"/>. Suppose we now add a small amount of noise to some of the <img alt="\psi_{t_i}^{i \rightarrow j}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7Bt_i%7D%5E%7Bi+%5Crightarrow+j%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi_{t_i}^{i \rightarrow j}"/>‘s (maybe think of this as injecting a small amount of additional information about the true marginals). We (heuristically) claim that if we now continue to run more steps of BP, either the messages will converge back to the fixed point <img alt="\psi_{t_i}^{i \rightarrow j}=n_{t_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7Bt_i%7D%5E%7Bi+%5Crightarrow+j%7D%3Dn_%7Bt_i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi_{t_i}^{i \rightarrow j}=n_{t_i}"/>, or they will diverge to something else, and whether or not this happens depends on the eigenvalue of some matrix of partial derivatives.</p>
<p>Following this idea, here’s a heuristic way of calculating the stability of the factored fixed point. Let’s pretend that our BP equations occur on a tree, which is a reasonable approximation in the sparse graph case. Let our tree be rooted at node <img alt="k_0" class="latex" src="https://s0.wp.com/latex.php?latex=k_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k_0"/> and have depth <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/>. Let’s try to approximately calculate the influence on <img alt="k_0" class="latex" src="https://s0.wp.com/latex.php?latex=k_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k_0"/> of perturbing a leaf <img alt="k_d" class="latex" src="https://s0.wp.com/latex.php?latex=k_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k_d"/> from its factored fixed point. In particular, let the path from the leaf to the root be <img alt="k_d,...,k_0" class="latex" src="https://s0.wp.com/latex.php?latex=k_d%2C...%2Ck_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k_d,...,k_0"/>. We’re going to apply a perturbation <img alt="\psi^{k_d}_{t} = n_t+\epsilon_{t}^{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bk_d%7D_%7Bt%7D+%3D+n_t%2B%5Cepsilon_%7Bt%7D%5E%7Bk%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{k_d}_{t} = n_t+\epsilon_{t}^{k}"/> for each <img alt="t \in [q]" class="latex" src="https://s0.wp.com/latex.php?latex=t+%5Cin+%5Bq%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t \in [q]"/>. In vector notation, this looks like <img alt="\psi^{k_d} = \mathbf{1}n_t+\epsilon^{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bk_d%7D+%3D+%5Cmathbf%7B1%7Dn_t%2B%5Cepsilon%5E%7Bk%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{k_d} = \mathbf{1}n_t+\epsilon^{k}"/>, where <img alt="\psi^{k_d} \in \mathbb{R}^{q \times 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bk_d%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bq+%5Ctimes+1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{k_d} \in \mathbb{R}^{q \times 1}"/> is a column vector. The next thing we’ll do is define the matrix of partial derivatives</p>
<p><img alt="eqs_4.png" class="alignnone size-full wp-image-6282" src="https://windowsontheory.files.wordpress.com/2018/10/eqs_4.png?w=600"/></p>
<p>Up to first order (and ignoring normalizing constants), the perturbation effect on <img alt="\epsilon^{k_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon%5E%7Bk_0%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon^{k_0}"/> is then (by chain rule) <img alt="\epsilon^{k_0} = \left[\prod_{i=0}^{d-1} T_{i} \right]\epsilon^{k_d}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon%5E%7Bk_0%7D+%3D+%5Cleft%5B%5Cprod_%7Bi%3D0%7D%5E%7Bd-1%7D+T_%7Bi%7D+%5Cright%5D%5Cepsilon%5E%7Bk_d%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon^{k_0} = \left[\prod_{i=0}^{d-1} T_{i} \right]\epsilon^{k_d}"/>. Since <img alt="T_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=T_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T_{i}"/> does not depend on <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>, we can write this as <img alt="\epsilon^{k_0}=T^{d} \epsilon^{k_d} \approx \lambda^{d} \epsilon^{k_d}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon%5E%7Bk_0%7D%3DT%5E%7Bd%7D+%5Cepsilon%5E%7Bk_d%7D+%5Capprox+%5Clambda%5E%7Bd%7D+%5Cepsilon%5E%7Bk_d%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon^{k_0}=T^{d} \epsilon^{k_d} \approx \lambda^{d} \epsilon^{k_d}"/>, where <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda"/> is the largest eigenvalue of <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/>. Now, on a random tree, we have approximately <img alt="c^{d}" class="latex" src="https://s0.wp.com/latex.php?latex=c%5E%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c^{d}"/> leaves. If we assume that the perturbation effect from each leaf is independent, and that <img alt="\epsilon^{k_d}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon%5E%7Bk_d%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon^{k_d}"/> has 0 mean, then the net mean perturbation from all the leaves will be 0. The variance will be</p>
<p><img alt="eqs_5.png" class="alignnone  wp-image-6283" height="152" src="https://windowsontheory.files.wordpress.com/2018/10/eqs_5.png?w=307&amp;h=152" width="307"/></p>
<p>if we assume that the cross terms vanish in expectation.</p>
<p>(Aside: You might want to ask: why are we assuming that <img alt="\epsilon^{k_{d}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon%5E%7Bk_%7Bd%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon^{k_{d}}"/> has mean zero, and that (say) the noise at each of the leaves are independent, so that the cross terms vanish? If we want to maximize the variance, then maybe choosing the <img alt="\epsilon^{k_{d}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon%5E%7Bk_%7Bd%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon^{k_{d}}"/>‘s to be correlated or have non-zero mean would give us a better bound. The problem is that we’re neglecting the effects of normalizing constants in this analysis: if we perturbed all the <img alt="\psi^{k_d}_{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%5E%7Bk_d%7D_%7Bt%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi^{k_d}_{t}"/> in the same direction (e.g. non-zero mean), our normalization conditions would cancel out our perturbations.)</p>
<p>We Therefore end up with the stability condition <img alt="c \lambda^{2} =1" class="latex" src="https://s0.wp.com/latex.php?latex=c+%5Clambda%5E%7B2%7D+%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c \lambda^{2} =1"/>. When <img alt="c \lambda^{2}&gt;1" class="latex" src="https://s0.wp.com/latex.php?latex=c+%5Clambda%5E%7B2%7D%3E1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c \lambda^{2}&gt;1"/>, a small perturbation will be magnified as we move up the tree, leading to the messages moving away from the factored fixed point after successive iterations of BP (the fixed point is unstable). If <img alt="c \lambda^{2}&lt;1" class="latex" src="https://s0.wp.com/latex.php?latex=c+%5Clambda%5E%7B2%7D%3C1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c \lambda^{2}&lt;1"/>, the effect of a small perturbation will vanish as we move up the tree, we expect the factored fixed point to be stable. If we restrict our attention to graphs of the form <img alt="c_{a,a}=c_{in}&gt;c_{a,b}=c_{out}" class="latex" src="https://s0.wp.com/latex.php?latex=c_%7Ba%2Ca%7D%3Dc_%7Bin%7D%3Ec_%7Ba%2Cb%7D%3Dc_%7Bout%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_{a,a}=c_{in}&gt;c_{a,b}=c_{out}"/> for <img alt="a \not = b" class="latex" src="https://s0.wp.com/latex.php?latex=a+%5Cnot+%3D+b&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a \not = b"/>, and have all our groups with size <img alt="n_{a}=\frac{1}{q}" class="latex" src="https://s0.wp.com/latex.php?latex=n_%7Ba%7D%3D%5Cfrac%7B1%7D%7Bq%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n_{a}=\frac{1}{q}"/>, then <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> is known to have eigenvalues <img alt="\lambda_{1}=0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda_%7B1%7D%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda_{1}=0"/> with eigenvector <img alt="(1,...,1)" class="latex" src="https://s0.wp.com/latex.php?latex=%281%2C...%2C1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(1,...,1)"/>, and <img alt="\lambda_{2}=(c_{in}-c_{out})/qc" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda_%7B2%7D%3D%28c_%7Bin%7D-c_%7Bout%7D%29%2Fqc&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda_{2}=(c_{in}-c_{out})/qc"/>. The stability threshold then becomes <img alt="|c_{in}-c_{out}|=q\sqrt{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Cc_%7Bin%7D-c_%7Bout%7D%7C%3Dq%5Csqrt%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|c_{in}-c_{out}|=q\sqrt{c}"/>. This condition is known as the Almeida-Thouless local stability condition for spin glasses, and the Kesten-Stigum bound on reconstruction on trees. It is also observed empirically that BP and MCMC succeed above this threshold, and converge to factored fixed points below this threshold. The eigenvalues of <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> are related to the belief propagation equations and the backtracking matrix. For more details, see [3]</p>
<h3>(3.4) Information Theoretic Results, and what we know algorithmically</h3>
<p>We’ve just seen a threshold for when BP is able to solve the community detection problem. Specifically, when <img alt="c&lt;\frac{1}{\lambda^{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=c%3C%5Cfrac%7B1%7D%7B%5Clambda%5E%7B2%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c&lt;\frac{1}{\lambda^{2}}"/>, BP doesn’t do better than chance. It’s natural to ask whether this is because BP is not powerful enough, or whether there really isn’t enough information in the random graph to recover the true labeling. For example, if <img alt="c_{out}" class="latex" src="https://s0.wp.com/latex.php?latex=c_%7Bout%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_{out}"/> is very close to <img alt="c_{in}" class="latex" src="https://s0.wp.com/latex.php?latex=c_%7Bin%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_{in}"/>, it might be impossible to distinguish between group boundaries up to random fluctuations in the edges.</p>
<p>It turns out that for <img alt="q=2" class="latex" src="https://s0.wp.com/latex.php?latex=q%3D2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q=2"/>, there is not enough information below the threshold <img alt="c&lt;\frac{1}{\lambda^{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=c%3C%5Cfrac%7B1%7D%7B%5Clambda%5E%7B2%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c&lt;\frac{1}{\lambda^{2}}"/> to find a labeling which is correlated with the true labeling [3]. However, it can be shown information-theoretically [1] that the threshold at which one can find a correlated labeling is <img alt="c_{c} = \Theta \left(\frac{\log(q)}{q \lambda^2} \right)" class="latex" src="https://s0.wp.com/latex.php?latex=c_%7Bc%7D+%3D+%5CTheta+%5Cleft%28%5Cfrac%7B%5Clog%28q%29%7D%7Bq+%5Clambda%5E2%7D+%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_{c} = \Theta \left(\frac{\log(q)}{q \lambda^2} \right)"/>. In particular, when <img alt="q&gt;11" class="latex" src="https://s0.wp.com/latex.php?latex=q%3E11&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q&gt;11"/>, there exists exponential time algorithms which recover a correlated labeling below the Kesten-Stigum threshold. This is interesting, because it suggests an information-computational gap: we observe empirically that heuristic belief propagation seems to perform as well as any other inference algorithm at finding a correlated labeling for the stochastic block model. However, belief propagation fails at a “computational” threshold below the information theoretic threshold for this problem. We’ll talk more about these kinds of information-computation gaps in the coming weeks.</p>
<p> </p>
<p> </p>
<h2>References</h2>
<p>[1] Jess Banks, Cristopher Moore, Joe Neeman, Praneeth Netrapalli,<br/>
<em>Information-theoretic thresholds for community detection in sparse</em><br/>
<em>networks</em>. AJMLR: Workshop and Conference Proceedings vol 49:1–34, 2016.<br/>
<a href="http://proceedings.mlr.press/v49/banks16.pdf">Link</a></p>
<p>[2] Aurelien Decelle, Florent Krzakala, Cristopher Moore, Lenka Zdeborová,<br/>
<em>Asymptotic analysis of the stochastic block model for modular networks and its</em><br/>
<em>algorithmic applications</em>. 2013.<br/>
<a href="https://arxiv.org/pdf/1109.3041.pdf">Link</a></p>
<p>[3] Cristopher Moore, <em>The Computer Science and Physics</em><br/>
<em>of Community Detection: </em><em>Landscapes, Phase Transitions, </em><em>and Hardness</em>. 2017.<br/>
<a href="https://arxiv.org/pdf/1702.00467.pdf">Link</a></p>
<p>[4] Jonathan Yedidia, William Freeman, Yair Weiss, U<em>nderstanding Belief Propogation and its Generalizations<br/>
</em><a href="http://people.csail.mit.edu/billf/publications/Understanding_Belief_Propogation.pdf">Link</a></p>
<p>[5] Afonso Banderia, Amelia Perry, Alexander Wein, <em>Notes on computational-to-statistical gaps: predictions using statistical physics</em>. 2018.<br/>
<a href="https://arxiv.org/pdf/1803.11132.pdf">Link</a></p>
<p>[6] Stephan Mertens, Marc Mézard, Riccardo Zecchina, <em>Threshold Values of Random K-SAT from the </em><em>Cavity Method</em>. 2005.<br/>
<a href="https://arxiv.org/pdf/cs/0309020.pdf">Link</a></p>
<p>[7] Andrea Montanari, Federico Ricci-Tersenghi, Guilhem Semerjian, <em>Clusters of solutions and replica symmetry breaking in </em><em>random k-satisfiability</em>. 2008.<br/>
<a href="https://arxiv.org/pdf/0802.3627.pdf">Link</a></p>
<p>A big thanks to Tselil for all the proof reading and recommendations, and to both Boaz and Tselil for their really detailed post-presentation feedback.</p></div></content><updated planet:format="October 20, 2018 12:30 PM">2018-10-20T12:30:18Z</updated><published planet:format="October 20, 2018 12:30 PM">2018-10-20T12:30:18Z</published><category term="physics"/><author><name>7omj0</name></author><source><id>https://windowsontheory.org</id><logo>https://s0.wp.com/i/buttonw-com.png</logo><link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/><link href="https://windowsontheory.org" rel="alternate" type="text/html"/><link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/><link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/><subtitle>A Research Blog</subtitle><title>Windows On Theory</title><updated planet:format="December 17, 2018 05:29 AM">2018-12-17T05:29:54Z</updated><planet:module>toc</planet:module><planet:format>atom10</planet:format><planet:http_last_modified>Mon, 17 Dec 2018 01:41:40 GMT</planet:http_last_modified><planet:bozo>false</planet:bozo><planet:items_per_page>40</planet:items_per_page><planet:css-id>windows-on-theory</planet:css-id><planet:face>wot.png</planet:face><planet:name>Windows on Theory</planet:name><planet:http_location>https://windowsontheory.org/feed/</planet:http_location><planet:http_status>301</planet:http_status></source></entry>
