<?xml version="1.0" ?><entry xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/"><id>tag:blogger.com,1999:blog-21224994.post-2722377535665181868</id><link href="https://research.googleblog.com/feeds/2722377535665181868/comments/default" rel="replies" type="application/atom+xml"/><link href="https://research.googleblog.com/2017/12/introducing-new-foveation-pipeline-for.html#comment-form" rel="replies" type="text/html"/><link href="https://www.blogger.com/feeds/21224994/posts/default/2722377535665181868" rel="edit" type="application/atom+xml"/><link href="https://www.blogger.com/feeds/21224994/posts/default/2722377535665181868" rel="self" type="application/atom+xml"/><link href="https://research.googleblog.com/2017/12/introducing-new-foveation-pipeline-for.html" rel="alternate" type="text/html"/><title>Introducing a New Foveation Pipeline for Virtual/Mixed Reality</title><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><span class="byline-author">Posted by Behnam Bastani, Software Engineer Manager and Eric Turner, Software Engineer, Daydream</span><br/><br/><a href="https://en.wikipedia.org/wiki/Virtual_reality">Virtual Reality</a> (VR) and <a href="https://en.wikipedia.org/wiki/Mixed_reality">Mixed Reality</a> (MR) offer a novel way to immerse people into new and compelling experiences, from gaming to professional training. However, current VR/MR technologies present a fundamental challenge: to present images at the extremely high resolution required for immersion places enormous demands on the rendering engine and transmission process. Headsets often have insufficient display resolution, which can limit the field of view, worsening the experience. But, to drive a higher resolution headset, the traditional rendering pipeline requires significant processing power that even high-end mobile processors cannot achieve. As <a href="https://www.youtube.com/watch?v=IlADpD1fvuA&amp;feature=youtu.be&amp;t=1330">research</a> continues to deliver promising new techniques to increase display resolution, the challenges of driving those displays will continue to grow. <br/><br/>In order to further improve the visual experience in VR and MR, we introduce a pipeline that takes advantage of the characteristics of human visual perception to enable an amazing visual experience at low compute and power cost. The pipeline proposed in this article considers the full system dependency including the rendering engine, memory bandwidth and capability of display module itself. We determined that the current limitation is not just in the content creation, but it also may be in transmitting data, handling latency and enabling interaction with real objects (mixed reality applications). The pipeline consists of 1. Foveated Rendering with a focus on reducing of compute per pixel. 2. Foveated Image Processing with a focus on the reduction of visual artifacts and 3. Foveated Transmission with a focus on bits per pixel transmitted.<br/><br/><b>Foveated Rendering</b><br/>In the human visual system, the <a href="https://en.wikipedia.org/wiki/Fovea_centralis">fovea centralis</a> allows us to see at high-fidelity in the center of our vision, allowing our brain to pay less attention to things in our peripheral vision. Foveated rendering takes advantage of this characteristic to improve the performance of the rendering engine by reducing the spatial or bit-depth resolution of objects in our peripheral vision. To make this work, the location of the High Acuity (HA) region needs to be updated with eye-tracking to align with eye <a href="https://en.wikipedia.org/wiki/Saccade">saccades,</a> which preserves the perception of a constant high-resolution across the field of view. In contrast, systems with no eye-tracking may need to render a much larger HA region. <br/><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="https://4.bp.blogspot.com/-dEriDuKV8-I/WiXZvlusJFI/AAAAAAAACPw/7I-Aih2eRlM6NCIaRRDLT5QMqR4knJTlwCLcBGAs/s1600/image2.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="202" src="https://4.bp.blogspot.com/-dEriDuKV8-I/WiXZvlusJFI/AAAAAAAACPw/7I-Aih2eRlM6NCIaRRDLT5QMqR4knJTlwCLcBGAs/s640/image2.png" width="640"/></a></td></tr><tr><td class="tr-caption" style="text-align: center;">The left image is rendered at full resolution. The right image uses two layers of foveation — one rendered at high resolution (inside the yellow region) and one at lower resolution (outside).</td></tr></tbody></table>A traditional foveation technique may divide a frame buffer into multiple spatial resolution regions. Aliasing introduced by rendering to lower spatial resolution may cause perceptible temporal artifacts when there is motion in the content due to head motion or animation. Below we show an example of temporal artifacts introduced by head rotation. <br/><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="https://2.bp.blogspot.com/-MQDessThj78/WiXZctBZGKI/AAAAAAAACPs/GLERcZMyhzkfv8o4iBh3C3zjKL_L7FZAgCLcBGAs/s1600/image8.gif" style="margin-left: auto; margin-right: auto;"><img border="0" height="187" src="https://2.bp.blogspot.com/-MQDessThj78/WiXZctBZGKI/AAAAAAAACPs/GLERcZMyhzkfv8o4iBh3C3zjKL_L7FZAgCLcBGAs/s640/image8.gif" width="640"/></a></td></tr><tr><td class="tr-caption" style="text-align: center;">A smooth full rendering (image on the left). The image on the right shows temporal artifacts introduced by motion in foveated region.</td></tr></tbody></table>In the following sections, we present two different methods we use aimed at reducing these artifacts: <i>Phase-Aligned Foveated Rendering</i> and <i>Conformal Foveated Rendering</i>. Each of these methods provide different benefits for visual quality during rendering and are useful under different conditions.<br/><br/><b>Phase-Aligned Rendering</b><br/>Aliasing occurs in the Low-Acuity (LA) region during foveated rendering due to the subsampling of rendered content. In traditional foveated rendering discussed above, these aliasing artifacts flicker from frame to frame, since the display pixel grid moves across the virtual scene as the user moves their head. The motion of these pixels relative to the scene cause any existing aliasing artifacts to flicker, which is highly perceptible to the user, even in the periphery.<br/><br/>In Phase-Aligned rendering, we force the LA region <a href="https://en.wikipedia.org/wiki/Frustum">frustums</a> to be aligned rotationally to the world (e.g. always facing north, east, south, etc.), not the current frame's head-rotation. The aliasing artifacts are mostly invariant to head pose and therefore much less detectable. After upsampling, these regions are then reprojected onto the final display screen to compensate for the user's head rotation, which reduces temporal flicker. As with traditional foveation, we render the high-acuity region in a separate pass, and overlay it onto the merged image at the location of the fovea. The figure below compares traditional foveated rendering with phase-aligned rendering, both at the same level of foveation.<br/><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="https://4.bp.blogspot.com/-fQo6pdqdq4Y/WiXY7s8q5_I/AAAAAAAACPg/UimtDf06AVQpuv2OvduxWptPs7tOFx-bgCLcBGAs/s1600/PA.gif" style="margin-left: auto; margin-right: auto;"><img border="0" height="192" src="https://4.bp.blogspot.com/-fQo6pdqdq4Y/WiXY7s8q5_I/AAAAAAAACPg/UimtDf06AVQpuv2OvduxWptPs7tOFx-bgCLcBGAs/s640/PA.gif" width="640"/></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Temporal artifacts in non-world aligned foveated rendered content (left) and the phase-aligned method (right).</td></tr></tbody></table>This method gives a major benefit to reducing the severity of visual artifacts during foveated rendering. Although phase-aligned rendering is more expensive to compute than traditional foveation under the same level of acuity reduction, we can still yield a net savings by pushing foveation to more aggressive levels that would otherwise have yielded too many artifacts.<br/><b><br/></b> <b>Conformal Rendering</b><br/>Another approach for foveated rendering is to render content in a space that matches the smoothly varying reduction in resolution of our visual acuity, based on a nonlinear mapping of the screen distance from the visual fixation point.<br/><br/>This method gives two main benefits. First, by more closely matching the visual fidelity fall-off of the human eye, we can reduce the total number of pixels computed compared to other foveation techniques. Second, by using a smooth fall-off in fidelity, we prevent the user from seeing a clear dividing line between High-Acuity and Low-Acuity, which is often one of the first artifacts that is noticed. These benefits allow for aggressive foveation to be used while preserving the same quality levels, yielding more savings.<br/><br/>We perform this method by warping the vertices of the virtual scene into non-linear space. This scene is then <a href="https://en.wikipedia.org/wiki/Rasterisation">rasterized </a>at a reduced resolution, then unwarped into linear space as a post-processing effect combined with lens distortion correction.<br/><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="https://2.bp.blogspot.com/-Hbb5CAdi_Fo/WiXYItN9IPI/AAAAAAAACPY/CCcKuVoy6uIk4mrhdEFW8Yv9rHySTd2yQCLcBGAs/s1600/conformal.gif" style="margin-left: auto; margin-right: auto;"><img border="0" height="187" src="https://2.bp.blogspot.com/-Hbb5CAdi_Fo/WiXYItN9IPI/AAAAAAAACPY/CCcKuVoy6uIk4mrhdEFW8Yv9rHySTd2yQCLcBGAs/s640/conformal.gif" width="640"/></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Comparison of traditional foveation (left) to conformal rendering (right), where content is rendered to a space matched to visual perception acuity and HMD lens characteristics. Both methods use the same number of total pixels.</td></tr></tbody></table>A major benefit of this method over the phase-aligned method above is that conformal rendering only requires a single pass of rasterization. For scenes with lots of vertices, this difference can provide major savings. Additionally, although phase-aligned rendering reduces flicker, it still produces a distinct boundary between the high- and low-acuity regions, whereas conformal rendering does not show this artifact. However, a downside of conformal rendering compared to phase-alignment is that aliasing artifacts still flicker in the periphery, which may be less desirable for applications that require high visual fidelity.<br/><b><br/></b> <b>Foveated Image Processing</b><br/>HMDs often require image processing steps to be performed after rendering, such as local tone mapping, lens distortion correction, or lighting blending. With foveated image-processing, different operations are applied for different foveation regions. As an example, lens distortion correction, including chromatic aberration correction, may not require the same spatial accuracy for each part of the display. By running lens distortion correction on foveated content before upscaling, significant savings are gained in computation. This technique does not introduce perceptible artifacts.<br/><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-sBfSyV35XkE/WiXV5ywtFUI/AAAAAAAACPM/BMRtcy5glwgaLhv8BRJUQg691yjXbeNtgCLcBGAs/s1600/image4.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="418" src="https://1.bp.blogspot.com/-sBfSyV35XkE/WiXV5ywtFUI/AAAAAAAACPM/BMRtcy5glwgaLhv8BRJUQg691yjXbeNtgCLcBGAs/s640/image4.png" width="640"/></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Correction for head-mounted-display lens chromatic aberration in foveated space. Top image shows the conventional pipeline. The bottom image (in Green) shows the operation in the foveated space.</td></tr></tbody></table><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="https://3.bp.blogspot.com/-bQ6K13SFUPQ/WiXVpgTODXI/AAAAAAAACPI/ivMiJ4hxGYkES0pbARCh-xQ911jkkB2QwCLcBGAs/s1600/image1.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="208" src="https://3.bp.blogspot.com/-bQ6K13SFUPQ/WiXVpgTODXI/AAAAAAAACPI/ivMiJ4hxGYkES0pbARCh-xQ911jkkB2QwCLcBGAs/s640/image1.png" width="640"/></a></td></tr><tr><td class="tr-caption" style="text-align: center;">The left image shows reconstructed foveated content after lens distortion. The right image shows image difference when lens distortion correction is performed in a foveated manner. The right image shows that minimal error is introduced close to edges of frame buffer. These errors are imperceptible in an HMD.</td></tr></tbody></table><b><br/></b> <b>Foveated Transmission</b><br/>A non-trivial source of power consumption for standalone HMDs is data transmission from the <a href="https://en.wikipedia.org/wiki/System_on_a_chip">system-on-a-chip</a> (SoC) to the display module. Foveated transmission aims to save power and bandwidth by transmitting the minimum amount of data necessary to the display as shown in figure below. <br/><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-clL3tevDVLw/WiXVaXSsaaI/AAAAAAAACPA/gSRRc7aluLIQxkrrX1Wl3mZGAWw4YPDdwCLcBGAs/s1600/image5.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="402" src="https://1.bp.blogspot.com/-clL3tevDVLw/WiXVaXSsaaI/AAAAAAAACPA/gSRRc7aluLIQxkrrX1Wl3mZGAWw4YPDdwCLcBGAs/s640/image5.png" width="640"/></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Rather than streaming upscaled foveated content (left image), foveated transmission enables streaming content pre-reconstruction (right image) and reducing the number of bits transmitted.</td></tr></tbody></table>This change requires moving the simple upscaling and blending operations to the display side and transmitting only the foveated rendered content. Complexity arises if the foveal region, the red box in above figure, moves with eyetracking. Such motion may cause temporal artifacts (figure below) since Display Stream Compression (DSC) used between SoC and the display is not designed for foveated content. <br/><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="https://2.bp.blogspot.com/-ilSskhoTTi8/WiXVIU6SaoI/AAAAAAAACO8/5M2Qh9NB3G8-QUoSPg7a7vrujoX2zFsRQCLcBGAs/s1600/image3.gif" style="margin-left: auto; margin-right: auto;"><img border="0" height="300" src="https://2.bp.blogspot.com/-ilSskhoTTi8/WiXVIU6SaoI/AAAAAAAACO8/5M2Qh9NB3G8-QUoSPg7a7vrujoX2zFsRQCLcBGAs/s640/image3.gif" width="640"/></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Comparison of full integration of foveation and compression techniques (left) versus typical flickering artifacts that may be introduced by applying DSC to foveated content (right).</td></tr></tbody></table><b>Toward a New Pipeline</b><br/>We have focused on a few components of a “foveation pipeline” for MR and VR applications. By considering the impact of foveation in every part of a display system — rendering, processing and transmission — we can enable the next generation of lightweight, low-power, and high resolution MR/VR HMDs. This topic has been an active area of research for many years and it seems reasonable to expect the appearance of VR and MR headsets with foveated pipelines in the coming years. <br/><br/><b>Acknowledgements</b><br/><i>We would like to recognize the work done by the following collaborators:</i><br/><ul><li><i>Haomiao Jiang and Carlin Vieri on display compression and foveated transmission</i></li><li><i>Brian Funt and Sylvain Vignaud on the development of new foveated rendering algorithms</i></li></ul></div></content><updated planet:format="December 05, 2017 06:00 PM">2017-12-05T18:00:00Z</updated><published planet:format="December 05, 2017 06:00 PM">2017-12-05T18:00:00Z</published><category scheme="http://www.blogger.com/atom/ns#" term="Algorithms"/><category scheme="http://www.blogger.com/atom/ns#" term="Image Processing"/><category scheme="http://www.blogger.com/atom/ns#" term="Virtual Reality"/><author><name>Google AI</name><email>noreply@blogger.com</email><uri>http://www.blogger.com/profile/12098626514775266161</uri></author><source><id>tag:blogger.com,1999:blog-21224994</id><category term="Machine Learning"/><category term="Google Brain"/><category term="Deep Learning"/><category term="Education"/><category term="University Relations"/><category term="TensorFlow"/><category term="Publications"/><category term="open source"/><category term="Computer Vision"/><category term="Research"/><category term="Natural Language Processing"/><category term="conference"/><category term="Neural Networks"/><category term="conferences"/><category term="Natural Language Understanding"/><category term="Research Awards"/><category term="MOOC"/><category term="Computer Science"/><category term="Machine Perception"/><category term="datasets"/><category term="Machine Intelligence"/><category term="YouTube"/><category term="Awards"/><category term="Algorithms"/><category term="Android"/><category term="Computational Photography"/><category term="Health"/><category term="Quantum Computing"/><category term="Speech"/><category term="Visualization"/><category term="ACM"/><category term="CVPR"/><category term="K-12"/><category term="Structured Data"/><category term="Earth Engine"/><category term="Machine Translation"/><category term="Security and Privacy"/><category term="Voice Search"/><category term="ph.d. fellowship"/><category term="Google Accelerated Science"/><category term="HCI"/><category term="Image Processing"/><category term="Search"/><category term="grants"/><category term="AI"/><category term="Collaboration"/><category term="Faculty Summit"/><category term="Graph Mining"/><category term="NIPS"/><category term="TTS"/><category term="Vision Research"/><category term="market algorithms"/><category term="statistics"/><category term="Course Builder"/><category term="Google Cloud Platform"/><category term="Google Genomics"/><category term="Google+"/><category term="Robotics"/><category term="Speech Recognition"/><category term="Translate"/><category term="UI"/><category term="User Experience"/><category term="WWW"/><category term="optimization"/><category term="ACL"/><category term="Fusion Tables"/><category term="Google Books"/><category term="Google Maps"/><category term="ICML"/><category term="Information Retrieval"/><category term="Moore's Law"/><category term="Ngram"/><category term="On-device Learning"/><category term="Physics"/><category term="data science"/><category term="renewable energy"/><category term="App Engine"/><category term="Art"/><category term="Chemistry"/><category term="Computational Imaging"/><category term="Diversity"/><category term="Europe"/><category term="Expander"/><category term="Gmail"/><category term="Google Play Apps"/><category term="Google Translate"/><category term="Hardware"/><category term="ICLR"/><category term="Image Classification"/><category term="Internet of Things"/><category term="Machine Hearing"/><category term="NLP"/><category term="Networks"/><category term="PhD Fellowship"/><category term="Pixel"/><category term="Semi-supervised Learning"/><category term="Software"/><category term="Virtual Reality"/><category term="accessibility"/><category term="crowd-sourcing"/><category term="distributed systems"/><category term="economics"/><category term="internationalization"/><category term="publication"/><category term="search ads"/><category term="wikipedia"/><category term="API"/><category term="Acoustic Modeling"/><category term="App Inventor"/><category term="Audio"/><category term="Automatic Speech Recognition"/><category term="China"/><category term="Cloud Computing"/><category term="Data Discovery"/><category term="DeepDream"/><category term="DeepMind"/><category term="EMEA"/><category term="Environment"/><category term="Exacycle"/><category term="Google Drive"/><category term="Google Science Fair"/><category term="Graph"/><category term="Inbox"/><category term="KDD"/><category term="Labs"/><category term="MapReduce"/><category term="Optical Character Recognition"/><category term="Policy"/><category term="Quantum AI"/><category term="Social Networks"/><category term="Supervised Learning"/><category term="Systems"/><category term="VLDB"/><category term="Video Analysis"/><category term="ads"/><category term="schema.org"/><category term="trends"/><category term="video"/><category term="Adaptive Data Analysis"/><category term="Africa"/><category term="Android Wear"/><category term="April Fools"/><category term="Augmented Reality"/><category term="Australia"/><category term="Cantonese"/><category term="Chrome"/><category term="Conservation"/><category term="Data Center"/><category term="EMNLP"/><category term="Electronic Commerce and Algorithms"/><category term="Encryption"/><category term="Entity Salience"/><category term="Faculty Institute"/><category term="Flu Trends"/><category term="Gboard"/><category term="Google Docs"/><category term="Google Photos"/><category term="Google Sheets"/><category term="Google Trips"/><category term="Google Voice Search"/><category term="Government"/><category term="High Dynamic Range Imaging"/><category term="ICSE"/><category term="IPython"/><category term="Image Annotation"/><category term="India"/><category term="Interspeech"/><category term="Journalism"/><category term="Keyboard Input"/><category term="Klingon"/><category term="Korean"/><category term="Linear Optimization"/><category term="Low-Light Photography"/><category term="ML"/><category term="Magenta"/><category term="Market Research"/><category term="Mixed Reality"/><category term="Multimodal Learning"/><category term="NAACL"/><category term="Network Management"/><category term="Nexus"/><category term="Peer Review"/><category term="PhotoScan"/><category term="PiLab"/><category term="Professional Development"/><category term="Proposals"/><category term="Public Data Explorer"/><category term="SIGCOMM"/><category term="SIGMOD"/><category term="Semantic Models"/><category term="Site Reliability Engineering"/><category term="Style Transfer"/><category term="TPU"/><category term="TV"/><category term="TensorBoard"/><category term="UNIX"/><category term="Visiting Faculty"/><category term="Wiki"/><category term="adsense"/><category term="adwords"/><category term="correlate"/><category term="electronics"/><category term="entities"/><category term="gamification"/><category term="jsm"/><category term="jsm2011"/><category term="localization"/><category term="operating systems"/><category term="osdi"/><category term="osdi10"/><category term="patents"/><category term="resource optimization"/><author><name>Unknown</name><email>noreply@blogger.com</email></author><link href="https://research.googleblog.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/><link href="https://www.blogger.com/feeds/21224994/posts/default/-/Algorithms" rel="self" type="application/atom+xml"/><link href="https://research.googleblog.com/search/label/Algorithms" rel="alternate" type="text/html"/><link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/><subtitle>The latest news from Research at Google</subtitle><title>Research Blog</title><updated planet:format="February 27, 2020 09:34 AM">2020-02-27T09:34:03Z</updated><planet:module>toc</planet:module><planet:format>atom10</planet:format><planet:http_etag>W/&quot;74bab70ddc23c603e713bf1869833c16d83475e8e02b320d22cc231ad4d1a831&quot;</planet:http_etag><planet:http_last_modified>Thu, 27 Feb 2020 09:34:03 GMT</planet:http_last_modified><planet:bozo>false</planet:bozo><planet:items_per_page>40</planet:items_per_page><planet:css-id>google-research-blog-algorithms</planet:css-id><planet:face>google.png</planet:face><planet:name>Google Research Blog: Algorithms</planet:name><planet:http_status>200</planet:http_status></source></entry>