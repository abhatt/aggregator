<?xml version="1.0" encoding="utf-8"?><entry xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/"><id>tag:blogger.com,1999:blog-4068183698747623113.post-3002787663081260018</id><link href="http://teachingintrotocs.blogspot.com/feeds/3002787663081260018/comments/default" rel="replies" type="application/atom+xml"/><link href="http://teachingintrotocs.blogspot.com/2012/06/avi-at-aofa-population-recovery.html#comment-form" rel="replies" type="text/html"/><link href="http://www.blogger.com/feeds/4068183698747623113/posts/default/3002787663081260018" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/4068183698747623113/posts/default/3002787663081260018" rel="self" type="application/atom+xml"/><link href="http://teachingintrotocs.blogspot.com/2012/06/avi-at-aofa-population-recovery.html" rel="alternate" type="text/html"/><title>Avi at AofA: population recovery</title><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Fans of &quot;The Land Before Time&quot; know that there are many kinds of dinosaurs: Apatosaurus, Tyrannosaurus, Triceratops, Saurolophus, Pteranodon, Stegosaurus, to mention a few. But is the movie an accurate representation of reality? Were they really in equal numbers, or were there, perhaps, more Triceratops dinosaurs? Enquiring minds want to know: what was the distribution of the various kinds of dinosaurs?    <p>Here is a way to bring the problem into the more familiar realm of mathematics. Think of each dinosaur as a vector, and of each bone as a coordinate. Paleontologists unearth random dinosaur bones, identifying them as they go (somewhat unreliably: errors are not unheard of), and digs can be viewed as &quot;queries&quot;. How can we mine the resulting data to recover the distribution of the dinosaur population? That's what Avi Wigderson talked about on Monday  at the Analysis of Algorithms conference (AofA) here in Montreal, presenting joint ork with Amir Yehudayoff. (If you don't care for dinosaurs, think &quot;Netflix&quot; instead.)    </p><p><b>The population recovery problem</b>: Given k n-dimensional binary vectors, recover a hidden distribution (pv) that draws vector v with probability pv. The hidden distribution is accessed via lossy, noisy queries. Each query draws a vector v with probability pv, and reveals a lossy and noisy version of v: lossy, meaning that the algorithm only sees a random 1 percent of the coordinates; noisy, meaning that each coordinate seen by the algorithm is flipped with probability 49 percent.     </p><p>The result: Avi gave an algorithm that estimates (p1,p2,…,pk) so that, (with high probability) for every v  the estimate for pv is at most an additive error epsilon away from the correct value. The number of queries is polynomial in n and in 1/epsilon (but slightly super polynomial in k).<br/>  </p><p>Here is the algorithm: <br/>(0) Take a few queries. <br/>(1) Find, for each vector v, a small set Sv of coordinates, such that the following condition holds: <br/>      (*) Sv uniquely distinguishes v from every vertex u such that |Su|≤|Sv|.  <br/>   Let Ambiguous(v) be the set of vectors u that are indistinguishable from v by looking at just the coordinates in Sv.  <br/>(2) Label the vectors 1,2,..,k by order of non-increasing |Sv| <br/>(3) For v = 1 to k <br/>   Let zv = the restriction of v to the coordinates in Sv. <br/>   Compute qv, the fraction of the queries whose Sv coordinates are (roughly) zv. <br/>    Let (**) estimate(pv)= qv- ∑ estimate(pu),  <br/>    where the sum is over every u in Ambiguous(v), other than v itself, of course <br/>Output (estimate(p1),estimate(p2),…,estimate(pk)).<br/>  </p><p>The first question that comes to the mind of the eager coder is the risk that the code may generate some &quot;undefined&quot; error: isn't there a risk of circularity? Step (3) computes estimates using estimates, but what if the estimates used have not yet been computed? <br/>It turns out that this is not an issue. Why? Because, in step (3), the sum is over u's that are in Ambiguous(v). But by condition (*) we must have |Su|&gt;|Sv|, and by step (2) they are been placed before v in the ordering, so estimate(pu) has already been computed by the time we need it in the right hand side. Hence, no circularity.   </p><p>The second question is that estimates have errors, inevitably, and when using the algebraic expression (**), those errors will accumulate. To make up for that, more queries will be needed. Now we are worried: in order to have reliable estimates, isn't this algorithm going to require lots and lots of queries? Well, no. The main idea is that the sets Sv constructed in step (1) are small: they all have cardinality at most log(k). Then the graph that has an arc (u,v) whenever estimate(pu) occurs in the right hand side of (**) is a partial order with depth at most log(k), and so errors don't accumulate.   </p><p>The main remaining difficulty is how to achieve (*) while maintaining sets Sv of cardinality at most log(k). Here is how to do it. <br/>  </p><p>Algorithm for Step (1): <br/>Start with the empty set for every Sv <br/>Repeat <br/>     While there exists an Sv and a coordinate i such that adding i to Sv decreases |Ambiguous(v)| by a factor of 2  <br/>         Add i to Sv   <br/>   If condition (*) is violated by some u and v,   <br/>      then replace Su by Sv <br/>Until condition (*) holds.   </p><p>Why does that work? By some elementary magic.</p></div><div class="commentbar"><p/><span class="commentbutton" href="http://teachingintrotocs.blogspot.com/feeds/3002787663081260018/comments/default"/><a href="http://teachingintrotocs.blogspot.com/feeds/3002787663081260018/comments/default"><img class="commenticon" src="/images/feed-icon.png"/> Subscribe to comments</a><![CDATA[  | ]]><a href="http://teachingintrotocs.blogspot.com/2012/06/avi-at-aofa-population-recovery.html#comment-form"><img class="commenticon" src="/images/post-icon.png"/> Post a comment</a></div></content><updated planet:format="June 20, 2012 10:14 AM">2012-06-20T10:14:00Z</updated><published planet:format="June 20, 2012 10:14 AM">2012-06-20T10:14:00Z</published><category scheme="http://www.blogger.com/atom/ns#" term="TCS"/><author><name>Claire Mathieu</name><email>noreply@blogger.com</email><uri>http://www.blogger.com/profile/10957755706440077623</uri></author><source><id>tag:blogger.com,1999:blog-4068183698747623113</id><category term="TCS"/><category term="Oops"/><category term="technology"/><author><name>Claire Mathieu</name><email>noreply@blogger.com</email><uri>http://www.blogger.com/profile/10957755706440077623</uri></author><link href="http://teachingintrotocs.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/4068183698747623113/posts/default/-/TCS" rel="self" type="application/atom+xml"/><link href="http://teachingintrotocs.blogspot.com/search/label/TCS" rel="alternate" type="text/html"/><link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/><title>A CS Professor's blog</title><updated planet:format="August 31, 2018 11:03 AM">2018-08-31T11:03:02Z</updated><planet:module>toc</planet:module><planet:format>atom10</planet:format><planet:http_etag>W/&quot;8089cd57e1113cac488458ecc6b57d2f5d63c2f2cb0051d1de6e12d23ed12d82&quot;</planet:http_etag><planet:http_last_modified>Fri, 31 Aug 2018 11:03:02 GMT</planet:http_last_modified><planet:bozo>false</planet:bozo><planet:items_per_page>40</planet:items_per_page><planet:css-id>claire-mathieu</planet:css-id><planet:face>mathieu.jpeg</planet:face><planet:name>Claire Mathieu</planet:name><planet:filters>category.py?cats=tcs</planet:filters><planet:http_status>200</planet:http_status></source></entry>

