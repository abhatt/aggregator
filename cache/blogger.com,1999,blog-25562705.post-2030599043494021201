<?xml version="1.0" ?><entry xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/"><id>tag:blogger.com,1999:blog-25562705.post-2030599043494021201</id><link href="http://aaronsadventures.blogspot.com/feeds/2030599043494021201/comments/default" rel="replies" type="application/atom+xml"/><link href="http://www.blogger.com/comment.g?blogID=25562705&amp;postID=2030599043494021201" rel="replies" type="text/html"/><link href="http://www.blogger.com/feeds/25562705/posts/default/2030599043494021201" rel="edit" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/25562705/posts/default/2030599043494021201" rel="self" type="application/atom+xml"/><link href="http://aaronsadventures.blogspot.com/2017/11/between-statistical-and-individual.html" rel="alternate" type="text/html"/><title>Between &quot;statistical&quot; and &quot;individual&quot; notions of fairness in Machine Learning</title><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">If you follow the fairness in machine learning literature, you might notice after awhile that there are two distinct families of fairness definitions:<br/><br/><ol><li>Statistical definitions of fairness (sometimes also called <i>group</i> fairness), and</li><li>Individual notions of fairness. </li></ol><div>The statistical definitions are far-and-away the most popular, but the individual notions of fairness offer substantially stronger semantic guarantees. In this post, I want to dive into this distinction a little bit. Along the way I'll link to some of my favorite recent fairness papers, and end with a description of a recent paper from our group which tries to get some of the best properties of both worlds.</div><div><br/></div><div><b><u>Statistical Definitions of Fairness</u></b></div><div>At a high level, statistical definitions of fairness partition the world into some set of &quot;protected groups&quot;, and then ask that some statistical property be approximately equalized across these groups. Typically, the groups are defined via some high level sensitive attribute, like race or gender. Then, statistical definitions of fairness ask that some relevant statistic about a classifier be equalized across those groups. For example, we could ask that the rate of positive classification be equal across the groups (this is sometimes called <i>statistical parity). </i>Or, we could ask that the false positive and false negative rates be equal across the groups (This is what <a href="http://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf">Hardt, Price, and Srebro</a> call &quot;Equalized Odds&quot;). Or, we could ask that the positive predictive value (also sometimes called calibration) be equalized across the groups.  These are all interesting definitions, and there are very interesting <a href="https://arxiv.org/abs/1703.00056">impossibility</a> <a href="https://arxiv.org/abs/1609.05807">results </a>that emerge if you think you might want to try and satisfy all of them simultaneously.</div><div><br/></div><div>These definitions are attractive: first of all, you can easily check whether a classifier satisfies them, since verifying that these fairness constraints hold on a particular distribution simply involves estimating some expectations, one for each protected group. It can be <a href="https://arxiv.org/abs/1702.06081">computationally hard</a> to find a classifier that satisfies them, while minimizing the usual convex surrogate losses --- but computational hardness pervades all of learning theory, and hasn't stopped the practice of machine learning. Already there are a number of practical algorithms for learning classifiers subject to these statistical fairness constraints --- this <a href="http://fatml.mysociety.org/media/documents/reductions_approach_to_fair_classification.pdf">recent paper</a> by Agarwal et al. is one of my favorites. </div><div><br/></div><div><i>Semantics </i></div><div>But what are the semantics of these constraints, and why do they correspond to &quot;fairness&quot;? You can make a different argument for each one, but here is the case for equalizing false positive and negative rates across groups (equalized odds), which is one of my favorites: </div><div><br/></div><div>You can argue (making the <i>big</i> assumption that the data is not itself already corrupted with bias --- see <a href="https://arxiv.org/abs/1609.07236">this paper</a> by Friedler et al. for a deep dive into these assumptions...) that a classifier potentially does harm to an individual when it misclassifies them. If the (say) false positive rate is higher amongst black people than white people, then this means that if you perform the thought experiment of imagining yourself as a uniformly random black person with a negative label, the classifier is more likely to harm you than if you imagine yourself as a uniformly random white person with a negative label. On the other hand, if the classifier equalizes false positive and negative rates across populations, then behind the &quot;veil of ignorance&quot;, choosing whether you want to be born as a uniformly random black or white person (conditioned on having a negative label), all else being equal, you should be indifferent!</div><div><br/></div><div><i>Concerns</i></div><div>However, in this thought experiment, it is crucial that you are a uniformly random individual from the population. If you know some fact about yourself that makes you think you are not a uniformly random white person, the guarantee can lack bite. Here is a simple example:</div><div><br/></div><div>Suppose we have a population of individuals, with two features and a label. Each individual has a race (either black or white) and a gender (either male or female). The features are uniformly random and independent. Each person also has a label (positive or negative) that is also uniformly random, and independent of the features. We can define four natural protected groups: &quot;Black people', &quot;White people&quot;, &quot;Men&quot;, and &quot;Women&quot;. </div><div><br/></div><div>Now suppose we have a classifier that labels an individual as positive if and only if they are either a black man or a white woman. Across these four protected groups, this classifier appears to be fair: the false positive rate and false negative rate are all exactly 50% on all four of the protected groups, so it satisfies equalized odds. But this hides a serious problem: on two subgroups (black men and white women), the false positive rate is 100%! If being labelled positive is a bad thing, you would not want to be a random (negative) member of those sub-populations.</div><div><br/></div><div>Of course, we could remedy this problem by simply adding more explicitly protected subgroups. But this doesn't scale well: If we have d protected binary features, there are 2^(2^d) such subgroups that we can define. If the features are real valued, things are even worse, and we quickly get into issues of overfitting (see the next section).   </div><div><br/></div><div><b><u>Individual Definitions of Fairness</u></b></div><div>At a high level, the problem with statistical definitions of fairness is that they are defined with respect to group averages, so if you are not an &quot;average&quot; member of your group, they don't promise you anything. Individual definitions of fairness attempt to remedy this problem by asking for a constraint that binds at the individual level. The first attempt at this was in the seminal paper <a href="https://dl.acm.org/citation.cfm?id=2090255">&quot;Fairness Through Awareness&quot;</a>, which suggested that fairness should mean that &quot;similar individuals should be treated similarly&quot;. Specifically, if the algorithm designer knows the correct metric by which individual similarity should be judged, the constraint requires that individuals who are close in that metric should have similar probabilities of any classification outcome. This is a semantically very strong definition. It hasn't gained much traction in the literature yet, because of the hard problem of specifying the similarity metric. But I expect that it will be an important definition going forward --- there are just some obstacles that have to be overcome. I've been thinking about it a lot myself. </div><div><br/></div><div>Another definition of individual fairness is one that my colleagues and I at Penn introduced in <a href="https://arxiv.org/abs/1605.07139">this paper</a>, which we have taken to calling &quot;Weakly Meritocratic Fairness&quot;. I have previously blogged about it <a href="https://aaronsadventures.blogspot.com/2016/05/fairness-in-learning.html">here</a>. We define it in a more general context, and in an online setting, but it makes sense to think about what it means in the special case of batch binary classification as well. In that case, it reduces to three constraints:</div><div><ol><li>For any two individuals with a negative label, the false positive rate on those individuals must be equal</li><li>For any two individuals with a positive label, the false negative rate on those individuals must be equal</li><li>For any pair of individuals A and B such that A has a negative label, and B has a positive label, the true positive rate on B can't be lower than the false positive rate on A. (Assuming that positive is the more desirable label --- otherwise this is reversed). </li></ol><div>Again, we are talking about false positive rates --- but now there is no distribution --- i.e. nothing to take an average over. Instead, the constraints bind on every pair of individuals. As a result, the &quot;rate&quot; we are talking about is calculated only over the randomness of the classifier itself. </div></div><div><br/></div><div>This constraint in particular implies that false positive rates are equal across any sub-population that you might define, even ex-post! Problem solved! Why don't we all just use this definition of fairness? Here is why:</div><div><br/></div><div><i>Concerns</i>:</div><div>Weakly meritocratic fairness is extremely similar (identical, except for the added condition 3) to asking for equalized odds fairness on the (potentially infinitely large) set of &quot;protected groups&quot; corresponding to singletons --- everyone forms their own protected group! As a result, you shouldn't hope to be able to satisfy a fairness definition like this without making some assumption on the data generating process. Even if on your sample of data, it <i>looks like</i> you are satisfying this definition of fairness, you are overfitting --- the complexity of this class of groups is too large, and your in-sample &quot;fairness&quot; won't generalize out of sample. If you <i>are</i> willing to make assumptions on the data generating process, then you can do it! In our original paper, we show how to do it if you assume the labels obey a linear relationship to the features. But assumptions of this sort generally won't hold exactly in practice, which makes this kind of approach difficult to implement in practical settings. </div><div><br/></div><div>The &quot;Fairness Through Awareness&quot; definition at a high level suffers from the same kind of problem: you can only use it if you make a very strong assumption (namely that you have what everyone agrees is the &quot;right&quot; fairness metric). And this is the crux of the problem with definitions of individual fairness: they seem to require such strong assumptions, that we cannot actually realize them in practice. </div><div><br/></div><div><b><u>A Middle Ground</u></b></div><div>Asking for statistical fairness across a small number of coarsely defined groups leaves us open to unfairness on structured subgroups we didn't explicitly designate as protected (what you might call &quot;Fairness Gerrymandering&quot;).  On the other hand, asking for statistical fairness across every possible division of the data that can be defined ex-post leaves us with an impossible statistical problem. (Consider, that every imperfect classifier could be accused of being &quot;unfair&quot; to the subgroup that we define, ex-post, to be the set of individuals that the classifier misclassifies! But this is just overfitting...)</div><div><br/></div><div>What if we instead ask for statistical fairness across exponentially many (or infinitely many) subgroups, defined over a set of features we think should be protected, but ask that this family of subgroups itself has bounded VC-dimension? This mitigates the statistical problem --- we can now in principle train &quot;fair&quot; classifiers according to this definition without worrying about overfitting, assuming we we have enough data (proportional to the VC-dimension of the set of groups we want to protect, and the set of classifiers we want to learn). And we can do this without making any assumptions about the data. It also mitigates the &quot;Fairness Gerrymandering&quot; problem --- at least now, we can explicitly protect an enormous number of detailed subgroups, not just coarsely defined ones. </div><div><br/></div><div>This is what we (<a href="http://www.cis.upenn.edu/~mkearns/">Michael Kearns</a>, <a href="https://sethstatistics.wordpress.com/">Seth Neel</a>, <a href="https://www-users.cs.umn.edu/~zsw/">Steven Wu</a>, and I) propose in our <a href="https://arxiv.org/abs/1711.05144v1">new paper</a>. In most of the paper, we investigate the computational challenges surrounding this kind of fairness constraint, when the statistical fairness notion we want is equality of false positive rates, false negative rates, or classification rates (statistical parity):</div><div><ul><li>First, it is no longer clear how to even <i>check</i> whether a fixed classifier satisfies a fairness constraint of this sort, without explicitly enumerating all of the protected groups (and recall, there might now be exponentially or even uncountably infinitely many such subgroups). We call this the <i>Auditing</i> problem. And indeed, in the worst case, this is a hard problem: we show that it is equivalent to weak agnostic learning, which brings with it a long list of computational hardness results from learning theory. It is hard to audit even for simple subgroup structures, definable by boolean conjunctions over features, or linear threshold functions. However, this connection also suggests an algorithmic approach to auditing. The fact that learning linear separators is NP hard in the worst case hasn't stopped us from doing this all the time, with simple heuristics like logistic regression and SVMs, as well as more sophisticated techniques. These same heuristics can be used to solve the auditing problem. </li><li>Going further, lets suppose those heuristics work --- i.e. we have oracles which can optimally solve agnostic learning problems over some collection of classifiers C, and can optimally solve the auditing problem over some class of subgroups G. Then we give an algorithm that only has to maintain small state, and provably converges to the optimal distribution over classifiers in C  that equalizes false positive rates (or false negative rates, or classification rates...) over the groups G. Our algorithm draws inspiration from the Agarwal et al. paper: &quot;<a href="http://fatml.mysociety.org/media/documents/reductions_approach_to_fair_classification.pdf">A Reductions Approach to Fair Classification</a>&quot;. </li><li>And we can run these algorithms! We use a simple linear regression heuristic to implement both the agnostic learning and auditing &quot;oracles&quot;, and run the algorithm to learn a distribution over linear threshold functions (defined over 122 attributes) that approximately equalizes false positive rates across every group definable by a linear threshold function over 18 real valued racially associated attributes, in the &quot;Communities and Crime&quot; dataset. It seems to work and do well! </li></ul></div><div><br/></div><div><b><u>In Conclusion...</u></b></div><div>I've long been bothered by the seemingly irreconcilable gap between individual and group notions of fairness. The individual notions of fairness have the right semantics, but they are unimplementable. The group notions of fairness promise something too weak. Going forward, I think these two schools of thought on fairness have to be reconciled. I think of our work as a small step in that direction.<br/><br/><div style="text-align: center;"><b>Michael recorded a short talk about this work, which you can watch on YouTube: <a href="https://www.youtube.com/watch?v=AjGQHJ0FKMg">https://www.youtube.com/watch?v=AjGQHJ0FKMg</a></b></div></div></div></content><updated planet:format="November 15, 2017 08:09 PM">2017-11-15T20:09:00Z</updated><published planet:format="November 15, 2017 08:09 PM">2017-11-15T20:09:00Z</published><author><name>Aaron</name><email>noreply@blogger.com</email><uri>http://www.blogger.com/profile/09952936358739421126</uri></author><source><id>tag:blogger.com,1999:blog-25562705</id><category term="game theory"/><category term="news"/><author><name>Aaron</name><email>noreply@blogger.com</email><uri>http://www.blogger.com/profile/09952936358739421126</uri></author><link href="http://aaronsadventures.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/><link href="http://www.blogger.com/feeds/25562705/posts/default" rel="self" type="application/atom+xml"/><link href="http://aaronsadventures.blogspot.com/" rel="alternate" type="text/html"/><link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/><link href="http://www.blogger.com/feeds/25562705/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/><title>Adventures in Computation</title><updated planet:format="February 28, 2020 02:50 AM">2020-02-28T02:50:20Z</updated><planet:module>toc</planet:module><planet:format>atom10</planet:format><planet:http_etag>W/&quot;54cfd69ca9a6a1248bbf09d7869e70e6fa99df3b840c0db1bfca393f04fae9e8&quot;</planet:http_etag><planet:http_last_modified>Fri, 28 Feb 2020 02:50:20 GMT</planet:http_last_modified><planet:bozo>false</planet:bozo><planet:items_per_page>40</planet:items_per_page><planet:css-id>aaron-roth</planet:css-id><planet:face>roth.jpeg</planet:face><planet:name>Aaron Roth</planet:name><planet:http_status>200</planet:http_status></source></entry>