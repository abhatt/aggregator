<?xml version="1.0" encoding="utf-8"?><entry xml:lang="en-US" xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/"><id>https://blogs.princeton.edu/imabandit/?p=1335</id><link href="https://blogs.princeton.edu/imabandit/2018/09/18/how-to-generalize-algorithmically/" rel="alternate" type="text/html"/><title>How to generalize (algorithmically)</title><summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A couple of months ago I taught an introduction to statistical learning theory. I took inspiration from two very good introductory books on SLT: “Foundations of ML”, and “Understanding Machine Learning: From Theory to Algorithms”. I also covered some classical … <a href="https://blogs.princeton.edu/imabandit/2018/09/18/how-to-generalize-algorithmically/">Continue reading <span class="meta-nav">→</span></a></p></div><div class="commentbar"><p/></div></summary><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A couple of months ago I taught an introduction to statistical learning theory. I took inspiration from two very good introductory books on SLT: <a class="liinternal" href="https://www.amazon.com/dp/B009093G7Q/ref=dp-kindle-redirect?_encoding=UTF8&amp;btkr=1">“Foundations of ML”</a>, and <a class="liinternal" href="https://www.amazon.com/Understanding-Machine-Learning-Theory-Algorithms-ebook/dp/B00J8LQU8I/ref=sr_1_3?s=digital-text&amp;ie=UTF8&amp;qid=1537278314&amp;sr=1-3&amp;keywords=understanding+machine+learning">“Understanding Machine Learning: From Theory to Algorithms”</a>. I also covered some classical results about nearest neighbors from the seminal book <a class="liinternal" href="https://www.amazon.com/Probabilistic-Recognition-Stochastic-Modelling-Probability/dp/0387946187">“A Probabilistic Theory of Pattern Recognition”</a>. The lectures were recorded and I put the videos on <a class="liinternal" href="https://www.youtube.com/sebastienbubeck">the youtube channel</a>. Sadly I still did not learn the basic lesson to stick to a black pen when teaching on a whiteboard, so at (many) times it is hard to read what I write. I also got sick after the first lecture so my voice is quite weak in the following lectures, making it hard to hear too… Despite these fatal flaws some people still found the material useful, so check it out if you are interested. Below I give a rough syllabus for the lectures, and I also give some notes on “algorithmic generalization” which was in my opinion the highlight of the lectures (you can also check my earlier notes on SLT <a class="liinternal" href="https://blogs.princeton.edu/imabandit/2015/10/13/crash-course-on-learning-theory-part-1/">here</a> and <a class="liinternal" href="https://blogs.princeton.edu/imabandit/2015/10/22/crash-course-on-learning-theory-part-2/">here</a>).</p>
<p><strong>Syllabus</strong></p>
<p><a class="liinternal" href="https://www.youtube.com/watch?v=jX7Ky76eI7E">Lecture 1</a>: Introduction to the statistical learning theory framework, its basic question (sample complexity) and its canonical settings (linear classification, linear regression, logistic regression, SVM, neural networks). Two basic methods for learning: (i) Empirical Risk Minimization, (ii) Nearest neighbor classification.</p>
<p><a class="liinternal" href="https://www.youtube.com/watch?v=Ze95sGXkdGE">Lecture 2</a>: Uniform law of large numbers approach to control the sample complexity of ERM (includes a brief reminder of concentration inequalities). Application: analysis of bounded regression (includes the non-standard topic of type/cotype and how it relates to different regularizations such as in LASSO).</p>
<p><a class="liinternal" href="https://www.youtube.com/watch?v=B5xidqMPJa0">Lecture 3</a>: Reminder of the first two lectures and relation with the famous VC dimension. How to generalize beyond uniform law of large numbers: stability and robustness approaches (see below).</p>
<p><a class="liinternal" href="https://www.youtube.com/watch?v=BZlq_Wd2q8k">Lecture 4</a>: How to generalize beyond uniform law of large numbers: information theoretic perspective (see below), PAC-Bayes, and online learning. Brief discussion of margin theory, and an introduction to modern questions in robust machine learning.</p>
<p> </p>
<p><strong>Some notes on algorithmic generalization</strong></p>
<p>Let <img alt="\mathcal{X}, \mathcal{Y}" class="ql-img-inline-formula " height="16" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-0afce3280f1a0d7203b1094700241036_l3.png?resize=36%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="36"/> be input/output spaces. Let <img alt="\ell : \mathcal{Y} \times \mathcal{Y} \rightarrow [0,1]" class="ql-img-inline-formula " height="18" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-635a207403ddf273acecb8fe100808b9_l3.png?resize=132%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="132"/> be a loss function, <img alt="\mu" class="ql-img-inline-formula " height="12" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-243abb230e11149a610dd2033f7db411_l3.png?resize=11%2C12&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="11"/> a probability measure supported on <img alt="\mathcal{X} \times \mathcal{Y}" class="ql-img-inline-formula " height="15" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-716b2fd03be87c276f8e6235b9e08084_l3.png?resize=50%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="50"/>, and <img alt="\underline{h} : (\mathcal{X} \times \mathcal{Y})^m \rightarrow \mathcal{Y}^\mathcal{X}" class="ql-img-inline-formula " height="19" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-2652300770afc3d1a6ddbe347c4f98a7_l3.png?resize=155%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="155"/> a learning rule (in words <img alt="\underline{h}" class="ql-img-inline-formula " height="16" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-17dfdaa6f0eb1935e19164615973c6fe_l3.png?resize=11%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="11"/> takes as input a dataset of <img alt="m" class="ql-img-inline-formula " height="8" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c9892c160739d7b5be03e1d300d29a2b_l3.png?resize=15%2C8&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="15"/> examples, and output a mapping from <img alt="\mathcal{X}" class="ql-img-inline-formula " height="12" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e44d6dd2d58e906a7f3ec11d7f3cac9c_l3.png?resize=15%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="15"/>-inputs to <img alt="\mathcal{Y}" class="ql-img-inline-formula " height="15" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b6ef78bbfc7645cb3130c48ff568854a_l3.png?resize=13%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="13"/>-outputs). With a slight abuse of notation, for <img alt="z=(x,y) \in \mathcal{X} \times \mathcal{Y}" class="ql-img-inline-formula " height="18" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-38f698d554014e703603b8390de7d6e0_l3.png?resize=145%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="145"/> and <img alt="h \in \mathcal{Y}^{\mathcal{X}}" class="ql-img-inline-formula " height="18" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-0c34826415e6a0ee3e56f6c693c7e28a_l3.png?resize=57%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="57"/>, we write <img alt="\ell(h, z) := \ell(h(x), y)" class="ql-img-inline-formula " height="18" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-fa50f017575b857d3d8b3ed8ff709d92_l3.png?resize=148%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="148"/>. We define the generalization of <img alt="\underline{h}" class="ql-img-inline-formula " height="16" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-17dfdaa6f0eb1935e19164615973c6fe_l3.png?resize=11%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="11"/> on <img alt="\mu" class="ql-img-inline-formula " height="12" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-243abb230e11149a610dd2033f7db411_l3.png?resize=11%2C12&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="11"/> by:</p>
<p class="ql-center-displayed-equation" style="line-height: 54px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[ \mathrm{gen}(\underline{h} , \mu) = \mathbb{E}_{S=(z_1,\hdots,z_m) \sim \mu^{\otimes m}, z \sim \mu} \left[ \ell(\underline{h}(S), z) - \frac1{m} \sum_{i=1}^m \ell(\underline{h}(S), z_i) \right] \,. \]" class="ql-img-displayed-equation " height="54" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-f4f8f5dc740c9cbe021148c9aabcd174_l3.png?resize=515%2C54&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="515"/></p>
<p>In words, if <img alt="\mathrm{gen}(\underline{h} , \mu) \leq \epsilon" class="ql-img-inline-formula " height="18" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-7061308bc28857f686bc89bf8531a514_l3.png?resize=100%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="100"/> then we expect the empirical performance of the learned classifier <img alt="\underline{h}(S)" class="ql-img-inline-formula " height="18" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-fe921b7e94a794b68fd18ed7a6c5b938_l3.png?resize=35%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="35"/> to be representative of its performance on a fresh out-of-sample data point, up to an additive <img alt="\epsilon" class="ql-img-inline-formula " height="8" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-bd8c92db9d4710285ccbc2b75c276150_l3.png?resize=7%2C8&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="7"/>. The whole difficulty of course is that the empirical evaluation is done with the <em>same </em>dataset <img alt="S" class="ql-img-inline-formula " height="12" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-6046e01a81406b0611d267d68b760dfb_l3.png?resize=12%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="12"/> that is used for training, leading to non-trivial dependencies. We should also note that in many situations one might be interested in the two-sided version of the generalization, as well as high probability bounds instead of bounds in expectation. For simplicity we focus on <img alt="\mathrm{gen}(\underline{h} , \mu)" class="ql-img-inline-formula " height="18" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c1cde338dd6203882d23d741c3f7d388_l3.png?resize=69%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="69"/> here.</p>
<p>The most classical approach to controlling generalization, which we covered in details in <a class="liinternal" href="https://blogs.princeton.edu/imabandit/2015/10/13/crash-course-on-learning-theory-part-1/">previous notes</a>, is via uniform law of large numbers. More precisely assuming that the range of the learning rule is some hypothesis class <img alt="\mathcal{H}" class="ql-img-inline-formula " height="12" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d8c7ae0e5e08bd1b3f5ef053720bf142_l3.png?resize=15%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="15"/> one trivially has</p>
<p class="ql-center-displayed-equation" style="line-height: 54px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[ \mathrm{gen}(\underline{h} , \mu) \leq \mathbb{E}_{S, z} \,\, \sup_{h \in \mathcal{H}} \left[ \frac1{m} \sum_{i=1}^m \ell(h, z_i) - \ell(h, z) \right] \,. \]" class="ql-img-displayed-equation " height="54" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-2c412a183c65900ed3b743f6d1d77c24_l3.png?resize=369%2C54&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="369"/></p>
<p>However this approach might be too coarse when the learning rule is searching through a potentially huge space of hypothesis (such as in the case of neural networks). Certainly such uniform bound has no chance of explaining why neural networks with billions of parameters would generalize with a data set of merely millions of examples. For this one has to use <em>algorithm-based</em> arguments.</p>
<p><strong>Stability</strong></p>
<p>The classical example of algorithmic generalization is due to Bousquet and Elisseeff 2002. It is a simple rewriting of the generalization as a <em>stability</em> notion:</p>
<p class="ql-center-displayed-equation" style="line-height: 50px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\begin{eqnarray*} \mathrm{gen}(\underline{h} , \mu) &amp; = &amp; \mathbb{E}_{S, z, i \sim [n]} \left[\ell(\underline{h}(S), z) - \ell(\underline{h}(S), z_i) \right] \\ &amp; = &amp; \mathbb{E}_{S, z, i \sim [n]} \left[\ell(\underline{h}(S^{-i}), z_i) - \ell(\underline{h}(S), z_i) \right] \,, \end{eqnarray*}" class="ql-img-displayed-equation " height="50" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-919f97ccdb86b939cfefc811661062cc_l3.png?resize=405%2C50&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="405"/></p>
<p>where <img alt="S^{-i}=(z_1,\hdots, z_{i-1}, z, z_{i+1}, \hdots, z_m)" class="ql-img-inline-formula " height="20" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-701c687bb718359abd167f6a196de813_l3.png?resize=268%2C20&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="268"/>. This viewpoint can be quite enlightening. For example in the uniform law of large numbers view, regularization enforces small capacity, while in the stability view we see that regularization ensures that the output hypothesis is not too brittle (this was covered in some details in the <a class="liinternal" href="https://blogs.princeton.edu/imabandit/2015/10/22/crash-course-on-learning-theory-part-2/">previous notes</a>).</p>
<p><strong>Robustness</strong></p>
<p>The next approach I would like to discuss is related to deep questions about current machine learning methods. One of the outstanding problem in machine learning is that current algorithms are not robust to even mild shift of distribution at test time. Intuitively this lack of robustness seem to indicate a lack of generalization. Can we formalize this intuition? I will now give one such formal link between robustness and generalization due to <a class="liinternal" href="https://arxiv.org/abs/1005.2243">Xu and Mannor 2010</a>, which shows the reverse direction (robustness implies generalization). At some level robustness can be viewed as a “stability at test time” (while in Bousquet and Elisseeff we care about “stability at training time”).</p>
<p>Xu and Mannor define <img alt="(K, \epsilon(\cdot))" class="ql-img-inline-formula " height="18" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c0a5ec0372d8ab9f32cd53fb447496fc_l3.png?resize=61%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="61"/>-robustness as follows: assume that <img alt="\mathcal{Z}" class="ql-img-inline-formula " height="12" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-dad9a970b689270e94f970c5f2a95597_l3.png?resize=14%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="14"/> can be partitioned into <img alt="K" class="ql-img-inline-formula " height="12" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b760ebc707e08dd6e1888ea8da4c2454_l3.png?resize=16%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="16"/> sets <img alt="\{C_k\}_{k \in [K]}" class="ql-img-inline-formula " height="21" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-48af2c12c9abaa63101dffa5f73fa8bd_l3.png?resize=74%2C21&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="74"/> such that if <img alt="z" class="ql-img-inline-formula " height="8" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d9d772a59543419785ce66946592259a_l3.png?resize=9%2C8&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="9"/> and <img alt="z_i" class="ql-img-inline-formula " height="11" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-28d8f2f4efdebf1f8d9cc4c9067855be_l3.png?resize=13%2C11&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="13"/> are in the same set <img alt="C_k" class="ql-img-inline-formula " height="15" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-a8072cca40d3173cdfcf82e7e4a4c71e_l3.png?resize=20%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="20"/> then</p>
<p class="ql-center-displayed-equation" style="line-height: 18px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[ | \ell(\underline{h}(S), z_i) - \ell(\underline{h}(S), z) | \leq \epsilon(S) \,. \]" class="ql-img-displayed-equation " height="18" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-a856ce8e86dd90def7f71e2b147ae032_l3.png?resize=245%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="245"/></p>
<p>A good example to have in mind would be a binary classifier with large margin, in which case <img alt="K" class="ql-img-inline-formula " height="12" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b760ebc707e08dd6e1888ea8da4c2454_l3.png?resize=16%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="16"/> corresponds to the covering number of <img alt="\mathcal{X}" class="ql-img-inline-formula " height="12" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e44d6dd2d58e906a7f3ec11d7f3cac9c_l3.png?resize=15%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="15"/> at the scale given by the margin. Another (related) example would be regression with a Lipschitz function. In both cases <img alt="K" class="ql-img-inline-formula " height="12" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b760ebc707e08dd6e1888ea8da4c2454_l3.png?resize=16%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="16"/> would be typically exponential in the dimension of <img alt="\mathcal{X}" class="ql-img-inline-formula " height="12" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e44d6dd2d58e906a7f3ec11d7f3cac9c_l3.png?resize=15%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="15"/>. The key result of Xu and Mannor that we prove next is a generalization bound of order <img alt="\sqrt{K / m}" class="ql-img-inline-formula " height="22" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-929f721ce13fef0d8ef31636872b1f90_l3.png?resize=57%2C22&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="57"/>. In any situation of interest this seems to me to be a pretty weak bound, yet on the other hand I find the framework to be very pretty and it is of topical interest. I would be surprised if this was the end of the road in the space of “generalization and robustness”.</p>
<blockquote><p><strong>Theorem (Xu and Mannor 2010):<br/>
</strong></p>
<p>A <img alt="(K, \epsilon(\cdot))" class="ql-img-inline-formula " height="18" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c0a5ec0372d8ab9f32cd53fb447496fc_l3.png?resize=61%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="61"/>-robust learning rule satisfies</p>
<p class="ql-center-displayed-equation" style="line-height: 43px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[ \mathrm{gen}(\underline{h} , \mu) \leq \mathbb{E}_{S} \, \epsilon(S) + \sqrt{\frac{K}{m}} \,. \]" class="ql-img-displayed-equation " height="43" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-daca8cad8d183d50f8aa0bdfa6169a0c_l3.png?resize=217%2C43&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="217"/></p>
</blockquote>
<p><strong>Proof:</strong> Let <img alt="N_k = |S \cap C_k|" class="ql-img-inline-formula " height="18" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d02c7cfdd59553630c1c491d1b1d6bc4_l3.png?resize=107%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="107"/> and note that <img alt="\mathbb{E}_S \, N_k = m \mu(C_k)" class="ql-img-inline-formula " height="18" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-65a64515566d528c7eab5285d4207f65_l3.png?resize=131%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="131"/>. Now one has for a robust <img alt="h" class="ql-img-inline-formula " height="13" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-1f60c707908cae43d340ee091916576c_l3.png?resize=10%2C13&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="10"/>:</p>
<p class="ql-center-displayed-equation" style="line-height: 180px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\begin{align*} \mathbb{E}_z \ \ell(h, z) &amp; =\sum_{k=1}^K \mu(C_k) \mathbb{E}_z [\ell(h,z) | z \in C_k] \\ &amp; = \sum_{k=1}^K \frac{N_k}{m} \mathbb{E}_z [\ell(h,z) | z \in C_k] + \sum_{k=1}^K \left(\mu(C_k) - \frac{N_k}{m}\right) \mathbb{E}_z [\ell(h,z) | z \in C_k] \\ &amp; \leq \epsilon(S) + \frac{1}{m} \sum_{i = 1}^m \ell(h, z_i) + \sum_{k=1}^K \left|\mu(C_k) - \frac{N_k}{m}\right| \, . \end{align*}" class="ql-img-displayed-equation " height="180" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-f779197b142638d80e7a537bdb029972_l3.png?resize=592%2C180&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="592"/></p>
<p>It only remains to observe that</p>
<p class="ql-center-displayed-equation" style="line-height: 55px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[ \mathbb{E}_S \, \sum_{k=1}^K \left|\mu(C_k) - \frac{N_k}{m}\right| \leq \frac{1}{m} \sum_{k=1}^K \sqrt{\mu(C_k)} \leq \sqrt{\frac{K}{m}} \, . \]" class="ql-img-displayed-equation " height="55" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-a8affe97588e4fc52d9038abcf5a0102_l3.png?resize=368%2C55&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="368"/></p>
<p> </p>
<p><strong>Information theoretic perspective</strong></p>
<p>Why do we think that a lack of robustness indicate a lack of generalization? Well it seems to me that a basic issue could simply be that the dataset was <em>memorized</em> by the neural network (which be a <em>very</em> non-robust way to learn). If true then one could basically find all the information about the data in the weights of the neural network. Again, can we prove at least the opposite direction, that is if the output hypothesis does not retain much information from the dataset then it must generalize. This is exactly what <a class="liexternal" href="http://proceedings.mlr.press/v51/russo16.html">Russo and Zou 2016</a>, where they use the mutual information <img alt="\mathrm{I}(\underline{h}(S), S)" class="ql-img-inline-formula " height="18" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e4ce514d8cce97c925b3c3e30ecf725e_l3.png?resize=75%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="75"/> as a measure of the “information” retained by the trained hypothesis about the dataset. More precisely they show the following result:</p>
<blockquote><p><strong>Theorem (Russo and Zou 2016):</strong></p>
<p class="ql-center-displayed-equation" style="line-height: 43px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[ \mathrm{gen}(\underline{h} , \mu) \leq \sqrt{\frac{\mathrm{I}(\underline{h}(S), S)}{2 m}} \, . \]" class="ql-img-displayed-equation " height="43" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-7df1d21a60269cd821277323fb46b105_l3.png?resize=197%2C43&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="197"/></p>
</blockquote>
<p>Note that here we have assumed that the codomain of the learning rule <img alt="\underline{h}" class="ql-img-inline-formula " height="16" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-17dfdaa6f0eb1935e19164615973c6fe_l3.png?resize=11%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="11"/> consists of deterministic maps from inputs to outputs, in which case the mutual information <img alt="\mathrm{I}(\underline{h}(S), S)" class="ql-img-inline-formula " height="18" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e4ce514d8cce97c925b3c3e30ecf725e_l3.png?resize=75%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="75"/> is simply the entropy <img alt="\mathrm{H}(\underline{h}(S))" class="ql-img-inline-formula " height="18" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-766f1fbe1a64596d4d2fd72856eaef53_l3.png?resize=62%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="62"/>. However the proof below also applies to the case where the codomain of the learning rule consists of probability measures, see e.g., <a class="liinternal" href="https://arxiv.org/abs/1705.07809">Xu and Raginsky 2017</a>. Let us now conclude this (long) post with the proof of the above theorem.</p>
<p>The key point is very simple: one can view generalization as a decoupling property by writing:</p>
<p class="ql-center-displayed-equation" style="line-height: 23px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[ \mathrm{gen}(\underline{h} , \mu) = \mathbb{E}_{S, S' \sim \mu^{\otimes m}} [ F(\underline{h}(S), S') - F(\underline{h}(S), S) ] \,, \]" class="ql-img-displayed-equation " height="23" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-676756cbac28abe49125ce73d59503fb_l3.png?resize=381%2C23&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="381"/></p>
<p>where <img alt="F(h, S) = \frac{1}{m} \sum_{z \in S} \ell(h, z)" class="ql-img-inline-formula " height="22" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c8ad36f2168d37b877d6f2254fd8fc81_l3.png?resize=195%2C22&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="195"/>.</p>
<p>Now the theorem follows straightforwardly (if one knows Hoeffding’s lemma) from an application of the following beautiful lemma:</p>
<blockquote><p><strong>Lemma:</strong></p>
<p>Let <img alt="f: \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}" class="ql-img-inline-formula " height="16" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-445a0199b5300e745323b026842acca9_l3.png?resize=116%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="116"/>. Let <img alt="(X,Y)" class="ql-img-inline-formula " height="18" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-41113ce00313a34b5d9227362a1c74c3_l3.png?resize=49%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="49"/> be random variables in <img alt="\mathcal{X} \times \mathcal{Y}" class="ql-img-inline-formula " height="15" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-716b2fd03be87c276f8e6235b9e08084_l3.png?resize=50%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="50"/> and <img alt="\bar{X}" class="ql-img-inline-formula " height="15" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e47f6fd8484457348ba5f636b7f8a2c1_l3.png?resize=16%2C15&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="16"/>, <img alt="\bar{Y}" class="ql-img-inline-formula " height="15" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-042f524c660aab74a2bfd2d687c6c8ca_l3.png?resize=14%2C15&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="14"/> be mutually independent copies of <img alt="X" class="ql-img-inline-formula " height="12" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-2453362c766504f3c3806fed710a5337_l3.png?resize=16%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="16"/> and <img alt="Y" class="ql-img-inline-formula " height="12" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ab67eda06fcf093fb3178b85c9214045_l3.png?resize=14%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="14"/>. Assume that <img alt="f(\bar{X}, \bar{Y})" class="ql-img-inline-formula " height="19" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b341609e24aeca2d2b20ef9e1177ca1e_l3.png?resize=62%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="62"/> is <img alt="\sigma" class="ql-img-inline-formula " height="8" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-218428ebbff86310fbdb1f7324215c46_l3.png?resize=11%2C8&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/>-subgaussian (i.e., <img alt="\log \mathbb{E} \exp( \lambda (f(\bar{X}, \bar{Y}) - \mathbb{E} f) ) \leq \lambda^2 \sigma^2 /2, \forall \lambda" class="ql-img-inline-formula " height="20" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-8b25bb05e04374f24bf250bbf9d1255c_l3.png?resize=318%2C20&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="318"/>) then</p>
<p class="ql-center-displayed-equation" style="line-height: 22px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[ \mathbb{E}[ f(X,Y) - f(\bar{X}, \bar{Y}) ] \leq \sqrt{2 \sigma^2 \mathrm{I}(X, Y)} \]" class="ql-img-displayed-equation " height="22" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-9e96cdf850332fe46b9d7e2d94b6755c_l3.png?resize=294%2C22&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="294"/></p>
</blockquote>
<p><strong>Proof:</strong> The mutual information is equal to the relative entropy between the distribution of <img alt="(X,Y)" class="ql-img-inline-formula " height="18" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-41113ce00313a34b5d9227362a1c74c3_l3.png?resize=49%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="49"/> and the distribution of <img alt="(\bar{X}, \bar{Y})" class="ql-img-inline-formula " height="19" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-13acb446c9dba1d0fcbed27706e603f6_l3.png?resize=50%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="50"/>. Recall also the variational representation of the relative entropy which is that the map <img alt="\mu \mapsto \mathrm{Ent}(\mu, \nu)" class="ql-img-inline-formula " height="18" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-50d4231225f4649758e0477f8ce935bc_l3.png?resize=108%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="108"/> is the convex conjugate of the log-partition function <img alt="\Phi_{\nu}(g) := \log(\int \exp(g(x)) d \nu(x))" class="ql-img-inline-formula " height="20" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-5710632c01696049adc99bb25c828924_l3.png?resize=240%2C20&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="240"/>. In particular one has a lower bound on the mutual information for any such <img alt="g" class="ql-img-inline-formula " height="12" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3bc95a1b193d6245c9a5efee30eac889_l3.png?resize=9%2C12&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="9"/> which means:</p>
<p class="ql-center-displayed-equation" style="line-height: 21px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[ \mathrm{I}(X, Y) \geq \mathbb{E}[ g(X,Y) ] - \log \mathbb{E} [ \exp(g(\bar{X},\bar{Y})) ] \,. \]" class="ql-img-displayed-equation " height="21" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-79b74493f08b043c788313271f33fc96_l3.png?resize=341%2C21&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="341"/></p>
<p>Now it only remains to use the definition of subgaussianity, that is take <img alt="g = \lambda f" class="ql-img-inline-formula " height="16" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-fb18e52615a918505d20850c782b3b56_l3.png?resize=53%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="53"/>, and optimize over <img alt="\lambda" class="ql-img-inline-formula " height="12" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ab48baf331239642a00255b86324280a_l3.png?resize=10%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="10"/>.</p></div></content><updated planet:format="September 18, 2018 03:46 PM">2018-09-18T15:46:34Z</updated><published planet:format="September 18, 2018 03:46 PM">2018-09-18T15:46:34Z</published><category term="Machine learning"/><author><name>Sebastien Bubeck</name></author><source><id>https://blogs.princeton.edu/imabandit</id><link href="https://blogs.princeton.edu/imabandit/feed/" rel="self" type="application/atom+xml"/><link href="https://blogs.princeton.edu/imabandit" rel="alternate" type="text/html"/><subtitle>Random topics in optimization, probability, and statistics. By Sébastien Bubeck</subtitle><title>I’m a bandit</title><updated planet:format="December 16, 2018 04:44 PM">2018-12-16T16:44:03Z</updated><planet:http_status>200</planet:http_status><planet:module>toc</planet:module><planet:format>atom10</planet:format><planet:items_per_page>40</planet:items_per_page><planet:name>S&amp;eacute;bastian Bubeck</planet:name><planet:http_last_modified>Fri, 23 Nov 2018 17:03:44 GMT</planet:http_last_modified><planet:bozo>false</planet:bozo><planet:http_etag>&quot;7a21033bc747c1eccfdc0e4c416aa3e2&quot;</planet:http_etag><planet:css-id>s-eacute-bastian-bubeck</planet:css-id></source></entry>
