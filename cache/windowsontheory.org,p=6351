<?xml version="1.0" encoding="utf-8"?><entry xml:lang="en" xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/"><id>http://windowsontheory.org/?p=6351</id><link href="https://windowsontheory.org/2018/12/13/ising-perceptron-under-gaussian-disorder-and-k-nae-sat/" rel="alternate" type="text/html"/><title>Ising Perceptron under Gaussian Disorder, and k-NAE-SAT</title><summary>Blog Post By: Patrick Guo, Vinh-Kha Le, Shyam Narayanan, and David Stoner Methods in statistical physics are known to be extremely useful for understanding certain problems in theoretical computer science. Physical observations can motivate the underlying theoretical models, which in turn explain some of the physical phenomena. This post is based on Professor Nike Sun’s […]<div class="commentbar"><p/></div></summary><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>Blog Post By: Patrick Guo, Vinh-Kha Le, Shyam Narayanan, and David Stoner</strong></p>
<p>Methods in statistical physics are known to be extremely useful for understanding certain problems in theoretical computer science. Physical observations can motivate the underlying theoretical models, which in turn explain some of the physical phenomena.</p>
<p>This post is based on Professor Nike Sun’s guest lecture on the Ising Perceptron model and regular NAE-SAT for CS 229R: Physics and Computation. These are both examples of random constraint satisfaction problems, where we have <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> variables <img alt="x_1, ..., x_n \in \{-1, 1\}" class="latex" src="https://s0.wp.com/latex.php?latex=x_1%2C+...%2C+x_n+%5Cin+%5C%7B-1%2C+1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_1, ..., x_n \in \{-1, 1\}"/> and certain relations, or constraints, between the <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>, and we wish to approximate the number of solutions or visualize the geometry of the solutions. For both problems, the problem instance can be random: for example, the linear constraints in the Ising perceptron model are random, and the clauses in the NAE-SAT instance are chosen at random. As in the previous blog posts, to understand the geometry of solutions, statistical physicists think of sampling random solutions from <img alt="\{-1, 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B-1%2C+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{-1, 1\}^n"/>, which introduces a second type of randomness.</p>
<p>This post is meant to be expository. For interested readers, we point to useful references at the end for more rigorous treatments of these topics.</p>
<p><strong>1. Perceptron Model</strong></p>
<p>The Ising Perceptron under Gaussian disorder asks how many points in <img alt="\{\pm 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{\pm 1\}^n"/> survive <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M"/> half-plane bisections (i.e. are satisfying assignments for <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M"/> constraints), where the planes’ coefficients are drawn from standard Gaussians. Like many problems in statistical physics, there is likely a critical capacity of constraints where having more constraints yields no survivors with high probability, and having fewer constraints yields survivors with high probability. This lecture gives an overview of a proof for one side of this physics prediction, i.e. the existence of a lower bound critical capacity, where having fewer constraints yields survivors with positive probability. Briefly, we use the <em>Cavity method</em> to approximate the distribution of the number of satisfying assignments, then attempt to use the second moment method on that distribution to get our lower bound. A direct application fails, however, due to the variance of the Gaussian constraints. The solution is to carefully choose exactly what to condition on without destroying the model. Specifically, we iteratively compute values related to the Gaussian disorder, after which we are able remove enough variance for the second moment method to work and thus establish the lower bound for the Ising Perceptron’s critical capacity. The result holds subject to an analytical condition which is detailed in the paper (Condition 1.2 in [6]) and which remains to be rigorously verified.</p>
<p><strong>1.1. Problem</strong></p>
<p>We pick a random direction in <img alt="\mathbb{R}^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{R}^n"/>, and delete all vertices in the hypercube <img alt="\{\pm 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{\pm 1\}^n"/> which are in the half-space negatively correlated with that direction. We repeat this process of picking a random half space and deleting points <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M"/> times, and see if any points in the hypercube survive. Formally, we define the perceptron model as follows:</p>
<p><strong>Definition 1:</strong> Let <img alt="G = (g_{\mu{}i})_{\mu\ge 1, i\ge 1}" class="latex" src="https://s0.wp.com/latex.php?latex=G+%3D+%28g_%7B%5Cmu%7B%7Di%7D%29_%7B%5Cmu%5Cge+1%2C+i%5Cge+1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G = (g_{\mu{}i})_{\mu\ge 1, i\ge 1}"/> array of i.i.d. standard Gaussians. Let <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M"/> be the largest integer such that:</p>
<p><img alt="\left\{J\in \{\pm 1\}^N:\sum_{i=1}^N\frac{g_{\mu{}i}J_i}{\sqrt{N}}\ge 0 \hspace{0.1cm} \forall \mu\le M\right\}\neq \emptyset." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%5C%7BJ%5Cin+%5C%7B%5Cpm+1%5C%7D%5EN%3A%5Csum_%7Bi%3D1%7D%5EN%5Cfrac%7Bg_%7B%5Cmu%7B%7Di%7DJ_i%7D%7B%5Csqrt%7BN%7D%7D%5Cge+0+%5Chspace%7B0.1cm%7D+%5Cforall+%5Cmu%5Cle+M%5Cright%5C%7D%5Cneq+%5Cemptyset.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\left\{J\in \{\pm 1\}^N:\sum_{i=1}^N\frac{g_{\mu{}i}J_i}{\sqrt{N}}\ge 0 \hspace{0.1cm} \forall \mu\le M\right\}\neq \emptyset."/></p>
<p>More compactly, <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M"/> is the largest integer such that</p>
<p><img alt="\left\{J\in \{\pm 1\}^N:\frac{GJ}{\sqrt{N}}\ge 0\right\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%5C%7BJ%5Cin+%5C%7B%5Cpm+1%5C%7D%5EN%3A%5Cfrac%7BGJ%7D%7B%5Csqrt%7BN%7D%7D%5Cge+0%5Cright%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\left\{J\in \{\pm 1\}^N:\frac{GJ}{\sqrt{N}}\ge 0\right\}"/></p>
<p>is nonempty, where the inequality is taken pointwise, <img alt="G \in \mathbb{R}^{M \times N}" class="latex" src="https://s0.wp.com/latex.php?latex=G+%5Cin+%5Cmathbb%7BR%7D%5E%7BM+%5Ctimes+N%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G \in \mathbb{R}^{M \times N}"/> is an array of i.i.d. std. Gaussians, and <img alt="J" class="latex" src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J"/> is treated as a vector in <img alt="\{\pm 1\}^N \subset \mathbb{R}^N" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpm+1%5C%7D%5EN+%5Csubset+%5Cmathbb%7BR%7D%5EN&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{\pm 1\}^N \subset \mathbb{R}^N"/>.</p>
<p><img alt="CS229 Pic 1" class="alignnone size-full wp-image-6353" src="https://windowsontheory.files.wordpress.com/2018/12/CS229-Pic-1.png?w=600"/></p>
<p>In the late 80’s, physicists conjectured that there is a critical capacity <img alt="\alpha_{*}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha_%7B%2A%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha_{*}"/> such that <img alt="\frac{M}{N} \overset{P}{\to} \alpha_{*}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BM%7D%7BN%7D+%5Coverset%7BP%7D%7B%5Cto%7D+%5Calpha_%7B%2A%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{M}{N} \overset{P}{\to} \alpha_{*}"/> where <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M"/> is a function of <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N"/>. The predicted critical capacity has been studied, for example in [7,9]. Our goal is to establish a lower bound on <img alt="\alpha_*" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha_%2A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha_*"/> for the Ising Perceptron under Gaussian disorder. To do this, let <img alt="Z(G)" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G)"/> denote the random variable which measures how many choices of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> variables survive <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M"/> Gaussian constraints in <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> (this is the partition function). We want to show <img alt="Z &gt; 0" class="latex" src="https://s0.wp.com/latex.php?latex=Z+%3E+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z &gt; 0"/> with high probability when there are <img alt="M &lt; \alpha_{*}N" class="latex" src="https://s0.wp.com/latex.php?latex=M+%3C+%5Calpha_%7B%2A%7DN&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M &lt; \alpha_{*}N"/> constraints. To this end, the second moment method seems promising:</p>
<p><strong>Second Moment Method: </strong>If <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z"/> is a nonnegative random variable with finite variance, then</p>
<p><img alt="P(Z &gt; 0) \ge \frac{\left(\mathbb{E}[Z]\right)^2}{\mathbb{E}[Z^2]}" class="latex" src="https://s0.wp.com/latex.php?latex=P%28Z+%3E+0%29+%5Cge+%5Cfrac%7B%5Cleft%28%5Cmathbb%7BE%7D%5BZ%5D%5Cright%29%5E2%7D%7B%5Cmathbb%7BE%7D%5BZ%5E2%5D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(Z &gt; 0) \ge \frac{\left(\mathbb{E}[Z]\right)^2}{\mathbb{E}[Z^2]}"/></p>
<p>However, this method actually fails due to various sources of variance in the perceptron model. We will briefly sketch the fix as given in [6].</p>
<p>Before we can start, however, what does the distribution of <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z"/> even look like? This is actually quite computationally intensive; we will use the <strong>cavity equations</strong>, a technique developed in [12,13], to approximate <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z"/>‘s distribution.</p>
<p><strong>1.2. Cavity Method</strong></p>
<p>The goal of the cavity method is to see how the solution space changes as we remove a row or a column of the matrix <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> with i.i.d. standard Gaussian entries, assuming that <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> is fixed. Since our matrix <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> is big and difficult to deal with, we try to see how the solution space changes as we add one row or one column at a time. Why is this valuable? We can think of the system of variables and constraints as an interaction between the rows (constraints) and columns (variables), so the number of solutions should behave proportionally to the product of the solutions attributed to each variable and each constraint. With this as motivation, define <img alt="G_{-\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=G_%7B-%5Cmu%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G_{-\mu}"/> as the matrix obtained by removing row <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu"/> and <img alt="G^{-i}" class="latex" src="https://s0.wp.com/latex.php?latex=G%5E%7B-i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G^{-i}"/> as the matrix obtained by removing column <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>. We can approximate</p>
<p><img alt="Z(G) \approx \prod\limits_{\mu = 1}^{M} \frac{Z(G)}{Z(G_{-\mu})} \cdot \prod\limits_{i = 1}^{N} \frac{Z(G)}{Z(G^{-i})}" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29+%5Capprox+%5Cprod%5Climits_%7B%5Cmu+%3D+1%7D%5E%7BM%7D+%5Cfrac%7BZ%28G%29%7D%7BZ%28G_%7B-%5Cmu%7D%29%7D+%5Ccdot+%5Cprod%5Climits_%7Bi+%3D+1%7D%5E%7BN%7D+%5Cfrac%7BZ%28G%29%7D%7BZ%28G%5E%7B-i%7D%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G) \approx \prod\limits_{\mu = 1}^{M} \frac{Z(G)}{Z(G_{-\mu})} \cdot \prod\limits_{i = 1}^{N} \frac{Z(G)}{Z(G^{-i})}"/></p>
<p>since we can think of the partition function as receiving a multiplicative factor from each addition of a row and each addition of a column. Thus, the cavity method seeks to compute <img alt="Z(G)/Z(G_{-\mu})" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G_%7B-%5Cmu%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G)/Z(G_{-\mu})"/> and <img alt="Z(G)/Z(G^{-i})." class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G%5E%7B-i%7D%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G)/Z(G^{-i})."/></p>
<p><strong>1.2.1. Removing a constraint</strong></p>
<p>Our goal in computing <img alt="Z(G)/Z(G_{-\mu})" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G_%7B-%5Cmu%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G)/Z(G_{-\mu})"/> is to understand how the solution space <img alt="SOL(G_{-\mu})" class="latex" src="https://s0.wp.com/latex.php?latex=SOL%28G_%7B-%5Cmu%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="SOL(G_{-\mu})"/> changes when we add in the constraint <img alt="\mu." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu."/> Recalling that <img alt="J \in \{\pm 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=J+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J \in \{\pm 1\}^n"/> is our vector that is potentially in the solution space, if we define</p>
<p><img alt="\Delta_\mu := \sum_{i = 1}^{N}\frac{g_{\mu{}i}J_i}{\sqrt{N}}," class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu+%3A%3D+%5Csum_%7Bi+%3D+1%7D%5E%7BN%7D%5Cfrac%7Bg_%7B%5Cmu%7B%7Di%7DJ_i%7D%7B%5Csqrt%7BN%7D%7D%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Delta_\mu := \sum_{i = 1}^{N}\frac{g_{\mu{}i}J_i}{\sqrt{N}},"/></p>
<p>then adding the constraint <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu"/> is equivalent to forcing <img alt="\Delta_\mu \ge 0." class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu+%5Cge+0.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Delta_\mu \ge 0."/> We will try to understand the distribution of <img alt="\Delta_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Delta_\mu"/> under the Gibbs measure <img alt="\nu_{-\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnu_%7B-%5Cmu%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\nu_{-\mu}"/>, which is the uniform measure on the solution space without the constraint <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu"/>. This way, we can determine the probability of <img alt="\Delta_\mu \ge 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu+%5Cge+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Delta_\mu \ge 0"/> under the Gibbs measure, which will equal <img alt="Z(G)/Z(G_{-\mu})." class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G_%7B-%5Cmu%7D%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G)/Z(G_{-\mu})."/> To do so, we will use the <strong>Replica Symmetric Cavity Assumption</strong> (which we will abbreviate as RS cavity assumption). The RS cavity assumption lets us assume that the <img alt="J_i" class="latex" src="https://s0.wp.com/latex.php?latex=J_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J_i"/>‘s for <img alt="1 \le i \le N" class="latex" src="https://s0.wp.com/latex.php?latex=1+%5Cle+i+%5Cle+N&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1 \le i \le N"/> are independent under <img alt="\nu_{-\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnu_%7B-%5Cmu%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\nu_{-\mu}"/>. As the <img alt="J_i" class="latex" src="https://s0.wp.com/latex.php?latex=J_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J_i"/>‘s are some discrete sample space that depend on our constraints, we do not actually have full independence, but the RS cavity assumption tells us there is very little dependence between the <img alt="J_i" class="latex" src="https://s0.wp.com/latex.php?latex=J_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J_i"/>‘s, so we pretend they are independent.</p>
<p>Note that <img alt="\Delta_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Delta_\mu"/> should be approximately normally distributed, since it is the sum of many “independent” terms <img alt="g_{\mu i} J_i" class="latex" src="https://s0.wp.com/latex.php?latex=g_%7B%5Cmu+i%7D+J_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_{\mu i} J_i"/> by the RS Cavity assumption. Now, define <img alt="h_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=h_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_\mu"/> as the expectation of <img alt="\Delta_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Delta_\mu"/> under the Gibbs measure without the constraint <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu"/>, and <img alt="v_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=v_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v_\mu"/> as the variance of <img alt="\Delta_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Delta_\mu"/> under the Gibbs measure without the constraint <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu"/>. It will also turn out that the variance <img alt="v_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=v_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v_\mu"/> will concentrate around a constant <img alt="\sigma^2." class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma%5E2.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma^2."/> Thus,</p>
<p><img alt="\frac{Z(G)}{Z(G_{-\mu})} \approx \nu_{-\mu}(\Delta_{\mu} \ge 0) = \overline{\Phi} (\frac{-h_\mu}{\sigma})." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BZ%28G%29%7D%7BZ%28G_%7B-%5Cmu%7D%29%7D+%5Capprox+%5Cnu_%7B-%5Cmu%7D%28%5CDelta_%7B%5Cmu%7D+%5Cge+0%29+%3D+%5Coverline%7B%5CPhi%7D+%28%5Cfrac%7B-h_%5Cmu%7D%7B%5Csigma%7D%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{Z(G)}{Z(G_{-\mu})} \approx \nu_{-\mu}(\Delta_{\mu} \ge 0) = \overline{\Phi} (\frac{-h_\mu}{\sigma})."/></p>
<p>Here, <img alt="\overline{\Phi}(\frac{-h_\mu}{\sigma})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Coverline%7B%5CPhi%7D%28%5Cfrac%7B-h_%5Cmu%7D%7B%5Csigma%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\overline{\Phi}(\frac{-h_\mu}{\sigma})"/> equals the probability that a random <img alt="\mathcal{N}(h_\mu, \sigma^2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BN%7D%28h_%5Cmu%2C+%5Csigma%5E2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{N}(h_\mu, \sigma^2)"/> Gaussian distribution is positive, or equivalently, the probability that a standard Gaussian <img alt="\mathcal{N}(0, 1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BN%7D%280%2C+1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{N}(0, 1)"/> distribution is greater than <img alt="\frac{-h_\mu}{\sigma}." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B-h_%5Cmu%7D%7B%5Csigma%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{-h_\mu}{\sigma}."/></p>
<p><strong>1.2.2. Removing a spin</strong></p>
<p>To calculate the cavity equation for removing one column, we think of removing a column as deleting one spin from <img alt="J" class="latex" src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J"/>. Define <img alt="J^{-i}" class="latex" src="https://s0.wp.com/latex.php?latex=J%5E%7B-i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J^{-i}"/> as the vector resulting from removing spin <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/> from <img alt="J" class="latex" src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J"/>. We want to calculate <img alt="Z(G)/Z(G^{-i})." class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G%5E%7B-i%7D%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G)/Z(G^{-i})."/> Note that it is possible for <img alt="J^{-i} \not\in SOL(G^{-i})" class="latex" src="https://s0.wp.com/latex.php?latex=J%5E%7B-i%7D+%5Cnot%5Cin+SOL%28G%5E%7B-i%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J^{-i} \not\in SOL(G^{-i})"/> but <img alt="J \in SOL(G)" class="latex" src="https://s0.wp.com/latex.php?latex=J+%5Cin+SOL%28G%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J \in SOL(G)"/> now, which complicates this calculation. It is possible to overcome this difficulty by passing to positive temperature, though this makes the calculations incredibly difficult. We do not worry about these issues here, and just briefly sketch how <img alt="Z(G)/Z(G^{-i})" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G%5E%7B-i%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G)/Z(G^{-i})"/> is computed.</p>
<p>To compute <img alt="Z(G)/Z(G^{-i})" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G%5E%7B-i%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G)/Z(G^{-i})"/>, we will split the numerator into a sum of two terms based on the sign of <img alt="J_i," class="latex" src="https://s0.wp.com/latex.php?latex=J_i%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J_i,"/> which will allow us to compute not only <img alt="Z(G)" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G)"/> but also <img alt="\langle J_i \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+J_i+%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\langle J_i \rangle"/>, which represents the magnetization at spin <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>. A series of complicated calculations (see the lecture notes for the details) will give us</p>
<p><img alt="\frac{Z(G)}{Z(G^{-i})} = \frac{\exp(H_i)+\exp(-H_i)}{exp(c)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BZ%28G%29%7D%7BZ%28G%5E%7B-i%7D%29%7D+%3D+%5Cfrac%7B%5Cexp%28H_i%29%2B%5Cexp%28-H_i%29%7D%7Bexp%28c%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{Z(G)}{Z(G^{-i})} = \frac{\exp(H_i)+\exp(-H_i)}{exp(c)}"/></p>
<p>for some constant <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c"/>, where <img alt="H_i" class="latex" src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H_i"/> is a quantity that compares how much more correlated spin <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/> is to the constraints than all other spins. The <img alt="\exp(+H_i)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28%2BH_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\exp(+H_i)"/> will come from the solutions for <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> with <img alt="J_i = 1" class="latex" src="https://s0.wp.com/latex.php?latex=J_i+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J_i = 1"/> and the <img alt="\exp(-H_i)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28-H_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\exp(-H_i)"/> will come from the solutions with <img alt="J_i = -1" class="latex" src="https://s0.wp.com/latex.php?latex=J_i+%3D+-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J_i = -1"/>.</p>
<p>The above equations allow us to deduce that</p>
<p><img alt="Z(G) \approx \prod\limits_{\mu = 1}^{M} \overline{\Phi}\left(-\frac{h_\mu}{\sigma}\right) \cdot \prod\limits_{i = 1}^{N} \frac{\exp(H_i) + \exp(-H_i)}{\exp(c)}." class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29+%5Capprox+%5Cprod%5Climits_%7B%5Cmu+%3D+1%7D%5E%7BM%7D+%5Coverline%7B%5CPhi%7D%5Cleft%28-%5Cfrac%7Bh_%5Cmu%7D%7B%5Csigma%7D%5Cright%29+%5Ccdot+%5Cprod%5Climits_%7Bi+%3D+1%7D%5E%7BN%7D+%5Cfrac%7B%5Cexp%28H_i%29+%2B+%5Cexp%28-H_i%29%7D%7B%5Cexp%28c%29%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G) \approx \prod\limits_{\mu = 1}^{M} \overline{\Phi}\left(-\frac{h_\mu}{\sigma}\right) \cdot \prod\limits_{i = 1}^{N} \frac{\exp(H_i) + \exp(-H_i)}{\exp(c)}."/></p>
<p><strong>1.3. The Randomness of G</strong></p>
<p>In the previous section, we regarded <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> as fixed. We now use the these results but allow for <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> to be random again. Recall <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/>‘s entries were i.i.d. Gaussians. We thus get that <img alt="h_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=h_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_\mu"/>, as a linear combination of the <img alt="g_{\mu{}i}" class="latex" src="https://s0.wp.com/latex.php?latex=g_%7B%5Cmu%7B%7Di%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_{\mu{}i}"/>‘s, is a Gaussian. We thus can write <img alt="h_\mu \sim \mathcal{N}(0, q)." class="latex" src="https://s0.wp.com/latex.php?latex=h_%5Cmu+%5Csim+%5Cmathcal%7BN%7D%280%2C+q%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_\mu \sim \mathcal{N}(0, q)."/> Similarly, <img alt="H_i" class="latex" src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H_i"/> is a linear combination of the <img alt="g_{\mu{}i}" class="latex" src="https://s0.wp.com/latex.php?latex=g_%7B%5Cmu%7B%7Di%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_{\mu{}i}"/>‘s and must be Gaussian. We thus write <img alt="H_i \sim \mathcal{N}(0, \psi)" class="latex" src="https://s0.wp.com/latex.php?latex=H_i+%5Csim+%5Cmathcal%7BN%7D%280%2C+%5Cpsi%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H_i \sim \mathcal{N}(0, \psi)"/>.</p>
<p>With some calculations detailed in the lecture notes and [12], we get <em>Gardner’s Formula</em>:</p>
<p><img alt="\frac{\ln Z}{N} \rightarrow \alpha \int \ln \overline{\Phi}\left(-\frac{\sqrt{q} z}{\sigma}\right) \varphi(z) dz + \int \ln \left(2 \cosh(\sqrt{\psi z})\right)\varphi(z) dz" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cln+Z%7D%7BN%7D+%5Crightarrow+%5Calpha+%5Cint+%5Cln+%5Coverline%7B%5CPhi%7D%5Cleft%28-%5Cfrac%7B%5Csqrt%7Bq%7D+z%7D%7B%5Csigma%7D%5Cright%29+%5Cvarphi%28z%29+dz+%2B+%5Cint+%5Cln+%5Cleft%282+%5Ccosh%28%5Csqrt%7B%5Cpsi+z%7D%29%5Cright%29%5Cvarphi%28z%29+dz&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{\ln Z}{N} \rightarrow \alpha \int \ln \overline{\Phi}\left(-\frac{\sqrt{q} z}{\sigma}\right) \varphi(z) dz + \int \ln \left(2 \cosh(\sqrt{\psi z})\right)\varphi(z) dz"/></p>
<p>where <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha"/> is our capacity, <img alt="\frac{M}{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BM%7D%7BN%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{M}{N}"/>. It turns out that <img alt="\sigma^2 = 1-q" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma%5E2+%3D+1-q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma^2 = 1-q"/> and <img alt="c = \psi(1-q)/2," class="latex" src="https://s0.wp.com/latex.php?latex=c+%3D+%5Cpsi%281-q%29%2F2%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c = \psi(1-q)/2,"/> so the above equation only depends on two parameters, <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q"/> and <img alt="\psi." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi."/> It will turn out that <img alt="q, \psi" class="latex" src="https://s0.wp.com/latex.php?latex=q%2C+%5Cpsi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q, \psi"/> have relations dependent on each other based on our definitions of <img alt="h_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=h_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_\mu"/> and <img alt="H_i" class="latex" src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H_i"/>, and these will give us a fixed point equation for <img alt="(q, \psi)," class="latex" src="https://s0.wp.com/latex.php?latex=%28q%2C+%5Cpsi%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(q, \psi),"/> which has two solutions: one near <img alt="\alpha \approx 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Capprox+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha \approx 1"/> and one near <img alt="\alpha \approx 0.83." class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Capprox+0.83.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha \approx 0.83."/> It is believed that the second point is correct, meaning that the critical capacity should equal <img alt="\alpha_* = 0.83." class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha_%2A+%3D+0.83.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha_* = 0.83."/></p>
<p><strong>1.4. Second Moment Method</strong></p>
<p>Now that we have Gardner’s formula, solving for <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z"/> gives us an approximation for its distribution as</p>
<p><img alt="Z \sim \exp\{N\mathcal{G}(\alpha) + \mathcal{N}(0, N\varepsilon^2)\}" class="latex" src="https://s0.wp.com/latex.php?latex=Z+%5Csim+%5Cexp%5C%7BN%5Cmathcal%7BG%7D%28%5Calpha%29+%2B+%5Cmathcal%7BN%7D%280%2C+N%5Cvarepsilon%5E2%29%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z \sim \exp\{N\mathcal{G}(\alpha) + \mathcal{N}(0, N\varepsilon^2)\}"/></p>
<p>where <img alt="\mathcal{G}(\alpha)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BG%7D%28%5Calpha%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{G}(\alpha)"/> is Gardner’s formula, and the Gaussian noise <img alt="\mathcal{N}(0,N\varepsilon^2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BN%7D%280%2CN%5Cvarepsilon%5E2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{N}(0,N\varepsilon^2)"/> arises from the Gaussian distribution of the <img alt="h_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=h_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_\mu"/>‘s and <img alt="H_i" class="latex" src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H_i"/>‘s. Again, we are interested in the probability that <img alt="Z &gt; 0" class="latex" src="https://s0.wp.com/latex.php?latex=Z+%3E+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z &gt; 0"/>, but due to the approximate nature of the above equation, we cannot work with this distribution directly, and instead can try the second moment method. However, the exponentiated Gaussian makes <img alt="\mathbb{E}[Z^2] \gg \mathbb{E}[Z]^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BZ%5E2%5D+%5Cgg+%5Cmathbb%7BE%7D%5BZ%5D%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{E}[Z^2] \gg \mathbb{E}[Z]^2"/> and the moment method just gives <img alt="P(Z&gt;0) \ge 0" class="latex" src="https://s0.wp.com/latex.php?latex=P%28Z%3E0%29+%5Cge+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(Z&gt;0) \ge 0"/>, whereas we need <img alt="P(Z&gt;0)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28Z%3E0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(Z&gt;0)"/> with high probability.</p>
<p>The fault here lies with Gaussian noise due to the <img alt="h_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=h_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_\mu"/>‘s and <img alt="H_i" class="latex" src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H_i"/>‘s, so it is natural to consider what happens when we condition on them, getting rid of the noise. We denote</p>
<p><img alt="\underline{h} = (h_\mu)_{\mu = 1}^M \text{ and } \underline{H} = (H_i)_{i=1}^N." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D+%3D+%28h_%5Cmu%29_%7B%5Cmu+%3D+1%7D%5EM+%5Ctext%7B+and+%7D+%5Cunderline%7BH%7D+%3D+%28H_i%29_%7Bi%3D1%7D%5EN.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{h} = (h_\mu)_{\mu = 1}^M \text{ and } \underline{H} = (H_i)_{i=1}^N."/></p>
<p>Then, we can compute that</p>
<p><img alt="\frac{1}{N}\log \mathbb{E}[Z | \underline{H},\underline{h}] \to \mathcal{G}(\alpha)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7BN%7D%5Clog+%5Cmathbb%7BE%7D%5BZ+%7C+%5Cunderline%7BH%7D%2C%5Cunderline%7Bh%7D%5D+%5Cto+%5Cmathcal%7BG%7D%28%5Calpha%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{1}{N}\log \mathbb{E}[Z | \underline{H},\underline{h}] \to \mathcal{G}(\alpha)"/></p>
<p>in probability as <img alt="N\to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=N%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N\to \infty"/>. (This is not an exact equality becacuse of the approximations made in the derivation of Gardner’s formula.) Moreover, we will have <img alt="\mathbb{E}[Z^2|\underline{H},\underline{h}] \approx \mathbb{E}[Z|\underline{H},\underline{h}]^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BZ%5E2%7C%5Cunderline%7BH%7D%2C%5Cunderline%7Bh%7D%5D+%5Capprox+%5Cmathbb%7BE%7D%5BZ%7C%5Cunderline%7BH%7D%2C%5Cunderline%7Bh%7D%5D%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{E}[Z^2|\underline{H},\underline{h}] \approx \mathbb{E}[Z|\underline{H},\underline{h}]^2"/>. The second moment method gives us the desired lower bound on <img alt="P(Z&gt;0)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28Z%3E0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(Z&gt;0)"/>. However, note that these <img alt="\underline{H},\underline{h}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7BH%7D%2C%5Cunderline%7Bh%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{H},\underline{h}"/> must satisfy the equations that defined them. In vector form, we have</p>
<p><img alt="\frac{1}{\sqrt{N}}G\tanh \underline{H} = \underline{h} + b_*F(\underline{h})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B%5Csqrt%7BN%7D%7DG%5Ctanh+%5Cunderline%7BH%7D+%3D+%5Cunderline%7Bh%7D+%2B+b_%2AF%28%5Cunderline%7Bh%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{1}{\sqrt{N}}G\tanh \underline{H} = \underline{h} + b_*F(\underline{h})"/><br/>
<img alt="\frac{1}{\sqrt{N}}G^T\tanh F(\underline{h}) = \underline{H} + d_*\tanh\underline{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B%5Csqrt%7BN%7D%7DG%5ET%5Ctanh+F%28%5Cunderline%7Bh%7D%29+%3D+%5Cunderline%7BH%7D+%2B+d_%2A%5Ctanh%5Cunderline%7BH%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{1}{\sqrt{N}}G^T\tanh F(\underline{h}) = \underline{H} + d_*\tanh\underline{H}"/></p>
<p>where <img alt="b_*" class="latex" src="https://s0.wp.com/latex.php?latex=b_%2A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b_*"/>, <img alt="F" class="latex" src="https://s0.wp.com/latex.php?latex=F&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="F"/>, and <img alt="d_*" class="latex" src="https://s0.wp.com/latex.php?latex=d_%2A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d_*"/> are constants and functions that appear in the rigorous definitions of <img alt="h_{\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=h_%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_{\mu}"/> and <img alt="H_i" class="latex" src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H_i"/>.<br/>
Here arises a problem, however. We cannot solve these equations easily, and furthermore when we condition on <img alt="\underline{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7BH%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{H}"/> and <img alt="\underline{h}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{h}"/>, the <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/>‘s that satisfy the above are not necessarily representative of matrices of standard Gaussians. Hence, simply conditioning on knowing the values of <img alt="\underline{h},\underline{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D%2C%5Cunderline%7BH%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{h},\underline{H}"/> destroys the model and will not prove the lower bound on <img alt="P(Z&gt;0)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28Z%3E0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(Z&gt;0)"/> for Gaussian disorder.</p>
<p>To solve this problem, we introduce iteration: first, initialize <img alt="\underline{h}^{(0)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D%5E%7B%280%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{h}^{(0)}"/> and <img alt="\underline{H}^{(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7BH%7D%5E%7B%281%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{H}^{(1)}"/> (initialized to specific values detailed in [6]. Then, a simplified version of each time step’s update looks like</p>
<p><img alt="\underline{h}^{(t)} = \frac{G\tanh'{\underline{H}^{(t)}}}{\sqrt{N}} - b_*F(\underline{h}^{(t - 1)})," class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D%5E%7B%28t%29%7D+%3D+%5Cfrac%7BG%5Ctanh%27%7B%5Cunderline%7BH%7D%5E%7B%28t%29%7D%7D%7D%7B%5Csqrt%7BN%7D%7D+-+b_%2AF%28%5Cunderline%7Bh%7D%5E%7B%28t+-+1%29%7D%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{h}^{(t)} = \frac{G\tanh'{\underline{H}^{(t)}}}{\sqrt{N}} - b_*F(\underline{h}^{(t - 1)}),"/><br/>
<img alt="\underline{H}^{(t + 1)} = \frac{G^TF(\underline{h}^t)}{\sqrt{N}} - d_*\tanh{\underline{H}^{(t + 1)}}." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7BH%7D%5E%7B%28t+%2B+1%29%7D+%3D+%5Cfrac%7BG%5ETF%28%5Cunderline%7Bh%7D%5Et%29%7D%7B%5Csqrt%7BN%7D%7D+-+d_%2A%5Ctanh%7B%5Cunderline%7BH%7D%5E%7B%28t+%2B+1%29%7D%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{H}^{(t + 1)} = \frac{G^TF(\underline{h}^t)}{\sqrt{N}} - d_*\tanh{\underline{H}^{(t + 1)}}."/></p>
<p>It has been proven (see [2,4]) that <img alt="\underline{h}^{(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{h}^{(t)}"/> and <img alt="\underline{H}^{(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7BH%7D%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{H}^{(t)}"/> converge as <img alt="t\to\infty" class="latex" src="https://s0.wp.com/latex.php?latex=t%5Cto%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t\to\infty"/>, and moreover the convergent values are distributed by what looks like a Gaussian at each time step. Since this sequence of <img alt="\underline{h},\underline{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D%2C%5Cunderline%7BH%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{h},\underline{H}"/> converge (at a rate that is independent of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/>) and are representative of Gaussian <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/>, for a special kind of truncated partition function <img alt="\tilde Z" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde+Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\tilde Z"/>, conditioning on these iteration values allows the second moment method to work and gives</p>
<p><img alt="\mathbb{E}[\tilde Z | \underline{h}^{(0)},\underline{H}^{(0)},\dots,\underline{h}^{(t)},\underline{H}^{(t)}]^2 \approx \mathbb{E}[\tilde Z^2 | \underline{h}^{(0)},\underline{H}^{(0)},\dots,\underline{h}^{(t)},\underline{H}^{(t)}]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5B%5Ctilde+Z+%7C+%5Cunderline%7Bh%7D%5E%7B%280%29%7D%2C%5Cunderline%7BH%7D%5E%7B%280%29%7D%2C%5Cdots%2C%5Cunderline%7Bh%7D%5E%7B%28t%29%7D%2C%5Cunderline%7BH%7D%5E%7B%28t%29%7D%5D%5E2+%5Capprox+%5Cmathbb%7BE%7D%5B%5Ctilde+Z%5E2+%7C+%5Cunderline%7Bh%7D%5E%7B%280%29%7D%2C%5Cunderline%7BH%7D%5E%7B%280%29%7D%2C%5Cdots%2C%5Cunderline%7Bh%7D%5E%7B%28t%29%7D%2C%5Cunderline%7BH%7D%5E%7B%28t%29%7D%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{E}[\tilde Z | \underline{h}^{(0)},\underline{H}^{(0)},\dots,\underline{h}^{(t)},\underline{H}^{(t)}]^2 \approx \mathbb{E}[\tilde Z^2 | \underline{h}^{(0)},\underline{H}^{(0)},\dots,\underline{h}^{(t)},\underline{H}^{(t)}]"/></p>
<p>which is then enough to establish the lower bound <img alt="P(Z&gt;0)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28Z%3E0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(Z&gt;0)"/> for the non-truncated partition function <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z"/> (see [6] for details, see also [3] for closely related computations).</p>
<p><strong>2. <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-NAE-SAT</strong></p>
<p>This lecture also gave a brief overview of results in <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-NAESAT. In the <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-SAT problem, we ask whether a boolean formula in <img alt="CNF" class="latex" src="https://s0.wp.com/latex.php?latex=CNF&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="CNF"/> form, with each clause containing exactly <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> literals, has a satisfying solution. For <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-NAESAT, we instead require that the satisfying solution is not uniform on any clause; equivalently, each clause must contain at least one true and at least one false value. Finally, we will restrict our set of possible boolean formulae to those for which every variable is contained in exactly <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/> clauses; we call this model <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/>-regular <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-NAESAT. We briefly outline the critical capacities of clauses where the solution space changes. For more background on the <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-NAESAT probem and its variants, see [10].</p>
<p>As with the perceptron model, we are concerned with the expected number of solutions <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z"/> of this system where the <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/>-regular formula is chosen uniformly at random. It turns out that for any fixed <img alt="\alpha=\frac{d}{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%3D%5Cfrac%7Bd%7D%7Bk%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha=\frac{d}{k}"/>, the expected proportion of satisfiable solutions as <img alt="n\to\infty" class="latex" src="https://s0.wp.com/latex.php?latex=n%5Cto%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n\to\infty"/> converges to some <img alt="f(\alpha)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Calpha%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\alpha)"/>. More on this, the model in general and the critical <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha"/> constants mentioned below can be found in [15]. Via an application of Markov’s inequality and Jensen’s Inequality for the convex function <img alt="x^n" class="latex" src="https://s0.wp.com/latex.php?latex=x%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x^n"/>, <img alt="f(\alpha)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Calpha%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\alpha)"/> may be bounded above by <img alt="f^{RS}(\alpha)=(\mathbb{E} Z)^{\frac{1}{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=f%5E%7BRS%7D%28%5Calpha%29%3D%28%5Cmathbb%7BE%7D+Z%29%5E%7B%5Cfrac%7B1%7D%7Bn%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f^{RS}(\alpha)=(\mathbb{E} Z)^{\frac{1}{n}}"/>, shown graphically below.</p>
<p><img alt="CS 229 Pic 2" class="alignnone size-full wp-image-6352" src="https://windowsontheory.files.wordpress.com/2018/12/CS-229-Pic-2.png?w=600"/><br/>
The diagram also shows the nature of the expected assortment of the solutions of a random <img alt="d-" class="latex" src="https://s0.wp.com/latex.php?latex=d-&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d-"/>regular <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-NAESAT for ranges of values of <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha"/>. Namely, physics analysis suggests the following:</p>
<ul>
<li>For <img alt="0&lt;\alpha&lt;\alpha_d" class="latex" src="https://s0.wp.com/latex.php?latex=0%3C%5Calpha%3C%5Calpha_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0&lt;\alpha&lt;\alpha_d"/>, w.h.p. the solutions are concentrated in a single large cluster.</li>
<li>For <img alt="\alpha_d&lt;\alpha&lt;\alpha_{\text{cond}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha_d%3C%5Calpha%3C%5Calpha_%7B%5Ctext%7Bcond%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha_d&lt;\alpha&lt;\alpha_{\text{cond}}"/>, w.h.p. the solutions are distributed among a large number of clusters.</li>
<li>For <img alt="\alpha_{\text{cond}}&lt;\alpha_{\text{sat}}&lt;0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha_%7B%5Ctext%7Bcond%7D%7D%3C%5Calpha_%7B%5Ctext%7Bsat%7D%7D%3C0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha_{\text{cond}}&lt;\alpha_{\text{sat}}&lt;0"/>, the function <img alt="f(\alpha)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Calpha%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\alpha)"/> breaks away from <img alt="f^{RS}(\alpha)" class="latex" src="https://s0.wp.com/latex.php?latex=f%5E%7BRS%7D%28%5Calpha%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f^{RS}(\alpha)"/>, and w.h.p. the solutions are concentrated in a small number of clusters.</li>
<li>For <img alt="\alpha&gt;\alpha_{\text{sat}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%3E%5Calpha_%7B%5Ctext%7Bsat%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha&gt;\alpha_{\text{sat}}"/>, w.h.p. there are no satisfiable solutions.</li>
</ul>
<p>One final note for this model: the solutions to satisfiability problems tend to be more clumpy than in the perceptron model, so correlation decay methods won’t immediately work. See [15] for how this is handled.</p>
<p><strong>References</strong></p>
<ol>
<li>N. Bansal. Constructive algorithms for discrepancy minimization. In <em>Proc. FOCS</em> 2010, pages 3–10.</li>
<li>M. Bayati and A. Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. <em>IEEE Trans. Inform. Theory</em>, 57(2):764-785, 2011.</li>
<li>Bolthausen, E., 2018. A Morita type proof of the replica-symmetric formula for SK. arXiv preprint arXiv:1809.07972.</li>
<li>E. Bolthausen. An iterative construction of solutions of the TAP equations for the Sherrington-Kirkpatrick model. <em>Commun. Math. Phys.</em>, 325(1):333-366, 2014.</li>
<li>T. Cover. Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. <em>IEEE Trans. Electron. Comput.</em> 14(3):326-334.</li>
<li>J. Ding and N. Sun. Capacity lower bound for the Ising perceptron. <a href="https://arxiv.org/pdf/1809.07742.pdf" rel="nofollow">https://arxiv.org/pdf/1809.07742.pdf</a></li>
<li>E. Gardner and B. Derrida. Optimal storage properties of neural network models. <em>J. Phys. A.</em>, 21(1): 271–284, 1988.</li>
<li>J. H. Kim and J. R. Roche. Covering cubes by random half cubes, with applications to binary neural networks. <em>J. Comput. Syst. Sci.</em>, 56(2):223–252, 1998</li>
<li>W. Krauth and M. Mézard. Storage capacity of memory networks with binary couplings. <em>J. Phy.</em> 50(20): 3057-3066, 1989.</li>
<li>F. Krzakala et al. “Gibbs states and the set of solutions of random constraint satisfaction problems.” <i>Proc. Natl. Acad. Sci.</i> 104.25 (2007): 10318-10323.</li>
<li>S. Lovett and R. Meka. Constructive discrepancy minimization by walking on the edges. <em>SIAM J. Comput.</em>, 44(5):1573-1582.</li>
<li>M. Mézard. The space of interactions in neural networks: Gardner’s computation with the cavity method. <em>J. Phys. A.</em>, 22(12):2181, 1989</li>
<li>M. Mézard and G. Parisi and M. Virasoro. SK Model: The Replica Solution without Replicas. <em>Europhys. Lett.</em>, 1(2): 77-82, 1986.</li>
<li>M. Shcherbina and B. Tirozzi. Rigorous solution of the Gardner problem. <em>Commun. Math. Phys.</em>, 234(3):383-422, 2003.</li>
<li>A. Sly and N. Sun and Y. Zhang. The Number of Solutions for Random Regular NAE-SAT. In <em>Proc. FOCS</em> 2016, pages 724-731.</li>
<li>J. Spencer. Six standard deviations suffice. <em>Trans. Amer. Math. Soc.</em> 289 (1985), 679-706</li>
<li>M. Talagrand. Intersecting random half cubes. <em>Random Struct. Algor.</em>, 15(3-4):436–449, 1999.</li>
</ol></div></content><updated planet:format="December 14, 2018 04:44 AM">2018-12-14T04:44:40Z</updated><published planet:format="December 14, 2018 04:44 AM">2018-12-14T04:44:40Z</published><category term="physics"/><author><name>degeneratetriangle</name></author><source><id>https://windowsontheory.org</id><logo>https://s0.wp.com/i/buttonw-com.png</logo><link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/><link href="https://windowsontheory.org" rel="alternate" type="text/html"/><link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/><link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/><subtitle>A Research Blog</subtitle><title>Windows On Theory</title><updated planet:format="December 17, 2018 05:29 AM">2018-12-17T05:29:53Z</updated><planet:module>toc</planet:module><planet:format>atom10</planet:format><planet:http_last_modified>Mon, 17 Dec 2018 01:41:40 GMT</planet:http_last_modified><planet:bozo>false</planet:bozo><planet:items_per_page>40</planet:items_per_page><planet:css-id>windows-on-theory</planet:css-id><planet:face>wot.png</planet:face><planet:name>Windows on Theory</planet:name><planet:http_location>https://windowsontheory.org/feed/</planet:http_location><planet:http_status>301</planet:http_status></source></entry>
